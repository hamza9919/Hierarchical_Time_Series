{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE AGG MATRIX AND READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\", index_col=0) \n",
    "agg_mat_df = pd.read_csv(\"agg_mat.csv\", index_col=0) # matrix of aggregated data with bottom time series\n",
    "level0total = 1\n",
    "level1total = 6\n",
    "level2total = 6*3\n",
    "level3total = 24\n",
    "level4total = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels_left = [0, level0total, level0total+level1total, level0total+level1total+level2total, level0total+level1total+level2total+level3total]\n",
    "levels_right = [0, level1total, level1total+level2total, level1total+level2total+level3total, level1total+level2total+level3total+level4total]\n",
    "nb_ts_levels = [level0total, level1total, level2total, level3total, level4total]\n",
    "total_ts = [0,level0total,level0total+level1total,level0total+level1total+level2total,level0total+level1total+level2total+level3total, level0total+level1total+level2total+level3total+level4total]\n",
    "lengths = nb_ts_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes=[[6], [3,3,3,3,3,3], [2,1,1,2,1,1,2,1,1,2,1,1,2,1,1,2,1,1], [9,6,3,10,6,7,6,4,9,7,9,8,9,6,3,6,3,3,6,8,7,2,8,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  3,  3,  3,  3,  3,  3,  2,  1,  1,  2,  1,  1,  2,  1,  1,  2,\n",
       "        1,  1,  2,  1,  1,  2,  1,  1,  9,  6,  3, 10,  6,  7,  6,  4,  9,\n",
       "        7,  9,  8,  9,  6,  3,  6,  3,  3,  6,  8,  7,  2,  8,  5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_list = np.array([])\n",
    "for i in range(len(nodes)):\n",
    "    for j in range(len(nodes[i])):\n",
    "        nodes_list = np.append(nodes_list, int(nodes[i][j]))\n",
    "nodes_list = nodes_list.astype(int)\n",
    "nodes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_agg_mat(nb_ts_levels, nodes_list):\n",
    "    nb_total_ts = sum(nb_ts_levels)\n",
    "    nb_ts_agg =sum(nb_ts_levels[:len(nb_ts_levels)-1])\n",
    "    global_Matrix = pd.DataFrame(np.zeros((nb_ts_agg, nb_total_ts-1)))\n",
    "    \n",
    "    c = 0\n",
    "    for i in range(len(nodes_list)):\n",
    "        for j in range(nodes_list[i]):\n",
    "            global_Matrix.iloc[i,c+j] = 1\n",
    "        c += nodes_list[i]\n",
    "            \n",
    "\n",
    "    return global_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create global matrix A\n",
    "### create list of number of TS in each level\n",
    "\n",
    "def matrix_per_level(global_Matrix, levels_left, levels_right, l):\n",
    "    return np.array(global_Matrix.iloc[levels_left[l-1]:levels_left[l], levels_right[l-1]:levels_right[l]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9    ...  188  189  190  191  \\\n",
       "0   1.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "5   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "6   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "7   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "8   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "9   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "10  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "11  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "12  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "13  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "14  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "15  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "16  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "17  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "18  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "19  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "20  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "21  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "22  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "23  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "24  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "25  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "26  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "27  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "28  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "29  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "30  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "31  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "32  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "33  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "34  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "35  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "36  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "37  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "38  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "39  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "40  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "41  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "42  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "43  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "44  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "45  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "46  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "47  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0  1.0  1.0   \n",
       "48  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    192  193  194  195  196  197  \n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "5   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "6   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "7   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "8   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "10  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "11  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "12  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "13  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "14  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "15  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "16  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "17  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "18  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "19  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "20  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "21  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "22  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "23  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "24  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "25  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "26  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "27  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "28  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "29  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "30  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "31  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "32  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "33  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "34  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "35  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "36  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "37  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "38  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "39  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "40  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "41  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "42  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "43  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "44  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "45  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "46  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "47  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "48  0.0  1.0  1.0  1.0  1.0  1.0  \n",
       "\n",
       "[49 rows x 198 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = creat_agg_mat(nb_ts_levels, nodes_list)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_per_level(A, levels_left, levels_right, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "### pivot data such as index is the name of columns\n",
    "#data = data.pivot(index='date', columns='symbol', values='close')\n",
    "pivot_df = data.T\n",
    "\n",
    "n_ts = 199\n",
    "n_timepoints = 365\n",
    "n_dates = 20\n",
    "input_size = n_ts*n_dates\n",
    "pred_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wmape(actual_values, forecasted_values):\n",
    "    n = len(actual_values)\n",
    "    num = np.sum(np.abs(actual_values - forecasted_values))\n",
    "    den = np.sum(np.abs(actual_values))\n",
    "    wmape = 100*num/den\n",
    "    return wmape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(n_timepoints-n_dates-pred_length+1):\n",
    "    X = pivot_df.iloc[:,i:i+n_dates].T.to_numpy().reshape(1,-1)\n",
    "    X_train.append(X[0])\n",
    "    y = pivot_df.iloc[:,i+n_dates:i+n_dates+1].T.to_numpy()\n",
    "    y_train.append(y[0])\n",
    "\n",
    "for i in range(n_timepoints-n_dates-pred_length+1, n_timepoints-n_dates+1):\n",
    "    X = pivot_df.iloc[:,i:i+n_dates].T.to_numpy().reshape(1,-1)\n",
    "    X_test.append(X[0])\n",
    "    y = pivot_df.iloc[:,i+n_dates:i+n_dates+1].T.to_numpy()\n",
    "    y_test.append(y[0])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First model without coherency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fix seed\n",
    "keras.utils.set_random_seed(99)\n",
    "\n",
    "# Enable eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Create a sequential model\n",
    "inp1 = Input(shape=(input_size,))\n",
    "h1_w1 = Dense(512*2*2, activation='relu')(inp1)\n",
    "h2_w1  = Dense(512*2, activation='relu')(h1_w1)\n",
    "h3_w1 = Dense(256*2, activation='relu')(h2_w1)\n",
    "h4_w1 = Dense(256, activation='relu')(h3_w1)\n",
    "out1 = Dense(199, activation='linear')(h4_w1)\n",
    "\n",
    "mdl1 = Model(inputs=inp1, outputs=out1)\n",
    "\n",
    "mdl1.compile(loss=\"mse\", optimizer='adam')\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_epochs = 0\n",
    "best_batch_size = 0\n",
    "\n",
    "for epochs in [100, 150, 200]:\n",
    "    for batch_size in [16, 32, 64]:\n",
    "        history = mdl1.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        loss = history.history['val_loss'][-1] if 'val_loss' in history.history else history.history['loss'][-1]\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_epochs = epochs\n",
    "            best_batch_size = batch_size\n",
    "\n",
    "print(\"Best configuration - Epochs:\", best_epochs, \"Batch Size:\", best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 147135392.0000\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 74337816.0000\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 56691624.0000\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 54962864.0000\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 52071672.0000\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 49626592.0000\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 47933940.0000\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 46607496.0000\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 45954456.0000\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 43876604.0000\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 43438616.0000\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 41505028.0000\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 39885528.0000\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 39102332.0000\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 37922968.0000\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 36119080.0000\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 34794724.0000\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 33915188.0000\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 31952872.0000\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 31061574.0000\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 29016494.0000\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 25870864.0000\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 22850692.0000\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 20658312.0000\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 20591118.0000\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 18152356.0000\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 14989545.0000\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 13748147.0000\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 10958548.0000\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 9837805.0000\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 82ms/step - loss: 8923365.0000\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 8360519.0000\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 7605062.0000\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 7847045.5000\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 8230971.5000\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 8089057.0000\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 12734556.0000\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 11718111.0000\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 8446360.0000\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 7488441.0000\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 6483315.0000\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 6183268.0000\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 5839946.0000\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 5580049.0000\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 5471802.0000\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 5219002.0000\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 4934028.0000\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 4801716.0000\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 4655158.0000\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 4493489.0000\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 4337038.5000\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 4238142.5000\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 4084485.2500\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 3944400.5000\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 3824318.2500\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 3699406.2500\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 3598815.2500\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 3556573.5000\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 3450203.2500\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 3318307.5000\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 3280611.5000\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 3188785.2500\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 3215751.0000\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 3053452.0000\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 2997452.7500\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 3111278.5000\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 3023573.2500\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 2852609.0000\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 2725439.5000\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 2695688.2500\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 2573609.2500\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 2571086.7500\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 2567010.7500\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 3737601.5000\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 4408223.0000\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 10861149.0000\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 13478713.0000\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 8521525.0000\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 7582616.0000\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 7707073.5000\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 6215056.0000\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 5390433.0000\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 4499080.0000\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 5180216.5000\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 4760100.5000\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 3757511.5000\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 3266951.5000\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 3480003.2500\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 2943351.2500\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 2517206.7500\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 2420613.2500\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 2402810.5000\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 2208351.2500\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 2122744.5000\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 2117774.2500\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 2099631.0000\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 1991078.7500\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 2038493.2500\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 1924840.6250\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 1894377.3750\n"
     ]
    }
   ],
   "source": [
    "best_epochs = 100\n",
    "best_batch_size = 64\n",
    "\n",
    "## fix seed\n",
    "keras.utils.set_random_seed(99)\n",
    "\n",
    "# Enable eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Create a sequential model\n",
    "inp1 = Input(shape=(input_size,))\n",
    "h1_w1 = Dense(512*4, activation='relu')(inp1)\n",
    "h2_w1  = Dense(512*2, activation='relu')(h1_w1)\n",
    "h3_w1 = Dense(256*2, activation='relu')(h2_w1)\n",
    "h4_w1 = Dense(256, activation='relu')(h3_w1)\n",
    "out1 = Dense(199, activation='linear')(h4_w1)\n",
    "\n",
    "mdl1 = Model(inputs=inp1, outputs=out1)\n",
    "\n",
    "mdl1.compile(loss=\"mse\", optimizer='adam')\n",
    "\n",
    "history1 = mdl1.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAhOklEQVR4nO3df5RcZZ3n8feHTiBpAihJQEiTTpiJ/BAh0QaRsBhw95igI/hrh0wTENEQDgqCMyaaVZjjcM7OrjPLssJgi8ivHtAVBpFFVJQYlVFoNDIEEoyQDi0onURDMGB+8N0/7q2k0qmqru7U7eqq+3mdU6frPvepqu9TVX2/dZ/n3vsoIjAzs/zap94BmJlZfTkRmJnlnBOBmVnOORGYmeWcE4GZWc45EZiZ5ZwTgeWepO9IOr/WdYcYwxxJfbV+XrNqjKl3AGbDIenlosVW4M/AjnT5oojorva5ImJeFnXNGoUTgTWkiJhQuC9pLfDRiHhwYD1JYyJi+0jGZtZo3DVkTaXQxSJpsaTfAV+T9HpJ90nql/SH9H5b0WOWSfpoev/Dkn4i6Ytp3WclzRtm3emSlkvaLOlBSddJur3KdhyTvtYfJa2U9N6idWdKejJ93t9K+tu0fFLatj9K2ijpx5L8P26D8pfEmtEbgIOBdmAhyff8a+nyVOAV4EsVHv82YDUwCfgfwFclaRh1/xV4BJgIXAUsqCZ4SWOBbwPfAw4BPgF0SzoqrfJVku6vA4DjgB+m5Z8C+oDJwKHAZwFfQ8YG1ZCJQNJNkl6U9EQVdadKekjSLyU9LunMkYjR6uo14MqI+HNEvBIRGyLirojYEhGbgauBd1R4fG9EfCUidgC3AIeRbFirritpKnAi8PmI2BoRPwHurTL+k4EJwH9PH/tD4D5gfrp+G3CspAMj4g8R8Yui8sOA9ojYFhE/Dl9MzKrQkIkAuBmYW2Xd/wZ8IyJmAecA12cVlI0a/RHxamFBUqukL0vqlfQSsBx4naSWMo//XeFORGxJ704YYt3DgY1FZQDPVRn/4cBzEfFaUVkvMCW9/wHgTKBX0o8kvT0t/5/AGuB7kp6RtKTK17Oca8hEEBHLgY3FZZL+QtIDkh5L+0aPLlQHDkzvHwQ8P4KhWn0M/BX8KeAo4G0RcSBwWlperrunFl4ADpbUWlR2RJWPfR44YkD//lTgtwAR8WhEnEXSbXQP8I20fHNEfCoijgT+CrhC0jv3rhmWBw2ZCMroAj4REW8F/pZdv/yvAs5Nj9G+n6S/1fLlAJJxgT9KOhi4MusXjIheoAe4StK+6a/2v6ry4T8H/gR8WtJYSXPSx96ZPlenpIMiYhvwEulhs5LeI+kv0zGKQvmOkq9gVqQpEoGkCcApwP+VtAL4MklfKST9qjdHRBvJ7vRtPpIid64BxgPrgZ8BD4zQ63YCbwc2AP8AfJ3kfIeKImIr8F5gHknM1wPnRcSqtMoCYG3azbUIODctnwE8CLwM/DtwfUQsq1VjrHmpUceSJE0D7ouI4yQdCKyOiMNK1FsJzI2I59LlZ4CTI+LFEQ3Yck/S14FVEZH5HonZUDTFL+OIeAl4VtKHAJQ4IV29DnhnWn4MMA7or0ugliuSTkzHrvaRNBc4i6RP32xUachEIOkOkl3fo9KThy4k2Q2/UNKvgJUk/3SQDBR+LC2/A/iwD6mzEfIGYBlJV821wMUR8cu6RmRWQsN2DZmZWW005B6BmZnVTsNddG7SpEkxbdq0eodhZtZQHnvssfURMbnUuoZLBNOmTaOnp6feYZiZNRRJveXWuWvIzCznnAjMzHLOicDMLOcabozAzEavbdu20dfXx6uvvjp4ZcvEuHHjaGtrY+zYsVU/xonAzGqmr6+PAw44gGnTplF+Lh/LSkSwYcMG+vr6mD59etWPy0XXUHc3TJsG++yT/O2uelpzMxuKV199lYkTJzoJ1IkkJk6cOOQ9sqbfI+juhoULYUs6PUhvb7IM0NlZv7jMmpWTQH0N5/1v+j2CpUt3JYGCLVuScjMzy0EiWLduaOVm1rg2bNjAzJkzmTlzJm94wxuYMmXKzuWtW7dWfGxPTw+XXnrpoK9xyimn1CTWZcuW8Z73vKcmz7W3mj4RTJ06tHIzGzm1Hr+bOHEiK1asYMWKFSxatIjLL7985/K+++7L9u3byz62o6ODa6+9dtDXePjhh/cuyFGo6RPB1VdDa+vuZa2tSbmZ1U9h/K63FyJ2jd/V+mCOD3/4w1xxxRWcfvrpLF68mEceeYRTTjmFWbNmccopp7B69Wpg91/oV111FR/5yEeYM2cORx555G4JYsKECTvrz5kzhw9+8IMcffTRdHZ2Uria8/3338/RRx/NqaeeyqWXXjroL/+NGzdy9tlnc/zxx3PyySfz+OOPA/CjH/1o5x7NrFmz2Lx5My+88AKnnXYaM2fO5LjjjuPHP/7xXr9HTT9YXBgQXro06Q6aOjVJAh4oNquvSuN3tf7/fPrpp3nwwQdpaWnhpZdeYvny5YwZM4YHH3yQz372s9x11117PGbVqlU89NBDbN68maOOOoqLL754j2Pzf/nLX7Jy5UoOP/xwZs+ezU9/+lM6Ojq46KKLWL58OdOnT2f+/PmDxnfllVcya9Ys7rnnHn74wx9y3nnnsWLFCr74xS9y3XXXMXv2bF5++WXGjRtHV1cX73rXu1i6dCk7duxgy8A3cRiaPhFA8qXyht9sdBnJ8bsPfehDtLS0ALBp0ybOP/98fv3rXyOJbdu2lXzMu9/9bvbbbz/2228/DjnkEH7/+9/T1ta2W52TTjppZ9nMmTNZu3YtEyZM4Mgjj9x5HP/8+fPp6uqqGN9PfvKTncnojDPOYMOGDWzatInZs2dzxRVX0NnZyfvf/37a2to48cQT+chHPsK2bds4++yzmTlz5t68NUAOuobMbHQayfG7/ffff+f9z33uc5x++uk88cQTfPvb3y57zP1+++23835LS0vJ8YVSdYYz2Vepx0hiyZIl3HjjjbzyyiucfPLJrFq1itNOO43ly5czZcoUFixYwK233jrk1xvIicDM6qJe43ebNm1iypQpANx88801f/6jjz6aZ555hrVr1wLw9a9/fdDHnHbaaXSngyPLli1j0qRJHHjggfzmN7/hzW9+M4sXL6ajo4NVq1bR29vLIYccwsc+9jEuvPBCfvGLX+x1zJklAkk3SXpR0hOD1DtR0g5JH8wqFjMbfTo7oasL2ttBSv52dWXfjfvpT3+az3zmM8yePZsdO3bU/PnHjx/P9ddfz9y5czn11FM59NBDOeiggyo+5qqrrqKnp4fjjz+eJUuWcMsttwBwzTXXcNxxx3HCCScwfvx45s2bx7Jly3YOHt91111cdtllex1zZnMWSzqNZNLuWyPiuDJ1WoDvA68CN0XENwd73o6OjvDENGaj01NPPcUxxxxT7zDq7uWXX2bChAlEBJdccgkzZszg8ssvH7HXL/U5SHosIjpK1c9sjyAilgMbB6n2CeAu4MWs4jAzG2lf+cpXmDlzJm9605vYtGkTF110Ub1DqqhuRw1JmgK8DzgDOHGQuguBhQBTfSaYmY1yl19++YjuAeyteg4WXwMsjohBO+kioisiOiKiY/LkknMvm9kokVV3s1VnOO9/Pc8j6ADuTK+UNwk4U9L2iLinjjGZ2V4YN24cGzZs8KWo66QwH8G4ceOG9Li6JYKI2DlrgqSbgfucBMwaW1tbG319ffT399c7lNwqzFA2FJklAkl3AHOASZL6gCuBsQARcUNWr2tm9TN27NghzYxlo0NmiSAiBr/Axq66H84qDjMzq8xnFpuZ5ZwTgZlZzjkRmJnlnBOBmVnOORGYmeWcE4GZWc45EZiZ5ZwTgZlZzjkRmJnlnBOBmVnOORGYmeWcE4GZWc45EZiZ5ZwTgZlZzjkRmJnlnBOBmVnOORGYmeWcE4GZWc5llggk3STpRUlPlFnfKenx9PawpBOyisXMzMrLco/gZmBuhfXPAu+IiOOBLwBdGcZiZmZlZDl5/XJJ0yqsf7ho8WdAW1axmJlZeaNljOBC4DvlVkpaKKlHUk9/f/8IhmVm1vzqnggknU6SCBaXqxMRXRHREREdkydPHrngzMxyILOuoWpIOh64EZgXERvqGYuZWV7VbY9A0lTgbmBBRDxdrzjMzPIusz0CSXcAc4BJkvqAK4GxABFxA/B5YCJwvSSA7RHRkVU8ZmZWWpZHDc0fZP1HgY9m9fpmZladug8Wm5lZfTkRmJnlnBOBmVnOORGYmeWcE4GZWc45EZiZ5ZwTgZlZzjkRmJnlnBOBmVnOORGYmeWcE4GZWc45EZiZ5ZwTgZlZzjkRmJnlnBOBmVnOORGYmeWcE4GZWc45EZiZ5VxmiUDSTZJelPREmfWSdK2kNZIel/SWrGIxM7PystwjuBmYW2H9PGBGelsI/EuGsZiZWRmZJYKIWA5srFDlLODWSPwMeJ2kw7KKx8zMSqvnGMEU4Lmi5b60bA+SFkrqkdTT398/IsGZmeVFPROBSpRFqYoR0RURHRHRMXny5IzDMjPLl3omgj7giKLlNuD5OsViZpZb9UwE9wLnpUcPnQxsiogX6hiPmVkujcnqiSXdAcwBJknqA64ExgJExA3A/cCZwBpgC3BBVrGYmVl5mSWCiJg/yPoALsnq9c3MrDo+s9jMLOecCMzMcs6JwMws55wIzMxyzonAzCznnAjMzHLOicDMLOecCMzMcs6JwMws55wIzMxyzonAzCznnAjMzHLOicDMLOecCMzMcs6JwMws55wIzMxyzonAzCznnAjMzHIu00Qgaa6k1ZLWSFpSYv1Bkr4t6VeSVkryvMVmZiMss0QgqQW4DpgHHAvMl3TsgGqXAE9GxAkkE93/k6R9s4rJzMz2lOUewUnAmoh4JiK2AncCZw2oE8ABkgRMADYC2zOMyczMBqgqEUjaX9I+6f03SnqvpLGDPGwK8FzRcl9aVuxLwDHA88B/AJdFxGtVRW5mZjVR7R7BcmCcpCnAD4ALgJsHeYxKlMWA5XcBK4DDgZnAlyQduMcTSQsl9Ujq6e/vrzJkMzOrRrWJQBGxBXg/8H8i4n0k/f6V9AFHFC23kfzyL3YBcHck1gDPAkcPfKKI6IqIjojomDx5cpUhm5lZNapOBJLeDnQC/y8tGzPIYx4FZkiang4AnwPcO6DOOuCd6QscChwFPFNlTGZmVgODbcwLPgl8Bvi3iFgp6UjgoUoPiIjtkj4OfBdoAW5KH7soXX8D8AXgZkn/QdKVtDgi1g+vKWZmNhyKGNhtP8gDkkHjCRHxUjYhVdbR0RE9PT3Dfnx3NyxdCuvWwdSpcPXV0NlZwwDNzEYhSY9FREepddUeNfSvkg6UtD/wJLBa0t/VMsiR0N0NCxdCby9EJH8XLkzKzczyqtoxgmPTPYCzgfuBqcCCrILKytKlsGXL7mVbtiTlZmZ5VW0iGJueN3A28K2I2Maeh4KOeuvWDa3czCwPqk0EXwbWAvsDyyW1A3UZI9gbU6cOrdzMLA+qSgQRcW1ETImIM9Nj/nuB0zOOreauvhpaW3cva21Nys3M8qraweKDJP1z4exeSf9EsnfQUDo7oasL2ttBSv52dfmoITPLt2rPI7gJeAL4r+nyAuBrJGcaN5TOTm/4zcyKVZsI/iIiPlC0/PeSVmQQj5mZjbBqB4tfkXRqYUHSbOCVbEIyM7ORVO0ewSLgVkkHpct/AM7PJiQzMxtJVSWCiPgVcELhEtER8ZKkTwKPZxibmZmNgCHNUBYRLxVdY+iKDOIxM7MRtjdTVZaaeMbMzBrM3iSChrvEhJmZ7aniGIGkzZTe4AsYn0lEZmY2oiomgog4YKQCMTOz+tibriEzM2sCTgRmZjnnRGBmlnOZJgJJcyWtlrRG0pIydeZIWiFppaQfZRmPmZntqdpLTAyZpBbgOuC/AH3Ao5LujYgni+q8DrgemBsR6yQdklU8ZmZWWpZ7BCcBayLimYjYCtwJnDWgzt8Ad0fEOoCIeDHDeMzMrIQsE8EU4Lmi5b60rNgbgddLWibpMUnnlXoiSQsLk+L09/dnFK6ZWT5lmQhKXYJi4MlpY4C3Au8G3gV8TtIb93hQRFdEdEREx+TJk2saZHc3TJsG++yT/O3urunTm5mNepmNEZDsARxRtNwGPF+izvqI+BPwJ0nLgROApzOMa6fubli4ELZsSZZ7e5Nl8CxmZpYfWe4RPArMkDRd0r7AOcC9A+p8C/hPksZIagXeBjyVYUy7Wbp0VxIo2LIlKTczy4vMEkFEbAc+DnyXZOP+jYhYKWmRpEVpnaeAB0jmNXgEuDEinsgqpoHWrStd3tvrbiIzyw9FNNZFRDs6OqKnp6cmzzVtWrLRL6e1Fbq63E1kZo1P0mMR0VFqXa7PLL766mRjX467icwsD3KdCDo7k1/87e3l65TrPjIzaxa5TgSQJIO1a8sngwiPF5hZc8t9Iiio1E1UOKzUycDMmpETQWqwbiKPF5hZs3IiKFLoJlKpc6LxYaVm1pycCEqYOrX8OncTmVmzcSIooZrDSs8913sHZtYcnAhKqOawUvDegZk1ByeCMgY7rLTAewdm1uicCAYxWDdRQW8vLFiQDDQ7KZhZI3EiGES13USQnHwG7jIys8biRFCFQjfR7bdXt3cAPu/AzBqHE8EQDGXvAJI9g0mTkptnQDOz0cqJYIiGunewYUNyi3CXkZmNTk4EwzRw76Dc2cgDucvIzEYbJ4K9UNg7iIDbbhtal5G7icxstHAiqJFqzzsocDeRmY0WmSYCSXMlrZa0RtKSCvVOlLRD0gezjGckVHveAfhkNDMbHTJLBJJagOuAecCxwHxJx5ap948kk9w3vOKxAwkmTkxulfhkNDOrpyz3CE4C1kTEMxGxFbgTOKtEvU8AdwEvZhjLiCp0E732Gqxfn9wG6zLyyWhmVi9ZJoIpwHNFy31p2U6SpgDvA26o9ESSFkrqkdTT399f80BHgruMzGy0yjIRlDqgMgYsXwMsjogdlZ4oIroioiMiOiZPnlyr+EbUUE9GA+8dmNnIyDIR9AFHFC23Ac8PqNMB3ClpLfBB4HpJZ2cYU10N91IV55/vM5PNLDtjMnzuR4EZkqYDvwXOAf6muEJETC/cl3QzcF9E3JNhTKNCZ2fyd+nS5Fe/tGuMoJQd6f5SYQ+h+DnMzPZWZnsEEbEd+DjJ0UBPAd+IiJWSFklalNXrNorhnozm8QMzqzVFpZ+io1BHR0f09PTUO4xMdHcnv/i3bKmufmFPor09GYz2XoKZlSPpsYjoKLXOZxaPIgPPQWhpqVzfh5yaWS04EYwyxecg3HKL5z8ws+w5EYxiw5n/wGMHZjZUTgSj3FAPOfXlKsxsqJwIGsRQ5j/w2IGZDYUTQQMZziGnHjsws8E4ETSoocx/4LEDM6vEiaDBVXsxO3cTmVk5TgQNbihjB+4mMrNSnAiawFDGDtatG7GwzKxBOBE0mcHGDiI8XmBmu3MiaFKVxg48XmBmxZwImtRgZyV7vMDMCpwImlihm6jcALLHC8wMnAhyYerU0uUeLzAzcCLIBY8XmFklTgQ54PECM6vEiSAnBhsv6O2FSZOS2z77uMvILE8yTQSS5kpaLWmNpCUl1ndKejy9PSzphCzjsfLjBQAbNiS3CHcZmeVJZolAUgtwHTAPOBaYL+nYAdWeBd4REccDXwC6sorHEtVemwjcZWSWF1nuEZwErImIZyJiK3AncFZxhYh4OCL+kC7+DGjLMB5j6LOe+RBTs+aXZSKYAjxXtNyXlpVzIfCdUiskLZTUI6mnv7+/hiHm01AuYe1DTM2aX5aJoNSwZJSsKJ1OkggWl1ofEV0R0RERHZMnT65hiPnmS1ibGWSbCPqAI4qW24DnB1aSdDxwI3BWRGzIMB4boLibSIKJE5NbKR4vMGteWSaCR4EZkqZL2hc4B7i3uIKkqcDdwIKIeDrDWKyMQjfRa6/B+vXJzZekMMuXzBJBRGwHPg58F3gK+EZErJS0SNKitNrngYnA9ZJWSOrJKh6rni9JYZYviijZbT9qdXR0RE+P80WWuruTMYEtW0qvb21NupQ6O0c2LjMbPkmPRURHqXU+s9j2UM0lKc4913sHVlvd3cl3yme2jzwnAitpsEtSwMgcTeSNQz4U9kJ7e31mez24a8gqmjYt+aespL09SRq1VqqLyt1Szanc9yyr71YeuWvIhq2acw2yOppo6dI9xyncLdWcyn2HfKTayHAisIqquSTFPvtk03VTaSPgroPmUOj6K9cxUekiiVY7TgQ2qMJ4we23l9472LFjV7/uggXJuEItksJgGwGf5NbYiscFSmltTfZILXtOBFa1gWcit7TsWafwy64WSaGe3VKWvVJdfwXt7R4LGklOBDYkxWciv/Za5bpDSQrFRwcVJshZsADGjy9/2YvCa3i8oDGVS+JS8iNg6VIfLTZSfNSQDVs1RxSVIiUb8MIGfsOGXWWltLbC+efDLbeU/wVZeHx7e7IR8S/J0a/c92fiRHjlld0/64HfmY0bk65Df9bV81FDlomhTHJTrLDBL8yIVlxWypYtcP/9lQeti/c+PIjcGEp9fwrLAxP+wO+MzzWoLScCG7aBRxRVOvlsb61bV91JbrDrEFPPwTy6DRxzKowLbNxY/XP4cOLacCKwvVLYOEfAbbdllxSKjyCq9pDCgb8eL7hgV2IoThLl7nvjkr3iMae1a5Pl4RwyWmkcymenVyEiGur21re+NWz0u/32iPb2CIiQkr/DvbW2Js9X/NytrXv3nNXeCrFPnJjcpPL329t3j9NKK3w3yr1ntfh8x45NPpdS37/Cct4+L6AnymxX675hH+rNiaDxVJsUSm10y/2z1jLR1CtxDGznxRdX3kA2skqf18BEP1j9LD6vZnu/S3EisFGj+NdgrX5NF280muk23KRSq/u12jhW8wu/vX3wz7c4vnq8342eLColAh8+ak1jsHkUbOjKHbZ55pnJkVzr1sHBB+9aV+r+hiomoJUGPy+lWL0+61LvR7n2j7bDWysdPupEYE2luzs5EanUBmrzZti6tb7xWWnDucpo4bPu7a18Hko9DZY4hppU9ybBOBGYUTlJDLw/2EluVju1uLT4wM92YNIvfJbN8pkO5z2r2wllkuZKWi1pjaQlJdZL0rXp+sclvSXLeCzfig9VXL8+uZW7H7HrcFgp+UU3cWL5+5DteRTNpvBe1eqaQgM/25tu2v38hNtu2/0zLY6hEdX8govlBg/29ga0AL8BjgT2BX4FHDugzpnAdwABJwM/H+x5PVhso1W5gfChHjWU9ZEy9b6NlkHX0Xrk2VAGt4eCCoPFY2qYUwY6CVgTEc8ASLoTOAt4sqjOWcCtaZA/k/Q6SYdFxAsZxmWWic7O2g0MDqUbK4v7WXSNjbbZ5Yo/r8He79HYVVjLuRqyTARTgOeKlvuAt1VRZwqwWyKQtBBYCDDVM1VYDtQyqQxXuY3jSA5wjpRq3u/RNMZU67kaskwEpXrgBr4t1dQhIrqALkgGi/c+NDMbzGhIRqPJUN+PahLHaEmqWSaCPuCIouU24Plh1DEzaziNlEizPGroUWCGpOmS9gXOAe4dUOde4Lz06KGTgU0eHzAzG1mZ7RFExHZJHwe+S3IE0U0RsVLSonT9DcD9JEcOrQG2ABdkFY+ZmZWWZdcQEXE/yca+uOyGovsBXJJlDGZmVpnnIzAzyzknAjOznGu4aw1J6geGMmX6JGB9RuGMZnlsdx7bDPlsdx7bDHvX7vaImFxqRcMlgqGS1BNlLrTUzPLY7jy2GfLZ7jy2GbJrt7uGzMxyzonAzCzn8pAIuuodQJ3ksd15bDPks915bDNk1O6mHyMwM7PK8rBHYGZmFTgRmJnlXFMngsGmymwGko6Q9JCkpyStlHRZWn6wpO9L+nX69/X1jrXWJLVI+qWk+9LlPLT5dZK+KWlV+pm/PSftvjz9fj8h6Q5J45qt3ZJukvSipCeKysq2UdJn0m3baknv2pvXbtpEIKkFuA6YBxwLzJd0bH2jysR24FMRcQzJdJ+XpO1cAvwgImYAP0iXm81lwFNFy3lo8/8GHoiIo4ETSNrf1O2WNAW4FOiIiONILmJ5Ds3X7puBuQPKSrYx/R8/B3hT+pjr023esDRtIqBoqsyI2AoUpspsKhHxQkT8Ir2/mWTDMIWkrbek1W4Bzq5LgBmR1Aa8G7ixqLjZ23wgcBrwVYCI2BoRf6TJ250aA4yXNAZoJZm3pKnaHRHLgY0Disu18Szgzoj4c0Q8S3IF55OG+9rNnAjKTYPZtCRNA2YBPwcOLcztkP49pI6hZeEa4NPAa0Vlzd7mI4F+4Gtpl9iNkvanydsdEb8FvgisI5nGdlNEfI8mb3eqXBtrun1r5kRQ1TSYzULSBOAu4JMR8VK948mSpPcAL0bEY/WOZYSNAd4C/EtEzAL+RON3hwwq7Rc/C5gOHA7sL+nc+kZVdzXdvjVzIsjNNJiSxpIkge6IuDst/r2kw9L1hwEv1iu+DMwG3itpLUmX3xmSbqe52wzJd7ovIn6eLn+TJDE0e7v/M/BsRPRHxDbgbuAUmr/dUL6NNd2+NXMiqGaqzIYnSSR9xk9FxD8XrboXOD+9fz7wrZGOLSsR8ZmIaIuIaSSf6w8j4lyauM0AEfE74DlJR6VF7wSepMnbTdIldLKk1vT7/k6SsbBmbzeUb+O9wDmS9pM0HZgBPDLsV4mIpr2RTIP5NPAbYGm948mojaeS7BI+DqxIb2cCE0mOMvh1+vfgeseaUfvnAPel95u+zcBMoCf9vO8BXp+Tdv89sAp4ArgN2K/Z2g3cQTIGso3kF/+FldoILE23bauBeXvz2r7EhJlZzjVz15CZmVXBicDMLOecCMzMcs6JwMws55wIzMxyzonALCVph6QVRbeanbUraVrxVSXNRpMx9Q7AbBR5JSJm1jsIs5HmPQKzQUhaK+kfJT2S3v4yLW+X9ANJj6d/p6blh0r6N0m/Sm+npE/VIukr6XX1vydpfFr/UklPps9zZ52aaTnmRGC2y/gBXUN/XbTupYg4CfgSyZVPSe/fGhHHA93AtWn5tcCPIuIEkmsBrUzLZwDXRcSbgD8CH0jLlwCz0udZlE3TzMrzmcVmKUkvR8SEEuVrgTMi4pn0An+/i4iJktYDh0XEtrT8hYiYJKkfaIuIPxc9xzTg+5FMMIKkxcDYiPgHSQ8AL5NcMuKeiHg546aa7cZ7BGbViTL3y9Up5c9F93ewa4zu3SSz6b0VeCydfMVsxDgRmFXnr4v+/nt6/2GSq58CdAI/Se//ALgYds6rfGC5J5W0D3BERDxEMtHO64A99krMsuRfHma7jJe0omj5gYgoHEK6n6Sfk/x4mp+WXQrcJOnvSGYOuyAtvwzoknQhyS//i0muKllKC3C7pINIJhv5X5FMP2k2YjxGYDaIdIygIyLW1zsWsyy4a8jMLOe8R2BmlnPeIzAzyzknAjOznHMiMDPLOScCM7OccyIwM8u5/w/q704hvJAu+AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history1.history\n",
    "history_dict.keys()\n",
    "loss_values = history_dict[\"loss\"]\n",
    "#val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "#plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28.866526710150293"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = np.array(mdl1.predict(X_test))#.reshape(-1)\n",
    "calculate_wmape(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL FOR HIERARCHICAL TIME SERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_weights(weight, lengths):\n",
    "    start_idx = 0\n",
    "    result = []\n",
    "    for length in lengths:\n",
    "        sub_array = weight[start_idx:start_idx + length]\n",
    "        result.append(sub_array)\n",
    "        start_idx += length\n",
    "\n",
    "    result = np.array(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom loss function\n",
    "def custom_loss_with_regularization(reg_weight, weight_matrix, lengths, A, levels_left, levels_right):\n",
    "    def loss(y_true, y_pred):\n",
    "        mse_loss = keras.losses.mean_squared_error(y_true, y_pred)  # MSE loss\n",
    "        \n",
    "        def loss_up(weight_matrix):\n",
    "            weights_list = weight_matrix\n",
    "            n, m = weights_list.shape\n",
    "            regularization_loss = 0\n",
    "\n",
    "            for i in range(n):\n",
    "                w = reshape_weights(weights_list[i], lengths)\n",
    "                for l in range(len(w) - 1):\n",
    "                    mat_agg = matrix_per_level(A, levels_left, levels_right, l + 1)\n",
    "                    mat_agg = tf.convert_to_tensor(mat_agg, dtype=tf.float32)  # Convert to TensorFlow tensor\n",
    "                    w_l = w[l]\n",
    "                    w_l2 = w[l + 1]\n",
    "                    w_l_expanded = tf.expand_dims(w_l, axis=0)\n",
    "                    w_l2_expanded = tf.expand_dims(w_l2, axis=-1)\n",
    "                    regularization_loss += tf.reduce_sum(w_l_expanded - tf.transpose(tf.matmul(mat_agg, w_l2_expanded)))\n",
    "            \n",
    "            return regularization_loss\n",
    "        \n",
    "        custom_reg_loss = loss_up(weight_matrix)\n",
    "        total_loss = mse_loss + reg_weight * custom_reg_loss \n",
    "        return total_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hamza\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:371: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration - Epochs: 150 Batch Size: 64\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 189841.3594\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 189240.8125\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 188612.9219\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 188024.0781\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 187426.0156\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 186830.0312\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 186261.5938\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 185659.7500\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 185117.5312\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 184526.6406\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 183991.0000\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 183423.6719\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 182877.8281\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 182342.7344\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 181804.4062\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 181291.2812\n",
      "Epoch 17/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 180766.2188\n",
      "Epoch 18/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 180247.0156\n",
      "Epoch 19/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 179756.5781\n",
      "Epoch 20/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 179236.8438\n",
      "Epoch 21/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 178753.9062\n",
      "Epoch 22/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 178262.3438\n",
      "Epoch 23/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 177765.2500\n",
      "Epoch 24/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 177296.3594\n",
      "Epoch 25/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 176814.6562\n",
      "Epoch 26/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 176344.9062\n",
      "Epoch 27/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 175886.2188\n",
      "Epoch 28/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 175428.8906\n",
      "Epoch 29/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 174977.2656\n",
      "Epoch 30/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 174538.1719\n",
      "Epoch 31/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 174089.9844\n",
      "Epoch 32/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 173640.5781\n",
      "Epoch 33/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 173173.9062\n",
      "Epoch 34/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 172725.3594\n",
      "Epoch 35/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 172275.9375\n",
      "Epoch 36/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 171825.1875\n",
      "Epoch 37/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 171387.1562\n",
      "Epoch 38/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 170945.9844\n",
      "Epoch 39/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 170515.6562\n",
      "Epoch 40/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 170088.9062\n",
      "Epoch 41/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 169658.7500\n",
      "Epoch 42/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 169252.5156\n",
      "Epoch 43/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 168835.7500\n",
      "Epoch 44/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 168414.5000\n",
      "Epoch 45/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 168035.2812\n",
      "Epoch 46/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 167619.2812\n",
      "Epoch 47/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 167214.4062\n",
      "Epoch 48/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 166850.7344\n",
      "Epoch 49/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 166441.9219\n",
      "Epoch 50/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 166054.8125\n",
      "Epoch 51/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 165710.9219\n",
      "Epoch 52/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 165472.7188\n",
      "Epoch 53/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 165026.8438\n",
      "Epoch 54/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 164633.5312\n",
      "Epoch 55/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 164363.7500\n",
      "Epoch 56/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 164061.9844\n",
      "Epoch 57/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 163613.8594\n",
      "Epoch 58/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 163414.6406\n",
      "Epoch 59/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 163114.3906\n",
      "Epoch 60/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 162656.0000\n",
      "Epoch 61/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 162530.0156\n",
      "Epoch 62/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 162069.7500\n",
      "Epoch 63/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 161772.1562\n",
      "Epoch 64/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 161500.0781\n",
      "Epoch 65/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 161110.1406\n",
      "Epoch 66/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 160859.2344\n",
      "Epoch 67/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 160525.5000\n",
      "Epoch 68/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 160243.5625\n",
      "Epoch 69/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159959.3125\n",
      "Epoch 70/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159648.8438\n",
      "Epoch 71/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159403.2812\n",
      "Epoch 72/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159092.6875\n",
      "Epoch 73/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 158854.2969\n",
      "Epoch 74/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 158556.8438\n",
      "Epoch 75/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 158286.2344\n",
      "Epoch 76/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 158021.7344\n",
      "Epoch 77/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 157783.9688\n",
      "Epoch 78/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 157522.9375\n",
      "Epoch 79/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 157253.3438\n",
      "Epoch 80/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 157000.4062\n",
      "Epoch 81/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 156749.9688\n",
      "Epoch 82/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 156500.0781\n",
      "Epoch 83/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 156251.2656\n",
      "Epoch 84/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 156004.3594\n",
      "Epoch 85/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 155789.1562\n",
      "Epoch 86/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 155518.9844\n",
      "Epoch 87/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 155309.6406\n",
      "Epoch 88/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 155065.7031\n",
      "Epoch 89/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 154808.8594\n",
      "Epoch 90/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 154623.8594\n",
      "Epoch 91/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 154334.7188\n",
      "Epoch 92/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 154117.4219\n",
      "Epoch 93/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 153881.3906\n",
      "Epoch 94/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 153634.7344\n",
      "Epoch 95/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 153410.4688\n",
      "Epoch 96/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 153186.8906\n",
      "Epoch 97/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152959.1094\n",
      "Epoch 98/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152730.6406\n",
      "Epoch 99/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152517.7031\n",
      "Epoch 100/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152290.2188\n",
      "Epoch 101/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 152071.2188\n",
      "Epoch 102/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 151845.8906\n",
      "Epoch 103/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 151625.7031\n",
      "Epoch 104/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 151407.6875\n",
      "Epoch 105/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 151193.7969\n",
      "Epoch 106/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 150977.4375\n",
      "Epoch 107/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 150765.0625\n",
      "Epoch 108/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 150562.6094\n",
      "Epoch 109/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 150361.1719\n",
      "Epoch 110/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 150141.4531\n",
      "Epoch 111/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 149941.7969\n",
      "Epoch 112/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 149722.0000\n",
      "Epoch 113/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 149522.3906\n",
      "Epoch 114/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 149310.9062\n",
      "Epoch 115/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 149121.5156\n",
      "Epoch 116/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 148912.9219\n",
      "Epoch 117/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 148711.7188\n",
      "Epoch 118/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 148503.5938\n",
      "Epoch 119/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 148321.0000\n",
      "Epoch 120/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 148107.8906\n",
      "Epoch 121/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 147921.7188\n",
      "Epoch 122/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 147705.1094\n",
      "Epoch 123/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 147522.8125\n",
      "Epoch 124/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 147313.9375\n",
      "Epoch 125/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 147123.9844\n",
      "Epoch 126/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 146922.5938\n",
      "Epoch 127/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 146741.7656\n",
      "Epoch 128/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 146546.2188\n",
      "Epoch 129/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 146354.1250\n",
      "Epoch 130/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 146165.6094\n",
      "Epoch 131/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 145977.6562\n",
      "Epoch 132/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 145792.0938\n",
      "Epoch 133/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 145605.0938\n",
      "Epoch 134/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 145424.4062\n",
      "Epoch 135/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 145237.5312\n",
      "Epoch 136/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 145062.7188\n",
      "Epoch 137/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 144871.5625\n",
      "Epoch 138/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 144696.7500\n",
      "Epoch 139/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 144514.1406\n",
      "Epoch 140/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 144339.0625\n",
      "Epoch 141/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 144159.2344\n",
      "Epoch 142/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 143982.1406\n",
      "Epoch 143/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 143804.5312\n",
      "Epoch 144/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 143631.7812\n",
      "Epoch 145/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 143458.9688\n",
      "Epoch 146/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 143285.4688\n",
      "Epoch 147/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 143117.3438\n",
      "Epoch 148/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 142944.3594\n",
      "Epoch 149/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 142776.7188\n",
      "Epoch 150/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 142605.6406\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have X_train and y_train\n",
    "# Assuming you have lengths, A, levels_left, and levels_right defined\n",
    "\n",
    "## fix seed\n",
    "keras.utils.set_random_seed(99)\n",
    "\n",
    "# Enable eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "layer = 5\n",
    "\n",
    "\n",
    "# Create a sequential model\n",
    "inp = Input(shape=(input_size,))\n",
    "h1_w1 = Dense(512*2*2, activation='relu')(inp)\n",
    "h2_w1  = Dense(512*2, activation='relu')(h1_w1)\n",
    "h3_w1 = Dense(256*2, activation='relu')(h2_w1)\n",
    "h4_w1 = Dense(256, activation='relu')(h3_w1)\n",
    "out = Dense(199, activation='linear')(h4_w1)\n",
    "\n",
    "\n",
    "\n",
    "mdl = Model(inputs=inp, outputs=out)\n",
    "\n",
    "# Compile the model with the custom loss function and an optimizer\n",
    "custom_loss = custom_loss_with_regularization(0.01, mdl.layers[layer].kernel, lengths, A, levels_left, levels_right)\n",
    "\n",
    "mdl.compile(loss=custom_loss, optimizer='adam')\n",
    "\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_epochs = 0\n",
    "best_batch_size = 0\n",
    "#who reads is gay\n",
    "for epochs in [100, 150, 200]:\n",
    "    for batch_size in [16, 32, 64]:\n",
    "        history = mdl.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        loss = history.history['val_loss'][-1] if 'val_loss' in history.history else history.history['loss'][-1]\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_epochs = epochs\n",
    "            best_batch_size = batch_size\n",
    "\n",
    "print(\"Best configuration - Epochs:\", best_epochs, \"Batch Size:\", best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hamza\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:371: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 18s 3s/step - loss: 147113056.0000\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 14s 2s/step - loss: 74258000.0000\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 27s 5s/step - loss: 56694996.0000\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 31s 5s/step - loss: 54838508.0000\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 28s 5s/step - loss: 52153448.0000\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 30s 5s/step - loss: 49646852.0000\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 29s 5s/step - loss: 48119260.0000\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 28s 5s/step - loss: 46615696.0000\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 29s 5s/step - loss: 46070968.0000\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 30s 5s/step - loss: 43951496.0000\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 31s 5s/step - loss: 43323876.0000\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 29s 5s/step - loss: 41800792.0000\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 32s 5s/step - loss: 40283300.0000\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 29s 5s/step - loss: 39502424.0000\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 28s 5s/step - loss: 38635944.0000\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 29s 5s/step - loss: 37641124.0000\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 29s 5s/step - loss: 35480264.0000\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 30s 5s/step - loss: 34361660.0000\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 29s 5s/step - loss: 32185852.0000\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 29s 5s/step - loss: 31242438.0000\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 30s 5s/step - loss: 29036592.0000\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 40s 7s/step - loss: 26344822.0000\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 37s 6s/step - loss: 23327838.0000\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 31s 5s/step - loss: 21196066.0000\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 32s 5s/step - loss: 21050118.0000\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 29s 5s/step - loss: 17380224.0000\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 29s 5s/step - loss: 14591390.0000\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 29s 5s/step - loss: 12283746.0000\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 25s 4s/step - loss: 10165348.0000\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 15s 3s/step - loss: 9037386.0000\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 15s 3s/step - loss: 8255070.0000\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 19s 3s/step - loss: 7556030.5000\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 15s 3s/step - loss: 7023996.0000\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 15s 2s/step - loss: 6642168.0000\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 15s 2s/step - loss: 6523490.0000\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 17s 3s/step - loss: 6330530.0000\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 15s 3s/step - loss: 6900248.5000\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 17s 3s/step - loss: 6962877.0000\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 6265035.5000\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 6007882.0000\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 5709741.0000\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 15s 3s/step - loss: 5350297.5000\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 5167425.5000\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 4959384.0000\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 4831151.0000\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 4690542.0000\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 15s 3s/step - loss: 4548257.0000\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 4301595.0000\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 17s 3s/step - loss: 4335931.0000\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 4361451.0000\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 4313005.5000\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 18s 3s/step - loss: 4280618.0000\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 15s 3s/step - loss: 4186413.0000\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 16s 2s/step - loss: 3932587.5000\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 15s 3s/step - loss: 3792469.7500\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 3694831.2500\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 14s 2s/step - loss: 3525918.0000\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 15s 2s/step - loss: 3478871.5000\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 15s 3s/step - loss: 3229581.7500\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 13s 2s/step - loss: 3067189.2500\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 20s 3s/step - loss: 3067841.7500\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 3101047.2500\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 25s 4s/step - loss: 3015613.5000\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2838410.0000\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 19s 3s/step - loss: 2767025.7500\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 17s 2s/step - loss: 3320683.2500\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 15s 3s/step - loss: 3017828.2500\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 15s 3s/step - loss: 3089944.7500\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 15s 2s/step - loss: 2711453.7500\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 15s 3s/step - loss: 2852022.2500\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 14s 2s/step - loss: 2784046.0000\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 2884131.5000\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 14s 2s/step - loss: 3301586.7500\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 15s 2s/step - loss: 7772372.0000\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 7149338.0000\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 18s 3s/step - loss: 17255174.0000\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 35151312.0000\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 22958002.0000\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 16s 2s/step - loss: 16916158.0000\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 15s 3s/step - loss: 13609913.0000\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 9868452.0000\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 13s 2s/step - loss: 7902086.5000\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 17s 3s/step - loss: 6667865.0000\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 19s 3s/step - loss: 5668504.5000\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 14s 2s/step - loss: 5343362.5000\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 14s 2s/step - loss: 3923410.2500\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 13s 2s/step - loss: 3408295.7500\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 13s 2s/step - loss: 3384914.0000\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 14s 2s/step - loss: 3212983.0000\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 12s 2s/step - loss: 2770578.2500\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 12s 2s/step - loss: 2466116.5000\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 11s 2s/step - loss: 2264462.0000\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 11s 2s/step - loss: 2045176.7500\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 11s 2s/step - loss: 1951621.2500\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 11s 2s/step - loss: 1879988.6250\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 11s 2s/step - loss: 1827715.3750\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 11s 2s/step - loss: 1757111.8750\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 11s 2s/step - loss: 1740262.7500\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 16s 3s/step - loss: 1704374.8750\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 14s 2s/step - loss: 1682735.5000\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have X_train and y_train\n",
    "# Assuming you have lengths, A, levels_left, and levels_right defined\n",
    "\n",
    "best_epochs = 100\n",
    "best_batch_size = 64\n",
    "\n",
    "## fix seed\n",
    "keras.utils.set_random_seed(99)\n",
    "\n",
    "# Enable eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "layer = 5\n",
    "\n",
    "\n",
    "# Create a sequential model\n",
    "inp = Input(shape=(input_size,))\n",
    "h1_w1 = Dense(512*4, activation='relu')(inp)\n",
    "h2_w1  = Dense(512*2, activation='relu')(h1_w1)\n",
    "h3_w1 = Dense(256*2, activation='relu')(h2_w1)\n",
    "h4_w1 = Dense(256, activation='relu')(h3_w1)\n",
    "out = Dense(199, activation='linear')(h4_w1)\n",
    "\n",
    "\n",
    "mdl = Model(inputs=inp, outputs=out)\n",
    "\n",
    "# Compile the model with the custom loss function and an optimizer\n",
    "custom_loss = custom_loss_with_regularization(0.01, mdl.layers[layer].kernel, lengths, A, levels_left, levels_right)\n",
    "\n",
    "mdl.compile(loss=custom_loss, optimizer='adam')\n",
    "\n",
    "# Now train the model with the best configuration\n",
    "history = mdl.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "loss_values = history_dict[\"loss\"]\n",
    "#val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "#plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27.284958046353612"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = np.array(mdl.predict(X_test))#.reshape(-1)\n",
    "calculate_wmape(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### METRIC WMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmape_level(actual_value, forecasted_value, total_ts, lengths):\n",
    "    nb_levels = len(lengths)\n",
    "    wmapes = []\n",
    "    for l in range(nb_levels):\n",
    "        actual_value_ts = actual_value[:, total_ts[l]:total_ts[l+1]]\n",
    "        forecasted_value_ts = forecasted_value[:, total_ts[l]:total_ts[l+1]]\n",
    "        wmapes.append(calculate_wmape(actual_value_ts, forecasted_value_ts))\n",
    "    return wmapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.58189004866522,\n",
       " 15.650902638539955,\n",
       " 26.309774917180185,\n",
       " 34.01127213590421,\n",
       " 61.7787938104619]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### w/o reco \n",
    "wmape_level(y_test, y_predict, total_ts, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.899456963342967,\n",
       " 13.313878145615739,\n",
       " 28.17120917694411,\n",
       " 29.513620469693286,\n",
       " 59.52662547617196]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### with reco\n",
    "wmape_level(y_test, y_predict, total_ts, lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METRIC RMSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "### round to int value of array\n",
    "def round_array(array):\n",
    "    for i in range(len(array)):\n",
    "        array[i] = round(array[i])\n",
    "        if array[i] <= 0:\n",
    "            array[i] = 0\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I have an array of shape (89,5)\n",
    "### create dataframe with predictions\n",
    "def create_df(y_predict, pred_length, data):\n",
    "    ### dataframe with name of columns same as in data_for_model_000\n",
    "    ### create a dataframe based on data, remove last pred_length rows, and add y_predict\n",
    "    ### return dataframe\n",
    "    y_predict_df = y_predict.astype(np.float32)\n",
    "    y_predict_df = pd.DataFrame(y_predict_df)\n",
    "    y_predict_df = y_predict_df\n",
    "    df = data.copy()\n",
    "    for i,col in enumerate(data.columns):\n",
    "        df[col][-(pred_length):] = round_array(y_predict_df[:][i])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_47492\\2594118051.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  array[i] = round(array[i])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_47492\\2594118051.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  array[i] = 0\n"
     ]
    }
   ],
   "source": [
    "data_pred = create_df(y_predict, pred_length, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsse_ts(pred_length, data, data_pred, ts):\n",
    "    H = pred_length\n",
    "    T = data.shape[0] - H\n",
    "    ts_array = data.iloc[:,ts].values\n",
    "    ts_array_pred = data_pred.iloc[:,ts].values\n",
    "    e = (1/H)*np.sum((ts_array[t] - ts_array_pred[t])**2 for t in range(T, T+H))\n",
    "    e_naive = (1/(T-1))*np.sum((ts_array[t] - ts_array[t-1])**2 for t in range(1, T))\n",
    "    return np.sqrt(e/e_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsse_level(pred_length, data, data_pred, total_ts, lengths):\n",
    "    nb_levels = len(lengths)\n",
    "    r_l = [0]*nb_levels\n",
    "    for l in range(nb_levels):\n",
    "        for j in range(total_ts[l], total_ts[l+1]):\n",
    "            #print(l, j)\n",
    "            r_l[l] += (1/lengths[l])*rmsse_ts(pred_length, data, data_pred, j)\n",
    "    #print(r_l)\n",
    "    R = np.mean(r_l)\n",
    "    return r_l, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_47492\\821578998.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  e = (1/H)*np.sum((ts_array[t] - ts_array_pred[t])**2 for t in range(T, T+H))\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_47492\\821578998.py:7: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  e_naive = (1/(T-1))*np.sum((ts_array[t] - ts_array[t-1])**2 for t in range(1, T))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.13479631314886564,\n",
       "  0.43712267171345387,\n",
       "  0.41126196601490544,\n",
       "  0.9194517968562482,\n",
       "  2.6993798939968707],\n",
       " 0.9204025283460687)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### w/o reco\n",
    "rmsse_level(pred_length, data, data_pred, total_ts, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_47492\\821578998.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  e = (1/H)*np.sum((ts_array[t] - ts_array_pred[t])**2 for t in range(T, T+H))\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_47492\\821578998.py:7: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  e_naive = (1/(T-1))*np.sum((ts_array[t] - ts_array[t-1])**2 for t in range(1, T))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.12081836902546132,\n",
       "  0.3962361500274339,\n",
       "  0.44799640003717633,\n",
       "  1.1013321517245236,\n",
       "  2.8067498099834776],\n",
       " 0.9746265761596146)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### w reco\n",
    "rmsse_level(pred_length, data, data_pred, total_ts, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
