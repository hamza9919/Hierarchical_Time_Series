{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE AGG MATRIX AND READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\", index_col=0) # 1 + 1*4 + 1*4*7 + 1*4*7*2\n",
    "agg_mat_df = pd.read_csv(\"agg_mat.csv\", index_col=0) # matrix of aggregated data with bottom time series\n",
    "level0ag = 1\n",
    "level0total = 1\n",
    "level1ag = 4\n",
    "level1total = level0total*level1ag\n",
    "level2ag = 7\n",
    "level2total = level1total*level2ag\n",
    "level3ag = 2\n",
    "level3total = level2total*level3ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [0, level0ag,level0ag*level1ag,level0ag*level1ag*level2ag,level0ag*level1ag*level2ag*level3ag]\n",
    "levels_left = [0, level0total, level0total+level1total, level0total+level1total+level2total]\n",
    "levels_right = [0, level1total, level1total+level2total, level1total+level2total+level3total]\n",
    "nb_ts_levels = [level0total, level1total, level2total, level3total]\n",
    "nb_ts_agg_levels = [level0ag,level1ag,level2ag,level3ag]\n",
    "lengths = nb_ts_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>hol</th>\n",
       "      <th>vfr</th>\n",
       "      <th>bus</th>\n",
       "      <th>oth</th>\n",
       "      <th>nsw-hol</th>\n",
       "      <th>vic-hol</th>\n",
       "      <th>qld-hol</th>\n",
       "      <th>sa-hol</th>\n",
       "      <th>wa-hol</th>\n",
       "      <th>...</th>\n",
       "      <th>qld-oth-city</th>\n",
       "      <th>qld-oth-noncity</th>\n",
       "      <th>sa-oth-city</th>\n",
       "      <th>sa-oth-noncity</th>\n",
       "      <th>wa-oth-city</th>\n",
       "      <th>wa-oth-noncity</th>\n",
       "      <th>tas-oth-city</th>\n",
       "      <th>tas-oth-noncity</th>\n",
       "      <th>nt-oth-city</th>\n",
       "      <th>nt-oth-noncity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1998-03-31</th>\n",
       "      <td>84503</td>\n",
       "      <td>45906</td>\n",
       "      <td>26042</td>\n",
       "      <td>9815</td>\n",
       "      <td>2740</td>\n",
       "      <td>17589</td>\n",
       "      <td>10412</td>\n",
       "      <td>9078</td>\n",
       "      <td>3089</td>\n",
       "      <td>3449</td>\n",
       "      <td>...</td>\n",
       "      <td>431</td>\n",
       "      <td>271</td>\n",
       "      <td>244</td>\n",
       "      <td>73</td>\n",
       "      <td>168</td>\n",
       "      <td>37</td>\n",
       "      <td>76</td>\n",
       "      <td>24</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-06-30</th>\n",
       "      <td>65312</td>\n",
       "      <td>29347</td>\n",
       "      <td>20676</td>\n",
       "      <td>11823</td>\n",
       "      <td>3466</td>\n",
       "      <td>11027</td>\n",
       "      <td>6025</td>\n",
       "      <td>6310</td>\n",
       "      <td>1935</td>\n",
       "      <td>2454</td>\n",
       "      <td>...</td>\n",
       "      <td>669</td>\n",
       "      <td>170</td>\n",
       "      <td>142</td>\n",
       "      <td>221</td>\n",
       "      <td>170</td>\n",
       "      <td>99</td>\n",
       "      <td>36</td>\n",
       "      <td>61</td>\n",
       "      <td>69</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-09-30</th>\n",
       "      <td>72753</td>\n",
       "      <td>32492</td>\n",
       "      <td>20582</td>\n",
       "      <td>13565</td>\n",
       "      <td>6114</td>\n",
       "      <td>8910</td>\n",
       "      <td>5060</td>\n",
       "      <td>11733</td>\n",
       "      <td>1569</td>\n",
       "      <td>3398</td>\n",
       "      <td>...</td>\n",
       "      <td>270</td>\n",
       "      <td>1164</td>\n",
       "      <td>397</td>\n",
       "      <td>315</td>\n",
       "      <td>380</td>\n",
       "      <td>1166</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>150</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-12-31</th>\n",
       "      <td>70880</td>\n",
       "      <td>31813</td>\n",
       "      <td>21613</td>\n",
       "      <td>11478</td>\n",
       "      <td>5976</td>\n",
       "      <td>10658</td>\n",
       "      <td>5481</td>\n",
       "      <td>8109</td>\n",
       "      <td>2270</td>\n",
       "      <td>3561</td>\n",
       "      <td>...</td>\n",
       "      <td>214</td>\n",
       "      <td>535</td>\n",
       "      <td>194</td>\n",
       "      <td>260</td>\n",
       "      <td>410</td>\n",
       "      <td>1139</td>\n",
       "      <td>48</td>\n",
       "      <td>43</td>\n",
       "      <td>172</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-03-31</th>\n",
       "      <td>86893</td>\n",
       "      <td>46793</td>\n",
       "      <td>26947</td>\n",
       "      <td>10027</td>\n",
       "      <td>3126</td>\n",
       "      <td>16152</td>\n",
       "      <td>10958</td>\n",
       "      <td>10047</td>\n",
       "      <td>3023</td>\n",
       "      <td>4287</td>\n",
       "      <td>...</td>\n",
       "      <td>458</td>\n",
       "      <td>557</td>\n",
       "      <td>147</td>\n",
       "      <td>33</td>\n",
       "      <td>162</td>\n",
       "      <td>28</td>\n",
       "      <td>77</td>\n",
       "      <td>60</td>\n",
       "      <td>15</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            total    hol    vfr    bus   oth  nsw-hol  vic-hol  qld-hol  \\\n",
       "1998-03-31  84503  45906  26042   9815  2740    17589    10412     9078   \n",
       "1998-06-30  65312  29347  20676  11823  3466    11027     6025     6310   \n",
       "1998-09-30  72753  32492  20582  13565  6114     8910     5060    11733   \n",
       "1998-12-31  70880  31813  21613  11478  5976    10658     5481     8109   \n",
       "1999-03-31  86893  46793  26947  10027  3126    16152    10958    10047   \n",
       "\n",
       "            sa-hol  wa-hol  ...  qld-oth-city  qld-oth-noncity  sa-oth-city  \\\n",
       "1998-03-31    3089    3449  ...           431              271          244   \n",
       "1998-06-30    1935    2454  ...           669              170          142   \n",
       "1998-09-30    1569    3398  ...           270             1164          397   \n",
       "1998-12-31    2270    3561  ...           214              535          194   \n",
       "1999-03-31    3023    4287  ...           458              557          147   \n",
       "\n",
       "            sa-oth-noncity  wa-oth-city  wa-oth-noncity  tas-oth-city  \\\n",
       "1998-03-31              73          168              37            76   \n",
       "1998-06-30             221          170              99            36   \n",
       "1998-09-30             315          380            1166            32   \n",
       "1998-12-31             260          410            1139            48   \n",
       "1999-03-31              33          162              28            77   \n",
       "\n",
       "            tas-oth-noncity  nt-oth-city  nt-oth-noncity  \n",
       "1998-03-31               24           35               8  \n",
       "1998-06-30               61           69              39  \n",
       "1998-09-30               23          150             338  \n",
       "1998-12-31               43          172             453  \n",
       "1999-03-31               60           15              47  \n",
       "\n",
       "[5 rows x 89 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create global matrix A\n",
    "### create list of number of TS in each level\n",
    "\n",
    "def matrix_per_level(global_Matrix, levels_left, levels_right, l):\n",
    "    return np.array(global_Matrix.iloc[levels_left[l-1]:levels_left[l], levels_right[l-1]:levels_right[l]])\n",
    "\n",
    "def creat_agg_mat(nb_ts_levels, nb_ts_agg_levels):\n",
    "    nb_total_ts = sum(nb_ts_levels)\n",
    "    nb_ts_agg =sum(nb_ts_levels[:len(nb_ts_levels)-1])\n",
    "    global_Matrix = pd.DataFrame(np.zeros((nb_ts_agg, nb_total_ts-1)))\n",
    "    \n",
    "    \n",
    "    for j in range(0,nb_ts_levels[1]):\n",
    "        global_Matrix.iloc[0,j] = 1\n",
    "    i=1\n",
    "    for k in range(nb_ts_levels[1]):\n",
    "        for j in range(4+k*7,4+(k+1)*7):\n",
    "            global_Matrix.iloc[i,j] = 1\n",
    "            #print(i,j)\n",
    "        i+=1\n",
    "    for k in range(nb_ts_levels[2]):\n",
    "        for j in range(4+28+k*2,4+28+(k+1)*2):\n",
    "            global_Matrix.iloc[i,j] = 1\n",
    "            #print(i,j)\n",
    "        i+=1\n",
    "            \n",
    "\n",
    "    return global_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows Ã— 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9   ...   78   79   80   81  \\\n",
       "0   1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "1   0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "5   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "6   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "7   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "8   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "9   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "10  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "11  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "12  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "13  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "14  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "15  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "16  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "17  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "18  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "19  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "20  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "21  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "22  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "23  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "24  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "25  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "26  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "27  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "28  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0  0.0  0.0   \n",
       "29  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0  1.0   \n",
       "30  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "31  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "32  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "     82   83   84   85   86   87  \n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "5   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "6   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "7   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "8   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "10  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "11  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "12  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "13  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "14  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "15  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "16  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "17  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "18  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "19  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "20  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "21  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "22  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "23  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "24  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "25  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "26  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "27  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "28  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "29  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "30  1.0  1.0  0.0  0.0  0.0  0.0  \n",
       "31  0.0  0.0  1.0  1.0  0.0  0.0  \n",
       "32  0.0  0.0  0.0  0.0  1.0  1.0  \n",
       "\n",
       "[33 rows x 88 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = creat_agg_mat(nb_ts_levels, nb_ts_agg_levels)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_per_level(A, levels_left, levels_right, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "### pivot data such as index is the name of columns\n",
    "#data = data.pivot(index='date', columns='symbol', values='close')\n",
    "pivot_df = data.T\n",
    "\n",
    "n_ts = 89\n",
    "n_timepoints = 35\n",
    "n_dates = 15\n",
    "input_size = n_ts*n_dates\n",
    "pred_length = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wmape(actual_values, forecasted_values):\n",
    "    n = len(actual_values)\n",
    "    num = np.sum(np.abs(actual_values - forecasted_values))\n",
    "    den = np.sum(np.abs(actual_values))\n",
    "    wmape = 100*num/den\n",
    "    return wmape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(n_timepoints-n_dates-pred_length+1):\n",
    "    X = pivot_df.iloc[:,i:i+n_dates].T.to_numpy().reshape(1,-1)\n",
    "    X_train.append(X[0])\n",
    "    y = pivot_df.iloc[:,i+n_dates:i+n_dates+1].T.to_numpy()\n",
    "    y_train.append(y[0])\n",
    "\n",
    "for i in range(n_timepoints-n_dates-pred_length+1, n_timepoints-n_dates+1):\n",
    "    X = pivot_df.iloc[:,i:i+n_dates].T.to_numpy().reshape(1,-1)\n",
    "    X_test.append(X[0])\n",
    "    y = pivot_df.iloc[:,i+n_dates:i+n_dates+1].T.to_numpy()\n",
    "    y_test.append(y[0])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 89)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First model without coherency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fix seed\n",
    "keras.utils.set_random_seed(99)\n",
    "\n",
    "# Enable eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Create a sequential model\n",
    "inp1 = Input(shape=(input_size,))\n",
    "h1_w1 = Dense(1000, activation='relu')(inp1)\n",
    "h2_w1  = Dense(512, activation='relu')(h1_w1)\n",
    "h3_w1 = Dense(256, activation='relu')(h2_w1)\n",
    "out1 = Dense(89, activation='linear')(h3_w1)\n",
    "\n",
    "mdl1 = Model(inputs=inp1, outputs=out1)\n",
    "\n",
    "mdl1.compile(loss=\"mse\", optimizer='adam')\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_epochs = 0\n",
    "best_batch_size = 0\n",
    "\n",
    "for epochs in [50, 100, 150, 200]:\n",
    "    for batch_size in [8, 16, 32, 64]:\n",
    "        history = mdl1.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        loss = history.history['val_loss'][-1] if 'val_loss' in history.history else history.history['loss'][-1]\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_epochs = epochs\n",
    "            best_batch_size = batch_size\n",
    "\n",
    "print(\"Best configuration - Epochs:\", best_epochs, \"Batch Size:\", best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 116491848.0000\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 69588584.0000\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 45679132.0000\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 32315704.0000\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 24455436.0000\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 18686240.0000\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 13099886.0000\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8513116.0000\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 5987681.0000\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4798325.0000\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 4329480.5000\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 4367347.5000\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4377060.0000\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 4276201.0000\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 4105110.2500\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3825315.5000\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 3523956.5000\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3306163.0000\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2994062.2500\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2655673.5000\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2301423.0000\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2010428.0000\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1873546.1250\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1701329.8750\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1537429.7500\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1448609.1250\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1332773.8750\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1259974.7500\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1266707.2500\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1257638.7500\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1227079.7500\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1212902.0000\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1182994.6250\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1140835.1250\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1064812.7500\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 992087.0000\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 907620.1250\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 835754.6875\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 758032.5625\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 701025.3750\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 666303.8750\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 632975.9375\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 621151.5625\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 612914.8750\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 606622.7500\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 600582.0625\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 584504.6250\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 567877.1250\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 546972.3750\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 521765.3750\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 493823.3750\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 468355.8125\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 448451.5000\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 428151.6562\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 414489.2812\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 405908.3750\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 400069.1875\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 391728.5312\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 385927.6562\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 380185.6875\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 372940.5312\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 365500.0625\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 357742.8438\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 349628.1562\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 340500.0625\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 332925.1875\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 325518.5625\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 319365.5000\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 313892.5312\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 309167.4688\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 305310.4688\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 301284.1562\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 297592.2188\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 293916.4375\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 289843.4688\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 285579.0312\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 281639.5000\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 278163.6875\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 274643.6875\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 271244.2188\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 268143.4375\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 264945.3125\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 262196.1875\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 259548.1094\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 256980.0156\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 254722.0156\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 252557.1875\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 250321.6562\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 251873.8906\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 252004.4219\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 245873.5000\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 245520.6875\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 241790.2500\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 241104.5000\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 237827.2188\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 236389.3281\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 234603.1406\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 232277.0781\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 230806.1406\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 228836.9219\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 227499.5000\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 225521.2344\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 223857.2812\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 222569.9375\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 220755.3281\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 219156.9844\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 217657.9062\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 216164.0000\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 214809.0938\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 213014.7188\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 211873.8125\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 210332.6406\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 209035.8594\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 207609.4375\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 206252.9844\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 205015.6562\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 203676.0625\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 202321.3438\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 201098.6562\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 199715.5938\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 198637.8594\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 197229.4844\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 196084.5156\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 194813.0781\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 193643.0938\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 192432.8906\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 191263.2344\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 190031.0938\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 188902.6562\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 196442.8906\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 206543.9688\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 197520.4844\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 194312.3281\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 192811.7812\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 190614.2812\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 191371.6406\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 186265.4688\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 196011.2500\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 231105.2188\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 212902.4844\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 202361.1719\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 197799.0781\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 202862.4844\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 194852.8438\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 183542.0625\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 192094.4844\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 181704.0938\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 183635.4688\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 181401.4844\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 175185.6562\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 181254.4688\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 171578.1562\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 172904.9844\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 171605.7344\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 168309.8125\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 167648.9062\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 164503.0625\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 165543.0000\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 161058.9219\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 161075.5156\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 160073.3750\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 156963.1875\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 158055.6094\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 155430.3594\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 153760.6875\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 154686.2812\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 151363.6094\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 151316.3281\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 150403.8438\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 148667.6094\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 148263.2656\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 146635.5625\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 146379.3906\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 144760.8906\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 143963.3281\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 143430.5781\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 141960.0156\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 141226.6719\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 140566.5156\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 139357.5625\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 138515.6719\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 137789.4062\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 136764.7031\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 136031.9062\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 135073.6875\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 134280.4062\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 133532.8906\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 132623.2500\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 131795.6406\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 131120.7656\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 130249.9062\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 129383.1719\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 128772.3594\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 127928.0938\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 127109.6406\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 126444.1719\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 125644.5078\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 124941.3828\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 124191.6953\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 123448.6953\n"
     ]
    }
   ],
   "source": [
    "## fix seed\n",
    "best_epochs = 200\n",
    "best_batch_size = 64\n",
    "keras.utils.set_random_seed(99)\n",
    "\n",
    "# Enable eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Create a sequential model\n",
    "inp1 = Input(shape=(input_size,))\n",
    "h1_w1 = Dense(1000, activation='relu')(inp1)\n",
    "h2_w1  = Dense(512, activation='relu')(h1_w1)\n",
    "h3_w1 = Dense(256, activation='relu')(h2_w1)\n",
    "out1 = Dense(89, activation='linear')(h3_w1)\n",
    "\n",
    "mdl1 = Model(inputs=inp1, outputs=out1)\n",
    "\n",
    "mdl1.compile(loss=\"mse\", optimizer='adam')\n",
    "\n",
    "history1 = mdl1.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAeZUlEQVR4nO3de5RcdZnu8e+TTkhowmVMAkI6VycCESHRBpFgDHrOkCAavM2QablrCAsEQcdEcxTOcljrOKNzkAOIDcbA0ArOQjG6IjooISKD0EBAIgmEkA4tCCFoLhMwF97zx95NKp2q7up07aru3s9nrVpV+7d/u/ab3ZV6at8VEZiZWX4NqnUBZmZWWw4CM7OccxCYmeWcg8DMLOccBGZmOecgMDPLOQeB5Z6kn0s6p9J9e1jDDEntlX5fs3IMrnUBZvtC0taCwXrgr8CudPjCiGgp970iYlYWfc36CweB9UsRMbzjtaR1wKcj4p7O/SQNjoid1azNrL/xpiEbUDo2sUiaL+lPwPck/Y2kn0naIOnP6euGgmmWSfp0+vpcSfdL+kba9zlJs/ax7wRJyyVtkXSPpOsl3Vbmv+PodF5/kbRS0kcKxp0m6Q/p+/5R0hfS9pHpv+0vkl6V9BtJ/j9u3fKHxAaitwJvAcYBc0k+599Lh8cCrwHXdTH9e4DVwEjgX4DvStI+9P0+8BAwArgKOKuc4iUNAX4K/BI4FPgs0CLpyLTLd0k2fx0IHAP8Om3/PNAOjAIOA74M+Boy1q1+GQSSFkl6WdKTZfQdK+leSY9JekLSadWo0WrqDeDKiPhrRLwWERsj4s6I2BYRW4Crgfd3MX1bRNwUEbuAW4DDSb5Yy+4raSxwPPDViNgeEfcDS8qs/0RgOPB/0ml/DfwMmJOO3wFMlnRQRPw5Ih4taD8cGBcROyLiN+GLiVkZ+mUQAIuBmWX2/V/ADyNiKnAmcENWRVmfsSEiXu8YkFQv6TuS2iRtBpYDh0iqKzH9nzpeRMS29OXwHvY9Ani1oA3g+TLrPwJ4PiLeKGhrA0anrz8OnAa0SbpP0nvT9n8F1gC/lLRW0oIy52c51y+DICKWA68Wtkl6m6S7JT2Sbhs9qqM7cFD6+mDghSqWarXR+Vfw54EjgfdExEHA9LS91OaeSngReIuk+oK2MWVO+wIwptP2/bHAHwEi4uGImE2y2egu4Idp+5aI+HxETAQ+DFwh6YO9+2dYHvTLICihGfhsRLwb+AK7f/lfBXwqPUZ7Kcn2VsuXA0n2C/xF0luAK7OeYUS0Aa3AVZL2S3+1f7jMyX8H/DfwRUlDJM1Ip709fa8mSQdHxA5gM+lhs5JOl/S36T6KjvZdRedgVmBABIGk4cBJwH9IWgF8h2RbKSTbVRdHRAPJ6vS/+0iK3LkG2B94BXgQuLtK820C3gtsBP4ZuIPkfIcuRcR24CPALJKabwDOjohVaZezgHXpZq55wKfS9knAPcBW4L+AGyJiWaX+MTZwqb/uS5I0HvhZRBwj6SBgdUQcXqTfSmBmRDyfDq8FToyIl6tasOWepDuAVRGR+RqJWU8MiF/GEbEZeE7SJwGUOC4dvR74YNp+NDAM2FCTQi1XJB2f7rsaJGkmMJtkm75Zn9Ivg0DSD0hWfY9MTx66gGQ1/AJJjwMrSf7TQbKj8DNp+w+Ac31InVXJW4FlJJtqrgUuiojHalqRWRH9dtOQmZlVRr9cIzAzs8rpdxedGzlyZIwfP77WZZiZ9SuPPPLIKxExqti4zIJA0iLgdODliDimyPgmYH46uJVk++nj3b3v+PHjaW1trWitZmYDnaS2UuOy3DS0mK4vA/Ec8P6IOBb4GskJYWZmVmWZrRFExPL0WP9S4x8oGHwQaCjV18zMstNXdhZfAPy81EhJcyW1SmrdsMGnAJiZVVLNdxZLOoUkCE4u1Scimkk3HTU2Nvp4V7M+aseOHbS3t/P6669339kyMWzYMBoaGhgyZEjZ09Q0CCQdC9wMzIqIjbWsxcx6r729nQMPPJDx48dT+l4+lpWIYOPGjbS3tzNhwoSyp6vZpqH0xh0/As6KiKeznFdLC4wfD4MGJc8tZd/W3Mx64vXXX2fEiBEOgRqRxIgRI3q8Rpbl4aM/AGYAI9NLQF8JDAGIiBuBr5Lcwu+G9EOzMyIaK11HSwvMnQvb0tuDtLUlwwBNTZWem5k5BGprX5Z/lkcNzelm/KeBT2c1/w4LF+4OgQ7btiXtDgIzs75z1FBm1q/vWbuZ9V8bN25kypQpTJkyhbe+9a2MHj36zeHt27d3OW1rayuXXnppt/M46aSTKlLrsmXLOP300yvyXr014INg7NietZtZ9VR6/92IESNYsWIFK1asYN68eVx++eVvDu+3337s3Lmz5LSNjY1ce+213c7jgQce6LZPfzPgg+Dqq6G+fs+2+vqk3cxqp2P/XVsbROzef1fpgznOPfdcrrjiCk455RTmz5/PQw89xEknncTUqVM56aSTWL16NbDnL/SrrrqK888/nxkzZjBx4sQ9AmL48OFv9p8xYwaf+MQnOOqoo2hqaqLjas5Lly7lqKOO4uSTT+bSSy/t9pf/q6++yhlnnMGxxx7LiSeeyBNPPAHAfffd9+YazdSpU9myZQsvvvgi06dPZ8qUKRxzzDH85je/6fUyqvl5BFnr2A+wcGGyOWjs2CQEvH/ArLaquf/u6aef5p577qGuro7NmzezfPlyBg8ezD333MOXv/xl7rzzzr2mWbVqFffeey9btmzhyCOP5KKLLtrr2PzHHnuMlStXcsQRRzBt2jR++9vf0tjYyIUXXsjy5cuZMGECc+Z0ubsUgCuvvJKpU6dy11138etf/5qzzz6bFStW8I1vfIPrr7+eadOmsXXrVoYNG0ZzczOnnnoqCxcuZNeuXWzrvBD3wYAPAkg+VP7iN+tbqrn/7pOf/CR1dXUAbNq0iXPOOYdnnnkGSezYsaPoNB/60IcYOnQoQ4cO5dBDD+Wll16ioWHPK+GccMIJb7ZNmTKFdevWMXz4cCZOnPjmcfxz5syhubnrS6ndf//9b4bRBz7wATZu3MimTZuYNm0aV1xxBU1NTXzsYx+joaGB448/nvPPP58dO3ZwxhlnMGXKlN4sGiAHm4bMrG+q5v67Aw444M3XX/nKVzjllFN48skn+elPf1rymPuhQ4e++bqurq7o/oViffblZl/FppHEggULuPnmm3nttdc48cQTWbVqFdOnT2f58uWMHj2as846i1tvvbXH8+vMQWBmNVGr/XebNm1i9OjRACxevLji73/UUUexdu1a1q1bB8Add9zR7TTTp0+nJd05smzZMkaOHMlBBx3Es88+yzvf+U7mz59PY2Mjq1atoq2tjUMPPZTPfOYzXHDBBTz66KO9rtlBYGY10dQEzc0wbhxIyXNzc/abcb/4xS/ypS99iWnTprFr166Kv//+++/PDTfcwMyZMzn55JM57LDDOPjgg7uc5qqrrqK1tZVjjz2WBQsWcMsttwBwzTXXcMwxx3Dcccex//77M2vWLJYtW/bmzuM777yTyy67rNc197t7Fjc2NoZvTGPWNz311FMcffTRtS6j5rZu3crw4cOJCC6++GImTZrE5ZdfXrX5F/s7SHqk1NUbvEZgZlZhN910E1OmTOEd73gHmzZt4sILL6x1SV3KxVFDZmbVdPnll1d1DaC3vEZgZhXV3zY3DzT7svwdBGZWMcOGDWPjxo0OgxrpuB/BsGHDejSdNw2ZWcU0NDTQ3t6ObylbOx13KOsJB4GZVcyQIUN6dGcs6xu8acjMLOccBGZmOecgMDPLOQeBmVnOOQjMzHLOQWBmlnMOAjOznHMQmJnlnIPAzCznHARmZjmXWRBIWiTpZUlPlhgvSddKWiPpCUnvyqoWMzMrLcs1gsXAzC7GzwImpY+5wLczrMXMzErILAgiYjnwahddZgO3RuJB4BBJh2dVj5mZFVfLfQSjgecLhtvTtr1ImiupVVKrL29rZlZZtQwCFWkrejeLiGiOiMaIaBw1alTGZZmZ5Ustg6AdGFMw3AC8UKNazMxyq5ZBsAQ4Oz166ERgU0S8WMN6zMxyKbM7lEn6ATADGCmpHbgSGAIQETcCS4HTgDXANuC8rGoxM7PSMguCiJjTzfgALs5q/mZmVh6fWWxmlnMOAjOznHMQmJnlnIPAzCznHARmZjnnIDAzyzkHgZlZzjkIzMxyzkFgZpZzDgIzs5xzEJiZ5ZyDwMws5xwEZmY55yAwM8s5B4GZWc45CMzMcs5BYGaWcw4CM7OccxCYmeWcg8DMLOccBGZmOecgMDPLOQeBmVnOOQjMzHLOQWBmlnOZBoGkmZJWS1ojaUGR8QdL+qmkxyWtlHRelvWYmdneMgsCSXXA9cAsYDIwR9LkTt0uBv4QEccBM4BvStovq5rMzGxvWa4RnACsiYi1EbEduB2Y3alPAAdKEjAceBXYmWFNZmbWSZZBMBp4vmC4PW0rdB1wNPAC8Hvgsoh4o/MbSZorqVVS64YNG7Kq18wsl7IMAhVpi07DpwIrgCOAKcB1kg7aa6KI5ohojIjGUaNGVbpOM7NcyzII2oExBcMNJL/8C50H/CgSa4DngKMyrMnMzDrJMggeBiZJmpDuAD4TWNKpz3rggwCSDgOOBNZmWJOZmXUyOKs3joidki4BfgHUAYsiYqWkeen4G4GvAYsl/Z5kU9L8iHglq5rMzGxvmQUBQEQsBZZ2arux4PULwN9lWYOZmXXNZxabmeWcg8DMLOccBGZmOecgMDPLOQeBmVnOOQjMzHLOQWBmlnMOAjOznHMQmJnlXK6CoKUFxo+HQYOS55aWWldkZlZ7mV5ioi9paYG5c2HbtmS4rS0ZBmhqql1dZma1lps1goULd4dAh23bknYzszzLTRCsX9+zdjOzvMhNEIwd27N2M7O8yE0QXH011Nfv2VZfn7SbmeVZboKgqQmam2HcOJCS5+Zm7yg2M8vNUUOQfOn7i9/MbE+5WSMwM7PiHARmZjnnIDAzyzkHgZlZzjkIzMxyzkFgZpZzZQWBpAMkDUpfv13SRyQNybY0MzOrhnLXCJYDwySNBn4FnAcszqooMzOrnnKDQBGxDfgY8P8i4qPA5G4nkmZKWi1pjaQFJfrMkLRC0kpJ95VfupmZVUK5ZxZL0nuBJuCCcqaVVAdcD/xPoB14WNKSiPhDQZ9DgBuAmRGxXtKhPazfzMx6qdw1gs8BXwJ+HBErJU0E7u1mmhOANRGxNiK2A7cDszv1+UfgRxGxHiAiXi67cjMzq4iy1ggi4j7gPoB0p/ErEXFpN5ONBp4vGG4H3tOpz9uBIZKWAQcC34qIWzu/kaS5wFyAsb5utJlZRZV71ND3JR0k6QDgD8BqSf/U3WRF2qLT8GDg3cCHgFOBr0h6+14TRTRHRGNENI4aNaqcks3MrEzlbhqaHBGbgTOApcBY4KxupmkHxhQMNwAvFOlzd0T8d0S8QnJ00nFl1mRmZhVQbhAMSc8bOAP4SUTsYO9f9509DEySNEHSfsCZwJJOfX4CvE/SYEn1JJuOniq7ejMz67Vyjxr6DrAOeBxYLmkcsLmrCSJip6RLgF8AdcCidEfzvHT8jRHxlKS7gSeAN4CbI+LJffunmJnZvlBEdz/sS0woDY6InRWup1uNjY3R2tpa7dmamfVrkh6JiMZi48rdWXywpH+T1Jo+vgkcUNEqzcysJsrdR7AI2AL8ffrYDHwvq6LMzKx6yt1H8LaI+HjB8P+WtCKDeszMrMrKXSN4TdLJHQOSpgGvZVOSmZlVU7lrBPOAWyUdnA7/GTgnm5LMzKyayr3ExOPAcZIOSoc3S/ocyWGfZmbWj/XoDmURsTk9wxjgigzqMTOzKuvNrSqLXUvIzMz6md4Ewb6diWZmZn1KdzeX2ULxL3wB+2dSkZmZVVWXQRARB1arEDMzq43ebBoyM7MBwEFgZpZzDgIzs5xzEJiZ5ZyDwMws53IXBC0tMH48DBqUPLe01LoiM7PaKveicwNCSwvMnQvbtiXDbW3JMEBTU+3qMjOrpVytESxcuDsEOmzblrSbmeVVroJg/fqetZuZ5UGugmDs2J61m5nlQa6C4Oqrob5+z7b6+qTdzCyvchUETU3Q3AzjxoGUPDc3e0exmeVbro4aguRL31/8Zma75WqNwMzM9pZpEEiaKWm1pDWSFnTR73hJuyR9Ist6zMxsb5kFgaQ64HpgFjAZmCNpcol+Xwd+kVUtZmZWWpZrBCcAayJibURsB24HZhfp91ngTuDlDGsxM7MSsgyC0cDzBcPtadubJI0GPgrc2NUbSZorqVVS64YNGypeqJlZnmUZBCrS1vn+x9cA8yNiV1dvFBHNEdEYEY2jRo2qVH1mZka2h4+2A2MKhhuAFzr1aQRulwQwEjhN0s6IuCvDuszMrECWQfAwMEnSBOCPwJnAPxZ2iIgJHa8lLQZ+5hAwM6uuzIIgInZKuoTkaKA6YFFErJQ0Lx3f5X4BMzOrjkzPLI6IpcDSTm1FAyAizs2yFjMzK85nFpuZ5ZyDwMws5xwEZmY55yAwM8u53AZBSwuMHw+DBiXPLS21rsjMrDZydz8CSL70587dfSP7trZkGHyvAjPLn1yuESxcuDsEOmzblrSbmeVNLoNg/fqetZuZDWS5DIKxY3vWbmY2kOUyCK6+Gurr92yrr0/azczyJpdB0NQEzc0wbhxIyXNzs3cUm1k+5fKoIUi+9P3Fb2aW0zUCMzPbzUFgZpZzDgIzs5xzEJiZ5ZyDwMws5xwEZmY5l/sg8FVIzSzvcnseAfgqpGZmkPM1Al+F1Mws50Hgq5CameU8CHwVUjOznAeBr0JqZpbzIPBVSM3MMg4CSTMlrZa0RtKCIuObJD2RPh6QdFyW9RTT1ATr1sEbbyTPDgEzy5vMgkBSHXA9MAuYDMyRNLlTt+eA90fEscDXgOas6jEzs+KyXCM4AVgTEWsjYjtwOzC7sENEPBARf04HHwQaMqynJJ9UZmZ5lmUQjAaeLxhuT9tKuQD4ebERkuZKapXUumHDhgqWuPuksrY2iNh9UpnDwMzyIssgUJG2KNpROoUkCOYXGx8RzRHRGBGNo0aNqmCJPqnMzCzLS0y0A2MKhhuAFzp3knQscDMwKyI2ZlhPUT6pzMzyLss1goeBSZImSNoPOBNYUthB0ljgR8BZEfF0hrWU5JPKzCzvMguCiNgJXAL8AngK+GFErJQ0T9K8tNtXgRHADZJWSGrNqp5SfFKZmeWdIoputu+zGhsbo7W1snnR0pLsE1i/PlkTuPpqn09gZgOLpEciorHYuFxfhrpDU5O/+M0sv3J9iQkzM3MQmJnlnoOggM8wNrM88j6ClG9baWZ55TWClM8wNrO8chCkfIaxmeWVgyDlM4zNLK8cBCmfYWxmeeUgSPm2lWaWVz5qqIDPMDazPPIaQSc+l8DM8sZrBAV8LoGZ5ZHXCAr4XAIzyyMHQQGfS2BmeeQgKFDqnIFBg7yvwMwGLgdBgWLnEgDs2pXsK3AYmNlA5CAo0HEuQV3d3uO2bYPLLqt+TWZmWXMQdNLUBG+8UXzcxo3JyWaFj7q65NmHmppZf+UgKKIn1xfqCI22NvjUpxwKZtb/OAiK6O31hRwKZtafOAiKaGqCESMq814OBTPr6xwEJXzrW8WPIOqNwlDwvgUz6yscBCV0HEFUqTWDzortW9iXhwPFzHrLQdCFpiZ45RW47bbsAqG3igXKoEH7Hiw9CZ9y5jNypAPKrK/LNAgkzZS0WtIaSQuKjJeka9PxT0h6V5b17KuOQIjY+3Hbbcm9C/qSiGzfvyN8ypnPxo37vsbTk8CpVrB5Pp5PreaT5dp/ZkEgqQ64HpgFTAbmSJrcqdssYFL6mAt8O6t6stLUBOvW9d1Q6M96Ejiej+cz0OdTuPZf6SsdZLlGcAKwJiLWRsR24HZgdqc+s4FbI/EgcIikwzOsKVOlQkGqaVlmNsBU+qrIWQbBaOD5guH2tK2nfZA0V1KrpNYNGzZUvNAsFIbCG2/07f0MZtb/VPKqyFkGQbHfwZ1XhsrpQ0Q0R0RjRDSOGjWqIsVVW+f9DF5jMLPe6MkVELqTZRC0A2MKhhuAF/ahz4DUeY2h2I7och6FgdJxsbysg2XQoOrMx8yKq6/v/RUQCmUZBA8DkyRNkLQfcCawpFOfJcDZ6dFDJwKbIuLFDGsacAoDZefO3gdLOY9du8qbT283h1UrcDwfz6c/zKdjunHjknOcKnn73MzuWRwROyVdAvwCqAMWRcRKSfPS8TcCS4HTgDXANuC8rOqx6mtq8r2ezfqDTG9eHxFLSb7sC9tuLHgdwMVZ1mBmZl3zmcVmZjnnIDAzyzkHgZlZzjkIzMxyTpH1hTUqTNIGoG0fJh0JvFLhcirBdfVcX63NdfVMX60L+m5tvalrXEQUPSO33wXBvpLUGhGNta6jM9fVc321NtfVM321Lui7tWVVlzcNmZnlnIPAzCzn8hQEzbUuoATX1XN9tTbX1TN9tS7ou7VlUldu9hGYmVlxeVojMDOzIhwEZmY5N+CDQNJMSaslrZG0oMa1jJF0r6SnJK2UdFnafpWkP0pakT5Oq0Ft6yT9Pp1/a9r2Fkn/KemZ9PlvqlzTkQXLZIWkzZI+V4vlJWmRpJclPVnQVnL5SPpS+plbLenUGtT2r5JWSXpC0o8lHZK2j5f0WsGyu7HkG2dTV8m/XbWWWYm67iioaZ2kFWl7NZdXqe+H7D9nETFgHySXv34WmAjsBzwOTK5hPYcD70pfHwg8DUwGrgK+UONltQ4Y2antX4AF6esFwNdr/Lf8EzCuFssLmA68C3iyu+WT/k0fB4YCE9LPYF2Va/s7YHD6+usFtY0v7FeDZVb0b1fNZVasrk7jvwl8tQbLq9T3Q+afs4G+RnACsCYi1kbEduB2YHatiomIFyPi0fT1FuApityjuQ+ZDdySvr4FOKN2pfBB4NmI2JezynstIpYDr3ZqLrV8ZgO3R8RfI+I5kvttnFDN2iLilxGxMx18kOTuf1VVYpmVUrVl1lVdkgT8PfCDLObdlS6+HzL/nA30IBgNPF8w3E4f+eKVNB6YCvwubbokXY1fVO1NMKkAfinpEUlz07bDIr1jXPp8aA3q6nAme/7nrPXygtLLp6997s4Hfl4wPEHSY5Luk/S+GtRT7G/XV5bZ+4CXIuKZgraqL69O3w+Zf84GehAUuxlczY+XlTQcuBP4XERsBr4NvA2YArxIsmpabdMi4l3ALOBiSdNrUENRSm51+hHgP9KmvrC8utJnPneSFgI7gZa06UVgbERMBa4Avi/poCqWVOpv11eW2Rz2/MFR9eVV5PuhZNcibfu0zAZ6ELQDYwqGG4AXalQLAJKGkPyRWyLiRwAR8VJE7IqIN4CbyHAzQikR8UL6/DLw47SGlyQdntZ9OPBytetKzQIejYiX0hprvrxSpZZPn/jcSToHOB1oinSjcroZYWP6+hGS7cpvr1ZNXfztar7MJA0GPgbc0dFW7eVV7PuBKnzOBnoQPAxMkjQh/VV5JrCkVsWk2x+/CzwVEf9W0H54QbePAk92njbjug6QdGDHa5IdjU+SLKtz0m7nAD+pZl0F9viVVuvlVaDU8lkCnClpqKQJwCTgoWoWJmkmMB/4SERsK2gfJakufT0xrW1tFesq9ber+TID/gewKiLaOxqqubxKfT9Qjc9ZNfaG1/IBnEay9/1ZYGGNazmZZNXtCWBF+jgN+Hfg92n7EuDwKtc1keTog8eBlR3LCRgB/Ap4Jn1+Sw2WWT2wETi4oK3qy4skiF4EdpD8Erugq+UDLEw/c6uBWTWobQ3J9uOOz9mNad+Pp3/jx4FHgQ9Xua6Sf7tqLbNidaXti4F5nfpWc3mV+n7I/HPmS0yYmeXcQN80ZGZm3XAQmJnlnIPAzCznHARmZjnnIDAzyzkHgVlK0i7tebXTil2tNr2KZa3OdzDr0uBaF2DWh7wWEVNqXYRZtXmNwKwb6fXpvy7pofTxt2n7OEm/Si+g9itJY9P2w5TcA+Dx9HFS+lZ1km5KrzX/S0n7p/0vlfSH9H1ur9E/03LMQWC22/6dNg39Q8G4zRFxAnAdcE3adh1wa0QcS3JRt2vT9muB+yLiOJLr3q9M2ycB10fEO4C/kJy1Csk15qem7zMvm3+aWWk+s9gsJWlrRAwv0r4O+EBErE0vCvaniBgh6RWSSyTsSNtfjIiRkjYADRHx14L3GA/8Z0RMSofnA0Mi4p8l3Q1sBe4C7oqIrRn/U8324DUCs/JEidel+hTz14LXu9i9j+5DwPXAu4FH0qtgmlWNg8CsPP9Q8Pxf6esHSK5oC9AE3J++/hVwEYCkuq6uXy9pEDAmIu4FvggcAuy1VmKWJf/yMNttf6U3LU/dHREdh5AOlfQ7kh9Pc9K2S4FFkv4J2ACcl7ZfBjRLuoDkl/9FJFe7LKYOuE3SwSQ3Gvm/EfGXCv17zMrifQRm3Uj3ETRGxCu1rsUsC940ZGaWc14jMDPLOa8RmJnlnIPAzCznHARmZjnnIDAzyzkHgZlZzv1/skiEYllHWIUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history1.history\n",
    "history_dict.keys()\n",
    "loss_values = history_dict[\"loss\"]\n",
    "#val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "#plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.185412410099119"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = np.array(mdl1.predict(X_test))#.reshape(-1)\n",
    "calculate_wmape(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL FOR HIERARCHICAL TIME SERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_weights(weight, lengths):\n",
    "    start_idx = 0\n",
    "    result = []\n",
    "    for length in lengths:\n",
    "        sub_array = weight[start_idx:start_idx + length]\n",
    "        result.append(sub_array)\n",
    "        start_idx += length\n",
    "\n",
    "    result = np.array(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom loss function\n",
    "def custom_loss_with_regularization(reg_weight, weight_matrix, lengths, A, levels_left, levels_right):\n",
    "    def loss(y_true, y_pred):\n",
    "        mse_loss = keras.losses.mean_squared_error(y_true, y_pred)  # MSE loss\n",
    "        \n",
    "        def loss_up(weight_matrix):\n",
    "            weights_list = weight_matrix\n",
    "            n, m = weights_list.shape\n",
    "            regularization_loss = 0\n",
    "\n",
    "            for i in range(n):\n",
    "                w = reshape_weights(weights_list[i], lengths)\n",
    "                for l in range(len(w) - 1):\n",
    "                    mat_agg = matrix_per_level(A, levels_left, levels_right, l + 1)\n",
    "                    mat_agg = tf.convert_to_tensor(mat_agg, dtype=tf.float32)  # Convert to TensorFlow tensor\n",
    "                    w_l = w[l]\n",
    "                    w_l2 = w[l + 1]\n",
    "                    w_l_expanded = tf.expand_dims(w_l, axis=0)\n",
    "                    w_l2_expanded = tf.expand_dims(w_l2, axis=-1)\n",
    "                    regularization_loss += tf.reduce_sum(w_l_expanded - tf.transpose(tf.matmul(mat_agg, w_l2_expanded)))\n",
    "            \n",
    "            return regularization_loss\n",
    "        \n",
    "        custom_reg_loss = loss_up(weight_matrix)\n",
    "        total_loss = mse_loss + reg_weight * custom_reg_loss \n",
    "        return total_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hamza\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:371: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration - Epochs: 150 Batch Size: 64\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 189841.3594\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 189240.8125\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 188612.9219\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 188024.0781\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 187426.0156\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 186830.0312\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 186261.5938\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 185659.7500\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 185117.5312\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 184526.6406\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 183991.0000\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 183423.6719\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 182877.8281\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 182342.7344\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 181804.4062\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 181291.2812\n",
      "Epoch 17/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 180766.2188\n",
      "Epoch 18/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 180247.0156\n",
      "Epoch 19/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 179756.5781\n",
      "Epoch 20/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 179236.8438\n",
      "Epoch 21/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 178753.9062\n",
      "Epoch 22/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 178262.3438\n",
      "Epoch 23/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 177765.2500\n",
      "Epoch 24/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 177296.3594\n",
      "Epoch 25/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 176814.6562\n",
      "Epoch 26/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 176344.9062\n",
      "Epoch 27/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 175886.2188\n",
      "Epoch 28/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 175428.8906\n",
      "Epoch 29/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 174977.2656\n",
      "Epoch 30/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 174538.1719\n",
      "Epoch 31/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 174089.9844\n",
      "Epoch 32/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 173640.5781\n",
      "Epoch 33/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 173173.9062\n",
      "Epoch 34/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 172725.3594\n",
      "Epoch 35/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 172275.9375\n",
      "Epoch 36/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 171825.1875\n",
      "Epoch 37/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 171387.1562\n",
      "Epoch 38/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 170945.9844\n",
      "Epoch 39/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 170515.6562\n",
      "Epoch 40/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 170088.9062\n",
      "Epoch 41/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 169658.7500\n",
      "Epoch 42/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 169252.5156\n",
      "Epoch 43/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 168835.7500\n",
      "Epoch 44/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 168414.5000\n",
      "Epoch 45/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 168035.2812\n",
      "Epoch 46/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 167619.2812\n",
      "Epoch 47/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 167214.4062\n",
      "Epoch 48/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 166850.7344\n",
      "Epoch 49/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 166441.9219\n",
      "Epoch 50/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 166054.8125\n",
      "Epoch 51/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 165710.9219\n",
      "Epoch 52/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 165472.7188\n",
      "Epoch 53/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 165026.8438\n",
      "Epoch 54/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 164633.5312\n",
      "Epoch 55/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 164363.7500\n",
      "Epoch 56/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 164061.9844\n",
      "Epoch 57/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 163613.8594\n",
      "Epoch 58/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 163414.6406\n",
      "Epoch 59/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 163114.3906\n",
      "Epoch 60/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 162656.0000\n",
      "Epoch 61/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 162530.0156\n",
      "Epoch 62/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 162069.7500\n",
      "Epoch 63/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 161772.1562\n",
      "Epoch 64/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 161500.0781\n",
      "Epoch 65/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 161110.1406\n",
      "Epoch 66/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 160859.2344\n",
      "Epoch 67/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 160525.5000\n",
      "Epoch 68/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 160243.5625\n",
      "Epoch 69/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159959.3125\n",
      "Epoch 70/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159648.8438\n",
      "Epoch 71/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159403.2812\n",
      "Epoch 72/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159092.6875\n",
      "Epoch 73/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 158854.2969\n",
      "Epoch 74/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 158556.8438\n",
      "Epoch 75/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 158286.2344\n",
      "Epoch 76/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 158021.7344\n",
      "Epoch 77/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 157783.9688\n",
      "Epoch 78/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 157522.9375\n",
      "Epoch 79/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 157253.3438\n",
      "Epoch 80/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 157000.4062\n",
      "Epoch 81/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 156749.9688\n",
      "Epoch 82/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 156500.0781\n",
      "Epoch 83/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 156251.2656\n",
      "Epoch 84/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 156004.3594\n",
      "Epoch 85/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 155789.1562\n",
      "Epoch 86/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 155518.9844\n",
      "Epoch 87/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 155309.6406\n",
      "Epoch 88/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 155065.7031\n",
      "Epoch 89/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 154808.8594\n",
      "Epoch 90/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 154623.8594\n",
      "Epoch 91/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 154334.7188\n",
      "Epoch 92/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 154117.4219\n",
      "Epoch 93/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 153881.3906\n",
      "Epoch 94/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 153634.7344\n",
      "Epoch 95/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 153410.4688\n",
      "Epoch 96/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 153186.8906\n",
      "Epoch 97/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152959.1094\n",
      "Epoch 98/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152730.6406\n",
      "Epoch 99/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152517.7031\n",
      "Epoch 100/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152290.2188\n",
      "Epoch 101/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 152071.2188\n",
      "Epoch 102/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 151845.8906\n",
      "Epoch 103/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 151625.7031\n",
      "Epoch 104/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 151407.6875\n",
      "Epoch 105/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 151193.7969\n",
      "Epoch 106/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 150977.4375\n",
      "Epoch 107/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 150765.0625\n",
      "Epoch 108/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 150562.6094\n",
      "Epoch 109/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 150361.1719\n",
      "Epoch 110/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 150141.4531\n",
      "Epoch 111/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 149941.7969\n",
      "Epoch 112/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 149722.0000\n",
      "Epoch 113/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 149522.3906\n",
      "Epoch 114/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 149310.9062\n",
      "Epoch 115/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 149121.5156\n",
      "Epoch 116/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 148912.9219\n",
      "Epoch 117/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 148711.7188\n",
      "Epoch 118/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 148503.5938\n",
      "Epoch 119/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 148321.0000\n",
      "Epoch 120/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 148107.8906\n",
      "Epoch 121/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 147921.7188\n",
      "Epoch 122/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 147705.1094\n",
      "Epoch 123/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 147522.8125\n",
      "Epoch 124/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 147313.9375\n",
      "Epoch 125/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 147123.9844\n",
      "Epoch 126/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 146922.5938\n",
      "Epoch 127/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 146741.7656\n",
      "Epoch 128/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 146546.2188\n",
      "Epoch 129/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 146354.1250\n",
      "Epoch 130/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 146165.6094\n",
      "Epoch 131/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 145977.6562\n",
      "Epoch 132/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 145792.0938\n",
      "Epoch 133/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 145605.0938\n",
      "Epoch 134/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 145424.4062\n",
      "Epoch 135/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 145237.5312\n",
      "Epoch 136/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 145062.7188\n",
      "Epoch 137/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 144871.5625\n",
      "Epoch 138/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 144696.7500\n",
      "Epoch 139/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 144514.1406\n",
      "Epoch 140/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 144339.0625\n",
      "Epoch 141/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 144159.2344\n",
      "Epoch 142/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 143982.1406\n",
      "Epoch 143/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 143804.5312\n",
      "Epoch 144/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 143631.7812\n",
      "Epoch 145/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 143458.9688\n",
      "Epoch 146/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 143285.4688\n",
      "Epoch 147/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 143117.3438\n",
      "Epoch 148/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 142944.3594\n",
      "Epoch 149/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 142776.7188\n",
      "Epoch 150/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 142605.6406\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have X_train and y_train\n",
    "# Assuming you have lengths, A, levels_left, and levels_right defined\n",
    "\n",
    "## fix seed\n",
    "keras.utils.set_random_seed(99)\n",
    "\n",
    "# Enable eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "layer = 4\n",
    "\n",
    "\n",
    "# Create a sequential model\n",
    "inp = Input(shape=(input_size,))\n",
    "h1_w1 = Dense(1000, activation='relu')(inp)\n",
    "h2_w1  = Dense(512, activation='relu')(h1_w1)\n",
    "h3_w1 = Dense(256, activation='relu')(h2_w1)\n",
    "out = Dense(89, activation='linear')(h3_w1)\n",
    "\n",
    "\n",
    "mdl = Model(inputs=inp, outputs=out)\n",
    "\n",
    "# Compile the model with the custom loss function and an optimizer\n",
    "custom_loss = custom_loss_with_regularization(0.01, mdl.layers[layer].kernel, lengths, A, levels_left, levels_right)\n",
    "\n",
    "mdl.compile(loss=custom_loss, optimizer='adam')\n",
    "\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_epochs = 0\n",
    "best_batch_size = 0\n",
    "#who reads is gay\n",
    "for epochs in [50, 100, 150, 200]:\n",
    "    for batch_size in [8, 16, 32, 64]:\n",
    "        history = mdl.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        loss = history.history['val_loss'][-1] if 'val_loss' in history.history else history.history['loss'][-1]\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_epochs = epochs\n",
    "            best_batch_size = batch_size\n",
    "\n",
    "print(\"Best configuration - Epochs:\", best_epochs, \"Batch Size:\", best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hamza\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hamza\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:371: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 12s 12s/step - loss: 116491848.0000\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 69719776.0000\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 45794348.0000\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 32522240.0000\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 24637928.0000\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 18863046.0000\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 13350252.0000\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 8833686.0000\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 6226208.5000\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 4775714.0000\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 4021475.7500\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 4272371.5000\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 4367563.5000\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 4423062.0000\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 8s 8s/step - loss: 4158261.5000\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 3861613.2500\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 3537262.5000\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 3306153.7500\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 3055539.7500\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 2686096.5000\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 2277923.5000\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 2003821.5000\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 1799780.1250\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1650036.6250\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1531994.7500\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1396510.0000\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1337797.2500\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1310242.7500\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1259012.2500\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 1233043.3750\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1222360.6250\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1201040.2500\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1170141.8750\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1112354.0000\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1035695.0625\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 959400.1250\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 884519.5625\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 4s 4s/step - loss: 816474.7500\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 761542.1250\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 719439.9375\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 676422.1250\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 638611.4375\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 624070.1250\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 619313.7500\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 614665.1250\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 604934.0625\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 586002.3750\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 562108.6250\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 535666.5625\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 514864.8438\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 493959.6250\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 471291.8125\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 450640.0625\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 434051.0312\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 419318.7188\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 410024.8750\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 4s 4s/step - loss: 400477.7188\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 394383.7188\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 389314.5625\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 384592.0312\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 379919.3750\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 372644.4375\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 363982.9688\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 356534.4688\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 349047.0312\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 341683.8125\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 8s 8s/step - loss: 335059.1562\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 8s 8s/step - loss: 329709.8750\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 325759.5000\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 4s 4s/step - loss: 321509.1250\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 317677.7812\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 313776.0625\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 309913.9688\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 305910.2812\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 302276.4062\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 298428.2812\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 294878.6562\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 291204.0625\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 289967.7188\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 287537.9688\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 281677.4688\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 279422.4375\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 274557.0312\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 273092.7188\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 269272.1562\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 267511.6250\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 264319.1875\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 262080.2344\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 259099.0625\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 256722.9688\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 254016.0000\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 251800.2812\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 249436.3906\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 247394.5781\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 244776.5938\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 243123.0000\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 240580.4219\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 239100.3281\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 236756.7656\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 235169.0000\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 233050.7656\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 231402.9219\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 5s 5s/step - loss: 229480.8906\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 227811.5156\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 226003.7812\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 224293.1406\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 222595.4688\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 220929.4219\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 219311.1562\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 217676.1719\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 222131.5625\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 226187.2500\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 217191.2656\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 5s 5s/step - loss: 216283.6406\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 217159.8906\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 4s 4s/step - loss: 210086.0156\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 213941.6875\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 207835.4375\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 207652.0781\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 207176.9844\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 202463.0156\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 204254.6875\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 200400.2812\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 199620.2500\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 198764.0312\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 195894.1875\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 195941.3906\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 193693.2656\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 200256.7344\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 206669.3125\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 202146.6094\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 206893.3281\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 8s 8s/step - loss: 201694.8438\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 196188.8281\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 195823.7344\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 195315.3906\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 187972.4375\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 192517.8438\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 186046.6406\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 185226.9844\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 184841.2500\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 181460.3125\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 180190.8281\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 179878.9219\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 176082.9219\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 177189.0781\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 173683.3594\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 173507.4375\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 172188.4062\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 169996.8281\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 169691.8594\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 168309.7344\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 166249.4062\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 166500.1562\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 164245.2812\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 163344.4062\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 162858.1094\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 161034.8281\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 160225.6406\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 172553.1406\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 211316.7812\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 194550.9844\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 167164.5781\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 189754.8906\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 181734.1719\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 163267.1406\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 178472.8438\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 168904.6875\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 160612.5312\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 168650.7656\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 159345.2344\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 4s 4s/step - loss: 158492.1094\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 158565.3906\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 154606.0000\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 152729.7188\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 153818.4844\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 148218.0625\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 149679.5156\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 168299.3594\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 187266.0781\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 151459.1875\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 169242.2188\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 154753.6719\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 154667.5312\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 157681.7656\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 145199.8906\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 153254.7188\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 143396.4375\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 147046.7188\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 140418.7344\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 143800.2656\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 137375.7344\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 139075.0156\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 136395.1562\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 135335.5625\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 133882.1094\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 132444.1406\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 133138.0625\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 128402.5078\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 131162.7344\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have X_train and y_train\n",
    "# Assuming you have lengths, A, levels_left, and levels_right defined\n",
    "\n",
    "best_epochs = 200\n",
    "best_batch_size = 64\n",
    "\n",
    "## fix seed\n",
    "keras.utils.set_random_seed(99)\n",
    "\n",
    "# Enable eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "layer = 4\n",
    "\n",
    "\n",
    "# Create a sequential model\n",
    "inp = Input(shape=(input_size,))\n",
    "h1_w1 = Dense(1000, activation='relu')(inp)\n",
    "h2_w1  = Dense(512, activation='relu')(h1_w1)\n",
    "h3_w1 = Dense(256, activation='relu')(h2_w1)\n",
    "out = Dense(89, activation='linear')(h3_w1)\n",
    "\n",
    "\n",
    "mdl = Model(inputs=inp, outputs=out)\n",
    "\n",
    "# Compile the model with the custom loss function and an optimizer\n",
    "custom_loss = custom_loss_with_regularization(0.01, mdl.layers[layer].kernel, lengths, A, levels_left, levels_right)\n",
    "\n",
    "mdl.compile(loss=custom_loss, optimizer='adam')\n",
    "\n",
    "# Now train the model with the best configuration\n",
    "history = mdl.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "loss_values = history_dict[\"loss\"]\n",
    "#val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "#plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.250740706848228"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = np.array(mdl.predict(X_test))#.reshape(-1)\n",
    "calculate_wmape(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### METRIC WMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ts = [0,1,5,5+28,5+28+56]\n",
    "def wmape_level(actual_value, forecasted_value, total_ts, lengths):\n",
    "    nb_levels = len(lengths)\n",
    "    wmapes = []\n",
    "    for l in range(nb_levels):\n",
    "        actual_value_ts = actual_value[:, total_ts[l]:total_ts[l+1]]\n",
    "        forecasted_value_ts = forecasted_value[:, total_ts[l]:total_ts[l+1]]\n",
    "        wmapes.append(calculate_wmape(actual_value_ts, forecasted_value_ts))\n",
    "    return wmapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.913537088230108, 7.932196338418551, 14.652043984371439, 20.24387222937637]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### w/o reco \n",
    "wmape_level(y_test, y_predict, total_ts, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.952067470200834, 7.964311903282049, 14.834913219646499, 20.251670234263536]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### with reco\n",
    "wmape_level(y_test, y_predict, total_ts, lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METRIC RMSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### round to int value of array\n",
    "def round_array(array):\n",
    "    for i in range(len(array)):\n",
    "        array[i] = round(array[i])\n",
    "        if array[i] <= 0:\n",
    "            array[i] = 0\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I have an array of shape (89,5)\n",
    "### create dataframe with predictions\n",
    "def create_df(y_predict, pred_length, data):\n",
    "    ### dataframe with name of columns same as in data_for_model_000\n",
    "    ### create a dataframe based on data, remove last pred_length rows, and add y_predict\n",
    "    ### return dataframe\n",
    "    y_predict_df = y_predict.astype(np.float32)\n",
    "    y_predict_df = pd.DataFrame(y_predict_df)\n",
    "    y_predict_df = y_predict_df\n",
    "    df = data.copy()\n",
    "    for i,col in enumerate(data.columns):\n",
    "        df[col][-(pred_length):] = round_array(y_predict_df[:][i])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_26108\\2594118051.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  array[i] = round(array[i])\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_26108\\2594118051.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  array[i] = 0\n"
     ]
    }
   ],
   "source": [
    "data_pred = create_df(y_predict, pred_length, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>hol</th>\n",
       "      <th>vfr</th>\n",
       "      <th>bus</th>\n",
       "      <th>oth</th>\n",
       "      <th>nsw-hol</th>\n",
       "      <th>vic-hol</th>\n",
       "      <th>qld-hol</th>\n",
       "      <th>sa-hol</th>\n",
       "      <th>wa-hol</th>\n",
       "      <th>...</th>\n",
       "      <th>qld-oth-city</th>\n",
       "      <th>qld-oth-noncity</th>\n",
       "      <th>sa-oth-city</th>\n",
       "      <th>sa-oth-noncity</th>\n",
       "      <th>wa-oth-city</th>\n",
       "      <th>wa-oth-noncity</th>\n",
       "      <th>tas-oth-city</th>\n",
       "      <th>tas-oth-noncity</th>\n",
       "      <th>nt-oth-city</th>\n",
       "      <th>nt-oth-noncity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1998-03-31</th>\n",
       "      <td>84503</td>\n",
       "      <td>45906</td>\n",
       "      <td>26042</td>\n",
       "      <td>9815</td>\n",
       "      <td>2740</td>\n",
       "      <td>17589</td>\n",
       "      <td>10412</td>\n",
       "      <td>9078</td>\n",
       "      <td>3089</td>\n",
       "      <td>3449</td>\n",
       "      <td>...</td>\n",
       "      <td>431</td>\n",
       "      <td>271</td>\n",
       "      <td>244</td>\n",
       "      <td>73</td>\n",
       "      <td>168</td>\n",
       "      <td>37</td>\n",
       "      <td>76</td>\n",
       "      <td>24</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-06-30</th>\n",
       "      <td>65312</td>\n",
       "      <td>29347</td>\n",
       "      <td>20676</td>\n",
       "      <td>11823</td>\n",
       "      <td>3466</td>\n",
       "      <td>11027</td>\n",
       "      <td>6025</td>\n",
       "      <td>6310</td>\n",
       "      <td>1935</td>\n",
       "      <td>2454</td>\n",
       "      <td>...</td>\n",
       "      <td>669</td>\n",
       "      <td>170</td>\n",
       "      <td>142</td>\n",
       "      <td>221</td>\n",
       "      <td>170</td>\n",
       "      <td>99</td>\n",
       "      <td>36</td>\n",
       "      <td>61</td>\n",
       "      <td>69</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-09-30</th>\n",
       "      <td>72753</td>\n",
       "      <td>32492</td>\n",
       "      <td>20582</td>\n",
       "      <td>13565</td>\n",
       "      <td>6114</td>\n",
       "      <td>8910</td>\n",
       "      <td>5060</td>\n",
       "      <td>11733</td>\n",
       "      <td>1569</td>\n",
       "      <td>3398</td>\n",
       "      <td>...</td>\n",
       "      <td>270</td>\n",
       "      <td>1164</td>\n",
       "      <td>397</td>\n",
       "      <td>315</td>\n",
       "      <td>380</td>\n",
       "      <td>1166</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>150</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-12-31</th>\n",
       "      <td>70880</td>\n",
       "      <td>31813</td>\n",
       "      <td>21613</td>\n",
       "      <td>11478</td>\n",
       "      <td>5976</td>\n",
       "      <td>10658</td>\n",
       "      <td>5481</td>\n",
       "      <td>8109</td>\n",
       "      <td>2270</td>\n",
       "      <td>3561</td>\n",
       "      <td>...</td>\n",
       "      <td>214</td>\n",
       "      <td>535</td>\n",
       "      <td>194</td>\n",
       "      <td>260</td>\n",
       "      <td>410</td>\n",
       "      <td>1139</td>\n",
       "      <td>48</td>\n",
       "      <td>43</td>\n",
       "      <td>172</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-03-31</th>\n",
       "      <td>86893</td>\n",
       "      <td>46793</td>\n",
       "      <td>26947</td>\n",
       "      <td>10027</td>\n",
       "      <td>3126</td>\n",
       "      <td>16152</td>\n",
       "      <td>10958</td>\n",
       "      <td>10047</td>\n",
       "      <td>3023</td>\n",
       "      <td>4287</td>\n",
       "      <td>...</td>\n",
       "      <td>458</td>\n",
       "      <td>557</td>\n",
       "      <td>147</td>\n",
       "      <td>33</td>\n",
       "      <td>162</td>\n",
       "      <td>28</td>\n",
       "      <td>77</td>\n",
       "      <td>60</td>\n",
       "      <td>15</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            total    hol    vfr    bus   oth  nsw-hol  vic-hol  qld-hol  \\\n",
       "1998-03-31  84503  45906  26042   9815  2740    17589    10412     9078   \n",
       "1998-06-30  65312  29347  20676  11823  3466    11027     6025     6310   \n",
       "1998-09-30  72753  32492  20582  13565  6114     8910     5060    11733   \n",
       "1998-12-31  70880  31813  21613  11478  5976    10658     5481     8109   \n",
       "1999-03-31  86893  46793  26947  10027  3126    16152    10958    10047   \n",
       "\n",
       "            sa-hol  wa-hol  ...  qld-oth-city  qld-oth-noncity  sa-oth-city  \\\n",
       "1998-03-31    3089    3449  ...           431              271          244   \n",
       "1998-06-30    1935    2454  ...           669              170          142   \n",
       "1998-09-30    1569    3398  ...           270             1164          397   \n",
       "1998-12-31    2270    3561  ...           214              535          194   \n",
       "1999-03-31    3023    4287  ...           458              557          147   \n",
       "\n",
       "            sa-oth-noncity  wa-oth-city  wa-oth-noncity  tas-oth-city  \\\n",
       "1998-03-31              73          168              37            76   \n",
       "1998-06-30             221          170              99            36   \n",
       "1998-09-30             315          380            1166            32   \n",
       "1998-12-31             260          410            1139            48   \n",
       "1999-03-31              33          162              28            77   \n",
       "\n",
       "            tas-oth-noncity  nt-oth-city  nt-oth-noncity  \n",
       "1998-03-31               24           35               8  \n",
       "1998-06-30               61           69              39  \n",
       "1998-09-30               23          150             338  \n",
       "1998-12-31               43          172             453  \n",
       "1999-03-31               60           15              47  \n",
       "\n",
       "[5 rows x 89 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsse_ts(pred_length, data, data_pred, ts):\n",
    "    H = pred_length\n",
    "    T = data.shape[0] - H\n",
    "    ts_array = data.iloc[:,ts].values\n",
    "    ts_array_pred = data_pred.iloc[:,ts].values\n",
    "    e = (1/H)*np.sum((ts_array[t] - ts_array_pred[t])**2 for t in range(T, T+H))\n",
    "    e_naive = (1/(T-1))*np.sum((ts_array[t] - ts_array[t-1])**2 for t in range(1, T))\n",
    "    return np.sqrt(e/e_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ts = [0,1,5,5+28,5+28+56]\n",
    "def rmsse_level(pred_length, data, data_pred, total_ts, lengths):\n",
    "    nb_levels = len(lengths)\n",
    "    r_l = [0]*nb_levels\n",
    "    for l in range(nb_levels):\n",
    "        for j in range(total_ts[l], total_ts[l+1]):\n",
    "            #print(l, j)\n",
    "            r_l[l] += (1/lengths[l])*rmsse_ts(pred_length, data, data_pred, j)\n",
    "    #print(r_l)\n",
    "    R = np.mean(r_l)\n",
    "    return r_l, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_26108\\821578998.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  e = (1/H)*np.sum((ts_array[t] - ts_array_pred[t])**2 for t in range(T, T+H))\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_26108\\821578998.py:7: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  e_naive = (1/(T-1))*np.sum((ts_array[t] - ts_array[t-1])**2 for t in range(1, T))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.42064407636231566,\n",
       "  0.4968916138878913,\n",
       "  0.8556967812422688,\n",
       "  0.9449537576887306],\n",
       " 0.6795465572953017)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### w/o reco\n",
    "rmsse_level(pred_length, data, data_pred, total_ts, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_26108\\821578998.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  e = (1/H)*np.sum((ts_array[t] - ts_array_pred[t])**2 for t in range(T, T+H))\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_26108\\821578998.py:7: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  e_naive = (1/(T-1))*np.sum((ts_array[t] - ts_array[t-1])**2 for t in range(1, T))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.42656884699940906,\n",
       "  0.5056206010221502,\n",
       "  0.8764100248686078,\n",
       "  0.9491167806348675],\n",
       " 0.6894290633812586)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### w reco\n",
    "rmsse_level(pred_length, data, data_pred, total_ts, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
