{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import datetime\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import statistics\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from gluonts.model import deepar\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy, GPyOpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\", index_col=0) \n",
    "agg_mat_df = pd.read_csv(\"agg_mat.csv\", index_col=0) # matrix of aggregated data with bottom time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>de</th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "      <th>ja</th>\n",
       "      <th>ru</th>\n",
       "      <th>zh</th>\n",
       "      <th>de_AAC</th>\n",
       "      <th>de_DES</th>\n",
       "      <th>de_MOB</th>\n",
       "      <th>...</th>\n",
       "      <th>zh_DES_AAG_054</th>\n",
       "      <th>zh_DES_AAG_056</th>\n",
       "      <th>zh_DES_AAG_068</th>\n",
       "      <th>zh_DES_AAG_089</th>\n",
       "      <th>zh_DES_AAG_139</th>\n",
       "      <th>zh_MOB_AAG_005</th>\n",
       "      <th>zh_MOB_AAG_028</th>\n",
       "      <th>zh_MOB_AAG_031</th>\n",
       "      <th>zh_MOB_AAG_036</th>\n",
       "      <th>zh_MOB_AAG_138</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>156508</td>\n",
       "      <td>15342</td>\n",
       "      <td>63319</td>\n",
       "      <td>33489</td>\n",
       "      <td>17242</td>\n",
       "      <td>12286</td>\n",
       "      <td>14830</td>\n",
       "      <td>10343</td>\n",
       "      <td>932</td>\n",
       "      <td>4067</td>\n",
       "      <td>...</td>\n",
       "      <td>228</td>\n",
       "      <td>874</td>\n",
       "      <td>329</td>\n",
       "      <td>335</td>\n",
       "      <td>251</td>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>230</td>\n",
       "      <td>262</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02</th>\n",
       "      <td>129902</td>\n",
       "      <td>16782</td>\n",
       "      <td>46894</td>\n",
       "      <td>15613</td>\n",
       "      <td>19981</td>\n",
       "      <td>14283</td>\n",
       "      <td>16349</td>\n",
       "      <td>11767</td>\n",
       "      <td>1127</td>\n",
       "      <td>3888</td>\n",
       "      <td>...</td>\n",
       "      <td>179</td>\n",
       "      <td>855</td>\n",
       "      <td>334</td>\n",
       "      <td>471</td>\n",
       "      <td>316</td>\n",
       "      <td>13</td>\n",
       "      <td>62</td>\n",
       "      <td>287</td>\n",
       "      <td>320</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03</th>\n",
       "      <td>138203</td>\n",
       "      <td>12662</td>\n",
       "      <td>47014</td>\n",
       "      <td>18796</td>\n",
       "      <td>18793</td>\n",
       "      <td>15537</td>\n",
       "      <td>25401</td>\n",
       "      <td>7188</td>\n",
       "      <td>1318</td>\n",
       "      <td>4156</td>\n",
       "      <td>...</td>\n",
       "      <td>200</td>\n",
       "      <td>1028</td>\n",
       "      <td>551</td>\n",
       "      <td>297</td>\n",
       "      <td>513</td>\n",
       "      <td>11</td>\n",
       "      <td>57</td>\n",
       "      <td>226</td>\n",
       "      <td>2184</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>115017</td>\n",
       "      <td>12305</td>\n",
       "      <td>42230</td>\n",
       "      <td>14975</td>\n",
       "      <td>18418</td>\n",
       "      <td>14484</td>\n",
       "      <td>12605</td>\n",
       "      <td>7251</td>\n",
       "      <td>2162</td>\n",
       "      <td>2892</td>\n",
       "      <td>...</td>\n",
       "      <td>309</td>\n",
       "      <td>1208</td>\n",
       "      <td>679</td>\n",
       "      <td>322</td>\n",
       "      <td>415</td>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "      <td>213</td>\n",
       "      <td>423</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>126042</td>\n",
       "      <td>14036</td>\n",
       "      <td>50473</td>\n",
       "      <td>14786</td>\n",
       "      <td>16794</td>\n",
       "      <td>17602</td>\n",
       "      <td>12351</td>\n",
       "      <td>9311</td>\n",
       "      <td>1935</td>\n",
       "      <td>2790</td>\n",
       "      <td>...</td>\n",
       "      <td>303</td>\n",
       "      <td>1319</td>\n",
       "      <td>716</td>\n",
       "      <td>262</td>\n",
       "      <td>351</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>156</td>\n",
       "      <td>309</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-27</th>\n",
       "      <td>147461</td>\n",
       "      <td>11449</td>\n",
       "      <td>45257</td>\n",
       "      <td>17133</td>\n",
       "      <td>48520</td>\n",
       "      <td>11160</td>\n",
       "      <td>13942</td>\n",
       "      <td>6208</td>\n",
       "      <td>1169</td>\n",
       "      <td>4072</td>\n",
       "      <td>...</td>\n",
       "      <td>237</td>\n",
       "      <td>1211</td>\n",
       "      <td>737</td>\n",
       "      <td>104</td>\n",
       "      <td>1828</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>490</td>\n",
       "      <td>136</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-28</th>\n",
       "      <td>152287</td>\n",
       "      <td>12407</td>\n",
       "      <td>43929</td>\n",
       "      <td>17097</td>\n",
       "      <td>52437</td>\n",
       "      <td>10479</td>\n",
       "      <td>15938</td>\n",
       "      <td>5210</td>\n",
       "      <td>1221</td>\n",
       "      <td>5976</td>\n",
       "      <td>...</td>\n",
       "      <td>300</td>\n",
       "      <td>1083</td>\n",
       "      <td>649</td>\n",
       "      <td>78</td>\n",
       "      <td>1158</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>400</td>\n",
       "      <td>117</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-29</th>\n",
       "      <td>137953</td>\n",
       "      <td>11042</td>\n",
       "      <td>51460</td>\n",
       "      <td>23959</td>\n",
       "      <td>22600</td>\n",
       "      <td>10331</td>\n",
       "      <td>18561</td>\n",
       "      <td>5978</td>\n",
       "      <td>1213</td>\n",
       "      <td>3851</td>\n",
       "      <td>...</td>\n",
       "      <td>313</td>\n",
       "      <td>912</td>\n",
       "      <td>466</td>\n",
       "      <td>113</td>\n",
       "      <td>905</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>406</td>\n",
       "      <td>133</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30</th>\n",
       "      <td>113121</td>\n",
       "      <td>10375</td>\n",
       "      <td>44781</td>\n",
       "      <td>14478</td>\n",
       "      <td>17949</td>\n",
       "      <td>9994</td>\n",
       "      <td>15544</td>\n",
       "      <td>5792</td>\n",
       "      <td>1011</td>\n",
       "      <td>3572</td>\n",
       "      <td>...</td>\n",
       "      <td>185</td>\n",
       "      <td>981</td>\n",
       "      <td>411</td>\n",
       "      <td>141</td>\n",
       "      <td>635</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>465</td>\n",
       "      <td>112</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>113839</td>\n",
       "      <td>14854</td>\n",
       "      <td>42342</td>\n",
       "      <td>11835</td>\n",
       "      <td>16716</td>\n",
       "      <td>8737</td>\n",
       "      <td>19355</td>\n",
       "      <td>10692</td>\n",
       "      <td>697</td>\n",
       "      <td>3465</td>\n",
       "      <td>...</td>\n",
       "      <td>187</td>\n",
       "      <td>721</td>\n",
       "      <td>274</td>\n",
       "      <td>153</td>\n",
       "      <td>499</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>555</td>\n",
       "      <td>110</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>366 rows × 199 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Total     de     en     fr     ja     ru     zh  de_AAC  de_DES  \\\n",
       "2016-01-01  156508  15342  63319  33489  17242  12286  14830   10343     932   \n",
       "2016-01-02  129902  16782  46894  15613  19981  14283  16349   11767    1127   \n",
       "2016-01-03  138203  12662  47014  18796  18793  15537  25401    7188    1318   \n",
       "2016-01-04  115017  12305  42230  14975  18418  14484  12605    7251    2162   \n",
       "2016-01-05  126042  14036  50473  14786  16794  17602  12351    9311    1935   \n",
       "...            ...    ...    ...    ...    ...    ...    ...     ...     ...   \n",
       "2016-12-27  147461  11449  45257  17133  48520  11160  13942    6208    1169   \n",
       "2016-12-28  152287  12407  43929  17097  52437  10479  15938    5210    1221   \n",
       "2016-12-29  137953  11042  51460  23959  22600  10331  18561    5978    1213   \n",
       "2016-12-30  113121  10375  44781  14478  17949   9994  15544    5792    1011   \n",
       "2016-12-31  113839  14854  42342  11835  16716   8737  19355   10692     697   \n",
       "\n",
       "            de_MOB  ...  zh_DES_AAG_054  zh_DES_AAG_056  zh_DES_AAG_068  \\\n",
       "2016-01-01    4067  ...             228             874             329   \n",
       "2016-01-02    3888  ...             179             855             334   \n",
       "2016-01-03    4156  ...             200            1028             551   \n",
       "2016-01-04    2892  ...             309            1208             679   \n",
       "2016-01-05    2790  ...             303            1319             716   \n",
       "...            ...  ...             ...             ...             ...   \n",
       "2016-12-27    4072  ...             237            1211             737   \n",
       "2016-12-28    5976  ...             300            1083             649   \n",
       "2016-12-29    3851  ...             313             912             466   \n",
       "2016-12-30    3572  ...             185             981             411   \n",
       "2016-12-31    3465  ...             187             721             274   \n",
       "\n",
       "            zh_DES_AAG_089  zh_DES_AAG_139  zh_MOB_AAG_005  zh_MOB_AAG_028  \\\n",
       "2016-01-01             335             251               6              42   \n",
       "2016-01-02             471             316              13              62   \n",
       "2016-01-03             297             513              11              57   \n",
       "2016-01-04             322             415              22              39   \n",
       "2016-01-05             262             351              13              32   \n",
       "...                    ...             ...             ...             ...   \n",
       "2016-12-27             104            1828              25              22   \n",
       "2016-12-28              78            1158              13              15   \n",
       "2016-12-29             113             905              11              21   \n",
       "2016-12-30             141             635              15              21   \n",
       "2016-12-31             153             499              13              17   \n",
       "\n",
       "            zh_MOB_AAG_031  zh_MOB_AAG_036  zh_MOB_AAG_138  \n",
       "2016-01-01             230             262              66  \n",
       "2016-01-02             287             320              88  \n",
       "2016-01-03             226            2184              99  \n",
       "2016-01-04             213             423              65  \n",
       "2016-01-05             156             309              96  \n",
       "...                    ...             ...             ...  \n",
       "2016-12-27             490             136              69  \n",
       "2016-12-28             400             117              64  \n",
       "2016-12-29             406             133              94  \n",
       "2016-12-30             465             112              99  \n",
       "2016-12-31             555             110              76  \n",
       "\n",
       "[366 rows x 199 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "### pivot data such as index is the name of columns\n",
    "#data = data.pivot(index='date', columns='symbol', values='close')\n",
    "pivot_df = data.T\n",
    "\n",
    "#X_train = pivot_df.iloc[:,:20]\n",
    "#X_val = data.iloc[nb_train:nb_train+nb_val,:]\n",
    "#X_test = pivot_df.iloc[:,8:28]\n",
    "\n",
    "#y_train = pivot_df.iloc[:,20:28]\n",
    "#y_val = data.iloc[nb_train+nb_val,:]\n",
    "y_test = pivot_df.iloc[:,365:]\n",
    "\n",
    "prediction_length = 1\n",
    "pred_length = prediction_length\n",
    "start_date = '2016-01-01'\n",
    "freq = \"1D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "level0total = 1\n",
    "level1total = 6\n",
    "level2total = 6*3\n",
    "level3total = 24\n",
    "level4total = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels_left = [0, level0total, level0total+level1total, level0total+level1total+level2total, level0total+level1total+level2total+level3total]\n",
    "levels_right = [0, level1total, level1total+level2total, level1total+level2total+level3total, level1total+level2total+level3total+level4total]\n",
    "nb_ts_levels = [level0total, level1total, level2total, level3total, level4total]\n",
    "total_ts = [0,level0total,level0total+level1total,level0total+level1total+level2total,level0total+level1total+level2total+level3total, level0total+level1total+level2total+level3total+level4total]\n",
    "lengths = nb_ts_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wmape(actual_values, forecasted_values):\n",
    "    n = len(actual_values)\n",
    "    num = np.sum(np.abs(actual_values - forecasted_values))\n",
    "    den = np.sum(np.abs(actual_values))\n",
    "    wmape = 100*num/den\n",
    "    return wmape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_model = data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepAR_ds(data_for_model, prediction_length, freq, start):\n",
    "    # train dataset: cut the last window of length \"prediction_length\", add \"target\" and \"start\" fields\n",
    "    train_ds = ListDataset([{'target': data_for_model[x][:-prediction_length], 'start': start}\n",
    "                        #'feat_static_cat':feat_static_cat[x].values}\n",
    "                        #'feat_dynamic_cat':[feat_dynamic_cat_month[x][:-prediction_length]]}\n",
    "                        for x in data_for_model.columns],\n",
    "                        freq=freq)\n",
    "    # test dataset: use the whole dataset, add \"target\" and \"start\" fields\n",
    "    test_ds = ListDataset([{'target': data_for_model[x].values, 'start': start}\n",
    "                        #'feat_static_cat':feat_static_cat[x].values}\n",
    "                        #'feat_dynamic_cat':[feat_dynamic_cat_month[x].values]}\n",
    "                        for x in data_for_model.columns],\n",
    "                        freq=freq)\n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepAR_fit(train_ds, test_ds, prediction_length, freq, learning_rate, cell_type, num_layers, num_cells, num_epochs, num_batches_per_epoch):\n",
    "    \n",
    "    trainer = Trainer(epochs=num_epochs, learning_rate=learning_rate,num_batches_per_epoch=num_batches_per_epoch)\n",
    "    \n",
    "    estimator = deepar.DeepAREstimator(\n",
    "        freq=freq, prediction_length=prediction_length, trainer=trainer, cell_type=cell_type,\n",
    "        num_layers=num_layers,num_cells=num_cells)\n",
    "    \n",
    "    predictor = estimator.train(training_data=train_ds)\n",
    "    \n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepAR_predict(predictor, test_ds):\n",
    "\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds,  # test dataset\n",
    "        predictor=predictor,  # predictor\n",
    "        num_samples=100,  # number of sample paths we want for evaluation\n",
    "    )\n",
    "    forecasts = list(forecast_it)\n",
    "    tss = list(ts_it)\n",
    "    \n",
    "    return forecasts,tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deepAR(data_for_model, prediction_length, freq, start, learning_rate, cell_type, num_layers, num_cells, num_epochs, num_batches_per_epoch):\n",
    "    train_ds, test_ds = deepAR_ds(data_for_model, prediction_length, freq, start)\n",
    "    predictor = deepAR_fit(train_ds, test_ds, prediction_length, freq, learning_rate, cell_type, num_layers, num_cells, num_epochs, num_batches_per_epoch)\n",
    "    forecasts,tss = deepAR_predict(predictor, test_ds)\n",
    "    return forecasts,tss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deepAR_FT(data_for_model, prediction_length, freq, start, learning_rate, cell_type, num_layers, num_cells, num_epochs, num_batches_per_epoch):\n",
    "    train_ds, test_ds = deepAR_ds(data_for_model, prediction_length, freq, start)\n",
    "    predictor = deepAR_fit(train_ds, test_ds, prediction_length, freq, learning_rate, cell_type, num_layers, num_cells, num_epochs, num_batches_per_epoch)\n",
    "    forecasts,tss = deepAR_predict(predictor, test_ds)\n",
    "    evaluator = Evaluator()\n",
    "    agg_metrics, item_metrics = evaluator(tss, forecasts)\n",
    "    return agg_metrics['RMSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_on_metric(data_for_model, prediction_length, freq, start):\n",
    "    # bounds for hyper-parameters\n",
    "    # the bounds dict should be in order of continuous type and then discrete type\n",
    "    bounds = [{'name': 'learning_rate', 'type': 'discrete',  'domain': (0.001, 0.05, 0.01)},\n",
    "              {'name': 'num_layers', 'type': 'discrete',    'domain': (3, 4, 5)},\n",
    "              {'name': 'num_cells', 'type': 'discrete',    'domain': (50,60)},\n",
    "              {'name': 'num_batches_per_epoch', 'type': 'discrete',    'domain': (5, 10, 32)},\n",
    "              {'name': 'epochs', 'type': 'discrete', 'domain': (10, 50, 100, 150)}\n",
    "              #{'name': 'cell_type', 'type': 'discrete', 'domain': (\"lstm\", \"gru\")}               \n",
    "              ]\n",
    "\n",
    "    def f(x):\n",
    "        print(x)\n",
    "        evaluation = run_deepAR_FT(\n",
    "            data_for_model, prediction_length, freq, start,\n",
    "            learning_rate = float(x[:,0]),  \n",
    "            cell_type= \"lstm\",\n",
    "            num_layers= int(x[:,1]), \n",
    "            num_cells= int(x[:,2]),\n",
    "            num_batches_per_epoch= int(x[:,3]), \n",
    "            num_epochs=  int(x[:,4]))\n",
    "        print(\"LOSS:\\t{0}\".format(evaluation))\n",
    "        print(evaluation)\n",
    "        return evaluation\n",
    "\n",
    "    opt_transformer = GPyOpt.methods.BayesianOptimization(f=f, domain=bounds)\n",
    "\n",
    "    opt_transformer.run_optimization(max_iter=10)\n",
    "\n",
    "    print(\"RESULTS:\")\n",
    "    print(opt_transformer.x_opt) \n",
    "    print(opt_transformer.fx_opt)\n",
    "\n",
    "    return opt_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.e-03 4.e+00 6.e+01 1.e+01 1.e+01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 14.12it/s, epoch=1/10, avg_epoch_loss=7.83]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.72it/s, epoch=2/10, avg_epoch_loss=7.57]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.94it/s, epoch=3/10, avg_epoch_loss=7.49]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.79it/s, epoch=4/10, avg_epoch_loss=7.58]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.79it/s, epoch=5/10, avg_epoch_loss=7.21]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.99it/s, epoch=6/10, avg_epoch_loss=7.41]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.01it/s, epoch=7/10, avg_epoch_loss=7.26]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.16it/s, epoch=8/10, avg_epoch_loss=7.59]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.23it/s, epoch=9/10, avg_epoch_loss=7.4]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.13it/s, epoch=10/10, avg_epoch_loss=7.44]\n",
      "Running evaluation: 89it [00:00, 181.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:\t1249.660653872944\n",
      "1249.660653872944\n",
      "[[5.0e-02 4.0e+00 6.0e+01 3.2e+01 5.0e+01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 13.89it/s, epoch=1/50, avg_epoch_loss=10.3]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.50it/s, epoch=2/50, avg_epoch_loss=8.01]\n",
      "100%|██████████| 32/32 [00:03<00:00,  9.06it/s, epoch=3/50, avg_epoch_loss=7.67]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.08it/s, epoch=4/50, avg_epoch_loss=7.64]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.47it/s, epoch=5/50, avg_epoch_loss=7.81]\n",
      "100%|██████████| 32/32 [00:03<00:00,  9.32it/s, epoch=6/50, avg_epoch_loss=7.95]\n",
      "100%|██████████| 32/32 [00:02<00:00, 10.88it/s, epoch=7/50, avg_epoch_loss=7.6]\n",
      "100%|██████████| 32/32 [00:03<00:00,  9.48it/s, epoch=8/50, avg_epoch_loss=7.76]\n",
      "100%|██████████| 32/32 [00:04<00:00,  6.97it/s, epoch=9/50, avg_epoch_loss=7.48]\n",
      "100%|██████████| 32/32 [00:05<00:00,  6.28it/s, epoch=10/50, avg_epoch_loss=7.64]\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.58it/s, epoch=11/50, avg_epoch_loss=7.63]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.12it/s, epoch=12/50, avg_epoch_loss=7.54]\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.53it/s, epoch=13/50, avg_epoch_loss=7.74]\n",
      "100%|██████████| 32/32 [00:06<00:00,  5.24it/s, epoch=14/50, avg_epoch_loss=7.49]\n",
      "100%|██████████| 32/32 [00:06<00:00,  5.23it/s, epoch=15/50, avg_epoch_loss=7.44]\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.49it/s, epoch=16/50, avg_epoch_loss=7.49]\n",
      "100%|██████████| 32/32 [00:04<00:00,  6.98it/s, epoch=17/50, avg_epoch_loss=7.68]\n",
      "100%|██████████| 32/32 [00:06<00:00,  5.02it/s, epoch=18/50, avg_epoch_loss=7.58]\n",
      "100%|██████████| 32/32 [00:06<00:00,  5.01it/s, epoch=19/50, avg_epoch_loss=7.39]\n",
      "100%|██████████| 32/32 [00:03<00:00,  9.36it/s, epoch=20/50, avg_epoch_loss=7.66]\n",
      "100%|██████████| 32/32 [00:03<00:00, 10.20it/s, epoch=21/50, avg_epoch_loss=7.78]\n",
      "100%|██████████| 32/32 [00:06<00:00,  5.13it/s, epoch=22/50, avg_epoch_loss=7.56]\n",
      "100%|██████████| 32/32 [00:04<00:00,  6.70it/s, epoch=23/50, avg_epoch_loss=7.47]\n",
      "100%|██████████| 32/32 [00:04<00:00,  6.79it/s, epoch=24/50, avg_epoch_loss=7.43]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.00it/s, epoch=25/50, avg_epoch_loss=7.63]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.40it/s, epoch=26/50, avg_epoch_loss=7.71]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.31it/s, epoch=27/50, avg_epoch_loss=7.3]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.19it/s, epoch=28/50, avg_epoch_loss=7.66]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.17it/s, epoch=29/50, avg_epoch_loss=7.39]\n",
      "100%|██████████| 32/32 [00:06<00:00,  5.31it/s, epoch=30/50, avg_epoch_loss=7.61]\n",
      "100%|██████████| 32/32 [00:01<00:00, 16.51it/s, epoch=31/50, avg_epoch_loss=8.55]\n",
      "100%|██████████| 32/32 [00:03<00:00, 10.04it/s, epoch=32/50, avg_epoch_loss=7.77]\n",
      "100%|██████████| 32/32 [00:03<00:00,  9.98it/s, epoch=33/50, avg_epoch_loss=7.48]\n",
      "100%|██████████| 32/32 [00:05<00:00,  6.22it/s, epoch=34/50, avg_epoch_loss=7.52]\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.85it/s, epoch=35/50, avg_epoch_loss=7.61]\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.82it/s, epoch=36/50, avg_epoch_loss=7.5]\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.77it/s, epoch=37/50, avg_epoch_loss=7.36]\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.85it/s, epoch=38/50, avg_epoch_loss=7.37]\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.57it/s, epoch=39/50, avg_epoch_loss=7.19]\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.83it/s, epoch=40/50, avg_epoch_loss=7.25]\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.95it/s, epoch=41/50, avg_epoch_loss=7.2]\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.93it/s, epoch=42/50, avg_epoch_loss=7.37]\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.79it/s, epoch=43/50, avg_epoch_loss=7.32]\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.43it/s, epoch=44/50, avg_epoch_loss=7.24]\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.70it/s, epoch=45/50, avg_epoch_loss=7.3]\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.43it/s, epoch=46/50, avg_epoch_loss=7.22]\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.37it/s, epoch=47/50, avg_epoch_loss=7.16]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.44it/s, epoch=48/50, avg_epoch_loss=7.22]\n",
      "100%|██████████| 32/32 [00:02<00:00, 11.24it/s, epoch=49/50, avg_epoch_loss=7.2]\n",
      "100%|██████████| 32/32 [00:03<00:00,  9.80it/s, epoch=50/50, avg_epoch_loss=7.36]\n",
      "Running evaluation: 89it [00:00, 126.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:\t907.2551228836478\n",
      "907.2551228836478\n",
      "[[1.e-02 5.e+00 6.e+01 1.e+01 1.e+01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.73it/s, epoch=1/10, avg_epoch_loss=9.97]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.60it/s, epoch=2/10, avg_epoch_loss=7.65]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.55it/s, epoch=3/10, avg_epoch_loss=7.4]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.02it/s, epoch=4/10, avg_epoch_loss=7.57]\n",
      "100%|██████████| 10/10 [00:01<00:00,  9.92it/s, epoch=5/10, avg_epoch_loss=7.22]\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.87it/s, epoch=6/10, avg_epoch_loss=7.51]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.70it/s, epoch=7/10, avg_epoch_loss=7.56]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17.68it/s, epoch=8/10, avg_epoch_loss=7.48]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17.90it/s, epoch=9/10, avg_epoch_loss=7.3]\n",
      "100%|██████████| 10/10 [00:00<00:00, 16.86it/s, epoch=10/10, avg_epoch_loss=7.3]\n",
      "Running evaluation: 89it [00:00, 166.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:\t1193.92475486046\n",
      "1193.92475486046\n",
      "[[1.e-02 3.e+00 6.e+01 1.e+01 1.e+01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.78it/s, epoch=1/10, avg_epoch_loss=8.43]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17.03it/s, epoch=2/10, avg_epoch_loss=7.6]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.23it/s, epoch=3/10, avg_epoch_loss=7.37]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.32it/s, epoch=4/10, avg_epoch_loss=7.47]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.12it/s, epoch=5/10, avg_epoch_loss=7.32]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.17it/s, epoch=6/10, avg_epoch_loss=7.28]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.30it/s, epoch=7/10, avg_epoch_loss=7.41]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.94it/s, epoch=8/10, avg_epoch_loss=7.22]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.82it/s, epoch=9/10, avg_epoch_loss=7.3]\n",
      "100%|██████████| 10/10 [00:00<00:00, 22.12it/s, epoch=10/10, avg_epoch_loss=7.18]\n",
      "Running evaluation: 89it [00:01, 88.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:\t852.5637959111086\n",
      "852.5637959111086\n",
      "[[1.e-03 5.e+00 6.e+01 5.e+00 5.e+01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.78it/s, epoch=1/50, avg_epoch_loss=7.83]\n",
      "100%|██████████| 5/5 [00:00<00:00, 17.17it/s, epoch=2/50, avg_epoch_loss=7.76]\n",
      "100%|██████████| 5/5 [00:00<00:00, 16.31it/s, epoch=3/50, avg_epoch_loss=7.6]\n",
      "100%|██████████| 5/5 [00:00<00:00, 14.55it/s, epoch=4/50, avg_epoch_loss=7.57]\n",
      "100%|██████████| 5/5 [00:00<00:00,  7.63it/s, epoch=5/50, avg_epoch_loss=7.45]\n",
      "100%|██████████| 5/5 [00:00<00:00,  9.18it/s, epoch=6/50, avg_epoch_loss=7.37]\n",
      "100%|██████████| 5/5 [00:00<00:00,  9.14it/s, epoch=7/50, avg_epoch_loss=7.56]\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.89it/s, epoch=8/50, avg_epoch_loss=7.67]\n",
      "100%|██████████| 5/5 [00:00<00:00, 16.97it/s, epoch=9/50, avg_epoch_loss=7.28]\n",
      "100%|██████████| 5/5 [00:00<00:00, 16.74it/s, epoch=10/50, avg_epoch_loss=7.36]\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.29it/s, epoch=11/50, avg_epoch_loss=7.47]\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.54it/s, epoch=12/50, avg_epoch_loss=7.21]\n",
      "100%|██████████| 5/5 [00:00<00:00, 17.55it/s, epoch=13/50, avg_epoch_loss=7.41]\n",
      "100%|██████████| 5/5 [00:00<00:00, 18.03it/s, epoch=14/50, avg_epoch_loss=7.39]\n",
      "100%|██████████| 5/5 [00:00<00:00, 17.64it/s, epoch=15/50, avg_epoch_loss=7.31]\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.68it/s, epoch=16/50, avg_epoch_loss=7.35]\n",
      "100%|██████████| 5/5 [00:00<00:00,  9.60it/s, epoch=17/50, avg_epoch_loss=7.29]\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.54it/s, epoch=18/50, avg_epoch_loss=7.26]\n",
      "100%|██████████| 5/5 [00:00<00:00, 18.07it/s, epoch=19/50, avg_epoch_loss=7.17]\n",
      "100%|██████████| 5/5 [00:00<00:00, 17.51it/s, epoch=20/50, avg_epoch_loss=7.33]\n",
      "100%|██████████| 5/5 [00:00<00:00, 15.61it/s, epoch=21/50, avg_epoch_loss=7.49]\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.56it/s, epoch=22/50, avg_epoch_loss=7.36]\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.54it/s, epoch=23/50, avg_epoch_loss=7.27]\n",
      "100%|██████████| 5/5 [00:00<00:00,  9.42it/s, epoch=24/50, avg_epoch_loss=7.59]\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.42it/s, epoch=25/50, avg_epoch_loss=7.42]\n",
      "100%|██████████| 5/5 [00:00<00:00, 16.51it/s, epoch=26/50, avg_epoch_loss=7.34]\n",
      "100%|██████████| 5/5 [00:00<00:00, 16.68it/s, epoch=27/50, avg_epoch_loss=7.36]\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.53it/s, epoch=28/50, avg_epoch_loss=7.4]\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.32it/s, epoch=29/50, avg_epoch_loss=7.33]\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.76it/s, epoch=30/50, avg_epoch_loss=7.34]\n",
      "100%|██████████| 5/5 [00:00<00:00,  9.25it/s, epoch=31/50, avg_epoch_loss=7.06]\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.36it/s, epoch=32/50, avg_epoch_loss=7.43]\n",
      "100%|██████████| 5/5 [00:00<00:00, 10.21it/s, epoch=33/50, avg_epoch_loss=7.09]\n",
      "100%|██████████| 5/5 [00:00<00:00, 17.44it/s, epoch=34/50, avg_epoch_loss=7.08]\n",
      "100%|██████████| 5/5 [00:00<00:00, 17.14it/s, epoch=35/50, avg_epoch_loss=7.16]\n",
      "100%|██████████| 5/5 [00:00<00:00, 16.14it/s, epoch=36/50, avg_epoch_loss=7.03]\n",
      "100%|██████████| 5/5 [00:00<00:00, 18.78it/s, epoch=37/50, avg_epoch_loss=7.16]\n",
      "100%|██████████| 5/5 [00:00<00:00, 16.65it/s, epoch=38/50, avg_epoch_loss=7.24]\n",
      "100%|██████████| 5/5 [00:00<00:00, 16.71it/s, epoch=39/50, avg_epoch_loss=7.18]\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.91it/s, epoch=40/50, avg_epoch_loss=7.27]\n",
      "100%|██████████| 5/5 [00:00<00:00, 12.41it/s, epoch=41/50, avg_epoch_loss=7.09]\n",
      "100%|██████████| 5/5 [00:00<00:00, 10.10it/s, epoch=42/50, avg_epoch_loss=7.47]\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.24it/s, epoch=43/50, avg_epoch_loss=7.3]\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.39it/s, epoch=44/50, avg_epoch_loss=7.25]\n",
      "100%|██████████| 5/5 [00:00<00:00,  9.36it/s, epoch=45/50, avg_epoch_loss=7.06]\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.75it/s, epoch=46/50, avg_epoch_loss=7.27]\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.72it/s, epoch=47/50, avg_epoch_loss=7.28]\n",
      "100%|██████████| 5/5 [00:00<00:00, 17.14it/s, epoch=48/50, avg_epoch_loss=7.2]\n",
      "100%|██████████| 5/5 [00:00<00:00, 16.13it/s, epoch=49/50, avg_epoch_loss=7.08]\n",
      "100%|██████████| 5/5 [00:00<00:00, 16.18it/s, epoch=50/50, avg_epoch_loss=7.27]\n",
      "Running evaluation: 89it [00:00, 128.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:\t1211.3238779935803\n",
      "1211.3238779935803\n",
      "[[5.e-02 3.e+00 6.e+01 1.e+01 1.e+01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18.07it/s, epoch=1/10, avg_epoch_loss=11]\n",
      "100%|██████████| 10/10 [00:00<00:00, 22.62it/s, epoch=2/10, avg_epoch_loss=9.3]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.73it/s, epoch=3/10, avg_epoch_loss=8.14]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.27it/s, epoch=4/10, avg_epoch_loss=7.87]\n",
      "100%|██████████| 10/10 [00:00<00:00, 23.32it/s, epoch=5/10, avg_epoch_loss=7.66]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17.84it/s, epoch=6/10, avg_epoch_loss=7.59]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.14it/s, epoch=7/10, avg_epoch_loss=7.46]\n",
      "100%|██████████| 10/10 [00:00<00:00, 23.82it/s, epoch=8/10, avg_epoch_loss=7.5]\n",
      "100%|██████████| 10/10 [00:00<00:00, 23.88it/s, epoch=9/10, avg_epoch_loss=7.45]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.77it/s, epoch=10/10, avg_epoch_loss=7.47]\n",
      "Running evaluation: 89it [00:00, 193.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:\t1447.6961896917153\n",
      "1447.6961896917153\n",
      "[[5.0e-02 4.0e+00 6.0e+01 3.2e+01 5.0e+01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 13.04it/s, epoch=1/50, avg_epoch_loss=9.42]\n",
      "100%|██████████| 32/32 [00:02<00:00, 13.54it/s, epoch=2/50, avg_epoch_loss=7.54]\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.30it/s, epoch=3/50, avg_epoch_loss=7.57]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.10it/s, epoch=4/50, avg_epoch_loss=7.56]\n",
      "100%|██████████| 32/32 [00:02<00:00, 12.86it/s, epoch=5/50, avg_epoch_loss=7.64]\n",
      "100%|██████████| 32/32 [00:02<00:00, 13.71it/s, epoch=6/50, avg_epoch_loss=7.32]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.25it/s, epoch=7/50, avg_epoch_loss=7.62]\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.32it/s, epoch=8/50, avg_epoch_loss=7.48]\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.11it/s, epoch=9/50, avg_epoch_loss=7.47]\n",
      "100%|██████████| 32/32 [00:02<00:00, 13.39it/s, epoch=10/50, avg_epoch_loss=7.42]\n",
      "100%|██████████| 32/32 [00:02<00:00, 10.99it/s, epoch=11/50, avg_epoch_loss=7.5]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.05it/s, epoch=12/50, avg_epoch_loss=7.57]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.67it/s, epoch=13/50, avg_epoch_loss=7.4]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.95it/s, epoch=14/50, avg_epoch_loss=7.55]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.04it/s, epoch=15/50, avg_epoch_loss=7.33]\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.49it/s, epoch=16/50, avg_epoch_loss=7.51]\n",
      "100%|██████████| 32/32 [00:03<00:00, 10.59it/s, epoch=17/50, avg_epoch_loss=7.21]\n",
      "100%|██████████| 32/32 [00:02<00:00, 11.20it/s, epoch=18/50, avg_epoch_loss=7.21]\n",
      "100%|██████████| 32/32 [00:02<00:00, 13.78it/s, epoch=19/50, avg_epoch_loss=7.23]\n",
      "100%|██████████| 32/32 [00:02<00:00, 13.30it/s, epoch=20/50, avg_epoch_loss=7.29]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.34it/s, epoch=21/50, avg_epoch_loss=7.18]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.12it/s, epoch=22/50, avg_epoch_loss=7.31]\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.73it/s, epoch=23/50, avg_epoch_loss=7.28]\n",
      "100%|██████████| 32/32 [00:02<00:00, 13.36it/s, epoch=24/50, avg_epoch_loss=7.16]\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.09it/s, epoch=25/50, avg_epoch_loss=7.2]\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.29it/s, epoch=26/50, avg_epoch_loss=7.29]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.11it/s, epoch=27/50, avg_epoch_loss=7.36]\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.54it/s, epoch=28/50, avg_epoch_loss=7.33]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.24it/s, epoch=29/50, avg_epoch_loss=7.15]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.15it/s, epoch=30/50, avg_epoch_loss=7.2]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.31it/s, epoch=31/50, avg_epoch_loss=7.18]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.31it/s, epoch=32/50, avg_epoch_loss=7.39]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.67it/s, epoch=33/50, avg_epoch_loss=7.33]\n",
      "100%|██████████| 32/32 [00:03<00:00, 10.18it/s, epoch=34/50, avg_epoch_loss=7.22]\n",
      "100%|██████████| 32/32 [00:02<00:00, 13.85it/s, epoch=35/50, avg_epoch_loss=7.18]\n",
      "100%|██████████| 32/32 [00:02<00:00, 11.48it/s, epoch=36/50, avg_epoch_loss=7.15]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.99it/s, epoch=37/50, avg_epoch_loss=7.27]\n",
      "100%|██████████| 32/32 [00:02<00:00, 12.95it/s, epoch=38/50, avg_epoch_loss=7.22]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.33it/s, epoch=39/50, avg_epoch_loss=7.17]\n",
      "100%|██████████| 32/32 [00:02<00:00, 10.74it/s, epoch=40/50, avg_epoch_loss=7.11]\n",
      "100%|██████████| 32/32 [00:02<00:00, 12.66it/s, epoch=41/50, avg_epoch_loss=7.35]\n",
      "100%|██████████| 32/32 [00:02<00:00, 12.62it/s, epoch=42/50, avg_epoch_loss=7.28]\n",
      "100%|██████████| 32/32 [00:02<00:00, 13.93it/s, epoch=43/50, avg_epoch_loss=7.23]\n",
      "100%|██████████| 32/32 [00:02<00:00, 12.57it/s, epoch=44/50, avg_epoch_loss=7.22]\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.38it/s, epoch=45/50, avg_epoch_loss=7.25]\n",
      "100%|██████████| 32/32 [00:02<00:00, 11.99it/s, epoch=46/50, avg_epoch_loss=7.23]\n",
      "100%|██████████| 32/32 [00:02<00:00, 13.88it/s, epoch=47/50, avg_epoch_loss=7.17]\n",
      "100%|██████████| 32/32 [00:03<00:00,  9.33it/s, epoch=48/50, avg_epoch_loss=7.2]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.37it/s, epoch=49/50, avg_epoch_loss=7.27]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.46it/s, epoch=50/50, avg_epoch_loss=7.33]\n",
      "Running evaluation: 89it [00:01, 76.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:\t1268.5501676868473\n",
      "1268.5501676868473\n",
      "[[5.0e-02 4.0e+00 6.0e+01 3.2e+01 5.0e+01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:03<00:00,  9.62it/s, epoch=1/50, avg_epoch_loss=9.11]\n",
      "100%|██████████| 32/32 [00:02<00:00, 11.10it/s, epoch=2/50, avg_epoch_loss=7.65]\n",
      "100%|██████████| 32/32 [00:01<00:00, 16.30it/s, epoch=3/50, avg_epoch_loss=7.43]\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.38it/s, epoch=4/50, avg_epoch_loss=7.48]\n",
      "100%|██████████| 32/32 [00:02<00:00, 13.47it/s, epoch=5/50, avg_epoch_loss=7.35]\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.28it/s, epoch=6/50, avg_epoch_loss=7.2]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.14it/s, epoch=7/50, avg_epoch_loss=7.89]\n",
      "100%|██████████| 32/32 [00:03<00:00,  9.35it/s, epoch=8/50, avg_epoch_loss=7.39]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.66it/s, epoch=9/50, avg_epoch_loss=7.32]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.63it/s, epoch=10/50, avg_epoch_loss=7.31]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.03it/s, epoch=11/50, avg_epoch_loss=7.4]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.14it/s, epoch=12/50, avg_epoch_loss=7.24]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.88it/s, epoch=13/50, avg_epoch_loss=7.17]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.02it/s, epoch=14/50, avg_epoch_loss=7.13]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.04it/s, epoch=15/50, avg_epoch_loss=7.38]\n",
      "100%|██████████| 32/32 [00:02<00:00, 10.96it/s, epoch=16/50, avg_epoch_loss=7.33]\n",
      "100%|██████████| 32/32 [00:02<00:00, 13.96it/s, epoch=17/50, avg_epoch_loss=7.21]\n",
      "100%|██████████| 32/32 [00:02<00:00, 11.32it/s, epoch=18/50, avg_epoch_loss=7.2]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.34it/s, epoch=19/50, avg_epoch_loss=7.38]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.09it/s, epoch=20/50, avg_epoch_loss=7.24]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.55it/s, epoch=21/50, avg_epoch_loss=7.16]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.49it/s, epoch=22/50, avg_epoch_loss=7.13]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.50it/s, epoch=23/50, avg_epoch_loss=7.15]\n",
      "100%|██████████| 32/32 [00:03<00:00,  8.62it/s, epoch=24/50, avg_epoch_loss=7.15]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.67it/s, epoch=25/50, avg_epoch_loss=7]\n",
      "100%|██████████| 32/32 [00:05<00:00,  6.37it/s, epoch=26/50, avg_epoch_loss=7.1]\n",
      "100%|██████████| 32/32 [00:04<00:00,  7.29it/s, epoch=27/50, avg_epoch_loss=7.05]\n",
      "100%|██████████| 32/32 [00:04<00:00,  6.69it/s, epoch=28/50, avg_epoch_loss=7.09]\n",
      "100%|██████████| 32/32 [00:02<00:00, 11.39it/s, epoch=29/50, avg_epoch_loss=7.05]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.45it/s, epoch=30/50, avg_epoch_loss=7.05]\n",
      "100%|██████████| 32/32 [00:02<00:00, 13.60it/s, epoch=31/50, avg_epoch_loss=7.14]\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.24it/s, epoch=32/50, avg_epoch_loss=7.06]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.15it/s, epoch=33/50, avg_epoch_loss=7.14]\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.25it/s, epoch=34/50, avg_epoch_loss=7.03]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.26it/s, epoch=35/50, avg_epoch_loss=7.1]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.87it/s, epoch=36/50, avg_epoch_loss=7.06]\n",
      "100%|██████████| 32/32 [00:02<00:00, 13.26it/s, epoch=37/50, avg_epoch_loss=7]\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.31it/s, epoch=38/50, avg_epoch_loss=6.99]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.11it/s, epoch=39/50, avg_epoch_loss=6.97]\n",
      "100%|██████████| 32/32 [00:02<00:00, 12.80it/s, epoch=40/50, avg_epoch_loss=6.94]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.37it/s, epoch=41/50, avg_epoch_loss=6.94]\n",
      "100%|██████████| 32/32 [00:01<00:00, 20.01it/s, epoch=42/50, avg_epoch_loss=6.98]\n",
      "100%|██████████| 32/32 [00:02<00:00, 12.62it/s, epoch=43/50, avg_epoch_loss=7]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.36it/s, epoch=44/50, avg_epoch_loss=6.99]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.39it/s, epoch=45/50, avg_epoch_loss=6.97]\n",
      "100%|██████████| 32/32 [00:02<00:00, 12.31it/s, epoch=46/50, avg_epoch_loss=6.99]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.89it/s, epoch=47/50, avg_epoch_loss=6.98]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.25it/s, epoch=48/50, avg_epoch_loss=6.99]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.30it/s, epoch=49/50, avg_epoch_loss=6.97]\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.79it/s, epoch=50/50, avg_epoch_loss=6.98]\n",
      "Running evaluation: 89it [00:00, 192.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS:\t727.4693941612077\n",
      "727.4693941612077\n",
      "RESULTS:\n",
      "[5.0e-02 4.0e+00 6.0e+01 3.2e+01 5.0e+01]\n",
      "727.4693941612077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "opt_sol = optimize_on_metric(data_for_model, prediction_length, freq, start_date)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 13.93it/s, epoch=1/100, avg_epoch_loss=8.82]\n",
      "100%|██████████| 16/16 [00:00<00:00, 18.15it/s, epoch=2/100, avg_epoch_loss=6.91]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.39it/s, epoch=3/100, avg_epoch_loss=6.61]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.86it/s, epoch=4/100, avg_epoch_loss=5.77]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.25it/s, epoch=5/100, avg_epoch_loss=5.93]\n",
      "100%|██████████| 16/16 [00:01<00:00, 12.86it/s, epoch=6/100, avg_epoch_loss=5.23]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.87it/s, epoch=7/100, avg_epoch_loss=5.71]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.34it/s, epoch=8/100, avg_epoch_loss=5.54]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.86it/s, epoch=9/100, avg_epoch_loss=5.41]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.15it/s, epoch=10/100, avg_epoch_loss=5.49]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.54it/s, epoch=11/100, avg_epoch_loss=5.46]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.91it/s, epoch=12/100, avg_epoch_loss=6.24]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.09it/s, epoch=13/100, avg_epoch_loss=5.66]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.84it/s, epoch=14/100, avg_epoch_loss=5.55]\n",
      "100%|██████████| 16/16 [00:00<00:00, 16.17it/s, epoch=15/100, avg_epoch_loss=6.36]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.91it/s, epoch=16/100, avg_epoch_loss=5.45]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.22it/s, epoch=17/100, avg_epoch_loss=4.96]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.65it/s, epoch=18/100, avg_epoch_loss=5.27]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.09it/s, epoch=19/100, avg_epoch_loss=4.99]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.56it/s, epoch=20/100, avg_epoch_loss=5.2]\n",
      "100%|██████████| 16/16 [00:00<00:00, 16.40it/s, epoch=21/100, avg_epoch_loss=4.71]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.68it/s, epoch=22/100, avg_epoch_loss=4.95]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.35it/s, epoch=23/100, avg_epoch_loss=4.99]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.12it/s, epoch=24/100, avg_epoch_loss=4.66]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.81it/s, epoch=25/100, avg_epoch_loss=4.96]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.30it/s, epoch=26/100, avg_epoch_loss=4.76]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.54it/s, epoch=27/100, avg_epoch_loss=4.69]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.12it/s, epoch=28/100, avg_epoch_loss=4.87]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.16it/s, epoch=29/100, avg_epoch_loss=4.72]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.08it/s, epoch=30/100, avg_epoch_loss=4.92]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.11it/s, epoch=31/100, avg_epoch_loss=5.01]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.51it/s, epoch=32/100, avg_epoch_loss=4.62]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.47it/s, epoch=33/100, avg_epoch_loss=4.81]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.61it/s, epoch=34/100, avg_epoch_loss=4.59]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.91it/s, epoch=35/100, avg_epoch_loss=4.8]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.23it/s, epoch=36/100, avg_epoch_loss=4.5]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.28it/s, epoch=37/100, avg_epoch_loss=4.83]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.60it/s, epoch=38/100, avg_epoch_loss=4.64]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.03it/s, epoch=39/100, avg_epoch_loss=4.7]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.59it/s, epoch=40/100, avg_epoch_loss=4.69]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.90it/s, epoch=41/100, avg_epoch_loss=4.64]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.10it/s, epoch=42/100, avg_epoch_loss=4.65]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.97it/s, epoch=43/100, avg_epoch_loss=4.87]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.13it/s, epoch=44/100, avg_epoch_loss=4.43]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.51it/s, epoch=45/100, avg_epoch_loss=4.8]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.85it/s, epoch=46/100, avg_epoch_loss=4.76]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.29it/s, epoch=47/100, avg_epoch_loss=4.72]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.38it/s, epoch=48/100, avg_epoch_loss=4.61]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.09it/s, epoch=49/100, avg_epoch_loss=4.81]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.59it/s, epoch=50/100, avg_epoch_loss=4.28]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.25it/s, epoch=51/100, avg_epoch_loss=4.8]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.17it/s, epoch=52/100, avg_epoch_loss=4.51]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.44it/s, epoch=53/100, avg_epoch_loss=4.56]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.45it/s, epoch=54/100, avg_epoch_loss=4.39]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.32it/s, epoch=55/100, avg_epoch_loss=4.76]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.60it/s, epoch=56/100, avg_epoch_loss=4.36]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.28it/s, epoch=57/100, avg_epoch_loss=4.6]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.91it/s, epoch=58/100, avg_epoch_loss=4.28]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.18it/s, epoch=59/100, avg_epoch_loss=4.69]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.26it/s, epoch=60/100, avg_epoch_loss=4.46]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.79it/s, epoch=61/100, avg_epoch_loss=4.68]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.65it/s, epoch=62/100, avg_epoch_loss=4.67]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.93it/s, epoch=63/100, avg_epoch_loss=4.52]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.39it/s, epoch=64/100, avg_epoch_loss=4.35]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.08it/s, epoch=65/100, avg_epoch_loss=4.22]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.12it/s, epoch=66/100, avg_epoch_loss=4.46]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.12it/s, epoch=67/100, avg_epoch_loss=4.54]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.34it/s, epoch=68/100, avg_epoch_loss=4.33]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.78it/s, epoch=69/100, avg_epoch_loss=4.63]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.39it/s, epoch=70/100, avg_epoch_loss=4.32]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.70it/s, epoch=71/100, avg_epoch_loss=4.44]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.04it/s, epoch=72/100, avg_epoch_loss=4.35]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.02it/s, epoch=73/100, avg_epoch_loss=4.56]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.61it/s, epoch=74/100, avg_epoch_loss=4.64]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.96it/s, epoch=75/100, avg_epoch_loss=4.35]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.07it/s, epoch=76/100, avg_epoch_loss=4.91]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.81it/s, epoch=77/100, avg_epoch_loss=4.6]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.69it/s, epoch=78/100, avg_epoch_loss=4.33]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.42it/s, epoch=79/100, avg_epoch_loss=4.56]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.79it/s, epoch=80/100, avg_epoch_loss=4.24]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.88it/s, epoch=81/100, avg_epoch_loss=4.43]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.03it/s, epoch=82/100, avg_epoch_loss=4.31]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.73it/s, epoch=83/100, avg_epoch_loss=4.51]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.21it/s, epoch=84/100, avg_epoch_loss=4.07]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.85it/s, epoch=85/100, avg_epoch_loss=4.64]\n",
      "100%|██████████| 16/16 [00:00<00:00, 17.20it/s, epoch=86/100, avg_epoch_loss=4.52]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.14it/s, epoch=87/100, avg_epoch_loss=4.13]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.67it/s, epoch=88/100, avg_epoch_loss=4.5]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.33it/s, epoch=89/100, avg_epoch_loss=4.31]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.37it/s, epoch=90/100, avg_epoch_loss=4.46]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.32it/s, epoch=91/100, avg_epoch_loss=4.42]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.13it/s, epoch=92/100, avg_epoch_loss=4.38]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.54it/s, epoch=93/100, avg_epoch_loss=4.41]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.53it/s, epoch=94/100, avg_epoch_loss=4.18]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.72it/s, epoch=95/100, avg_epoch_loss=4.28]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.28it/s, epoch=96/100, avg_epoch_loss=3.88]\n",
      "100%|██████████| 16/16 [00:01<00:00, 15.80it/s, epoch=97/100, avg_epoch_loss=4.19]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.40it/s, epoch=98/100, avg_epoch_loss=3.95]\n",
      "100%|██████████| 16/16 [00:01<00:00, 14.01it/s, epoch=99/100, avg_epoch_loss=4.14]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.00it/s, epoch=100/100, avg_epoch_loss=4.13]\n"
     ]
    }
   ],
   "source": [
    "### Prediction for first class\n",
    "### learning rate, cell type, num of layers, num of cells, num of epochs, batch size\n",
    "forecasts0,tss0 = run_deepAR(data, prediction_length, freq, start_date, 0.05, \"lstm\", 4, 60, 100, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "### round to int value of array\n",
    "def round_array(array):\n",
    "    for i in range(len(array)):\n",
    "        array[i] = round(array[i])\n",
    "        if array[i] <= 0:\n",
    "            array[i] = 0\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create dataframe with predictions\n",
    "def create_df_deepar(forecast, data_for_model):\n",
    "    ### dataframe with name of columns same as in data_for_model_000\n",
    "    df = pd.DataFrame(columns=data_for_model.columns)\n",
    "    for i,col in enumerate(data_for_model.columns):\n",
    "        df[col] = round_array(forecast[i].median)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = create_df_deepar(forecasts0, data)\n",
    "y_predict = y_predict.T\n",
    "y_predict.columns = y_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2016-12-31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>121422.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>11083.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>47919.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>15266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ja</th>\n",
       "      <td>18884.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh_MOB_AAG_005</th>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh_MOB_AAG_028</th>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh_MOB_AAG_031</th>\n",
       "      <td>565.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh_MOB_AAG_036</th>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh_MOB_AAG_138</th>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                2016-12-31\n",
       "Total             121422.0\n",
       "de                 11083.0\n",
       "en                 47919.0\n",
       "fr                 15266.0\n",
       "ja                 18884.0\n",
       "...                    ...\n",
       "zh_MOB_AAG_005        17.0\n",
       "zh_MOB_AAG_028        22.0\n",
       "zh_MOB_AAG_031       565.0\n",
       "zh_MOB_AAG_036       113.0\n",
       "zh_MOB_AAG_138        95.0\n",
       "\n",
       "[199 rows x 1 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2016-12-31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>113839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>14854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>42342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>11835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ja</th>\n",
       "      <td>16716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh_MOB_AAG_005</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh_MOB_AAG_028</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh_MOB_AAG_031</th>\n",
       "      <td>555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh_MOB_AAG_036</th>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh_MOB_AAG_138</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                2016-12-31\n",
       "Total               113839\n",
       "de                   14854\n",
       "en                   42342\n",
       "fr                   11835\n",
       "ja                   16716\n",
       "...                    ...\n",
       "zh_MOB_AAG_005          13\n",
       "zh_MOB_AAG_028          17\n",
       "zh_MOB_AAG_031         555\n",
       "zh_MOB_AAG_036         110\n",
       "zh_MOB_AAG_138          76\n",
       "\n",
       "[199 rows x 1 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmape_level(actual_value, forecasted_value, total_ts, lengths):\n",
    "    nb_levels = len(lengths)\n",
    "    wmapes = []\n",
    "    for l in range(nb_levels):\n",
    "        actual_value_ts = actual_value[total_ts[l]:total_ts[l+1], :]\n",
    "        forecasted_value_ts = forecasted_value[total_ts[l]:total_ts[l+1], :]\n",
    "        wmapes.append(calculate_wmape(actual_value_ts, forecasted_value_ts))\n",
    "    return wmapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.6611618162492645,\n",
       " 17.36487495498028,\n",
       " 22.989485150080377,\n",
       " 26.590184383207863,\n",
       " 38.06077003487381]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmape_level(y_test.to_numpy(), y_predict.to_numpy(), total_ts, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.33329526787832"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_wmape(y_test.to_numpy(), y_predict.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I have an array of shape (89,5)\n",
    "### create dataframe with predictions\n",
    "def create_df(y_predict, pred_length, data):\n",
    "    ### dataframe with name of columns same as in data_for_model_000\n",
    "    ### create a dataframe based on data, remove last pred_length rows, and add y_predict\n",
    "    ### return dataframe\n",
    "    y_predict_df = y_predict.astype(np.float32)\n",
    "    y_predict_df = pd.DataFrame(y_predict_df)\n",
    "    y_predict_df = y_predict_df.T\n",
    "    df = data.copy()\n",
    "    for i,col in enumerate(data.columns):\n",
    "        df[col][-(pred_length):] = y_predict_df[:][i]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred = create_df(y_predict.to_numpy(), pred_length, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsse_ts(pred_length, data, data_pred, ts):\n",
    "    H = pred_length\n",
    "    T = data.shape[0] - H\n",
    "    ts_array = data.iloc[:,ts].values\n",
    "    ts_array_pred = data_pred.iloc[:,ts].values\n",
    "    e = (1/H)*np.sum((ts_array[t] - ts_array_pred[t])**2 for t in range(T, T+H))\n",
    "    e_naive = (1/(T-1))*np.sum((ts_array[t] - ts_array[t-1])**2 for t in range(1, T))\n",
    "    return np.sqrt(e/e_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ts = [0,1,5,5+28,5+28+56]\n",
    "lengths = [1, 4, 28, 56]\n",
    "def rmsse_level(pred_length, data, data_pred, total_ts, lengths):\n",
    "    nb_levels = len(lengths)\n",
    "    R = 0\n",
    "    r_l = [0]*nb_levels\n",
    "    for l in range(nb_levels):\n",
    "        for j in range(total_ts[l], total_ts[l+1]):\n",
    "            r_l[l] += (1/lengths[l])*rmsse_ts(pred_length, data, data_pred, j)\n",
    "            #print(l, j)\n",
    "    print(r_l)\n",
    "    R += np.mean(r_l)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13641538003574646, 0.31638487909891044, 0.26427348425376634, 0.3654991693043279]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2706432281731878"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsse_level(pred_length, data, data_pred, total_ts, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1512652ce61a4a833ba8a34b873e0989e58ed502e45d389d9fbd644bedf0ef09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
