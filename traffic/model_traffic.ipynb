{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE AGG MATRIX AND READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\", index_col=0) \n",
    "agg_mat_df = pd.read_csv(\"agg_mat.csv\", index_col=0) # matrix of aggregated data with bottom time series\n",
    "level0total = 1\n",
    "level1total = 2\n",
    "level2total = 4\n",
    "level3total = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y11</th>\n",
       "      <th>y12</th>\n",
       "      <th>y21</th>\n",
       "      <th>y22</th>\n",
       "      <th>Bottom1</th>\n",
       "      <th>Bottom2</th>\n",
       "      <th>Bottom3</th>\n",
       "      <th>...</th>\n",
       "      <th>Bottom191</th>\n",
       "      <th>Bottom192</th>\n",
       "      <th>Bottom193</th>\n",
       "      <th>Bottom194</th>\n",
       "      <th>Bottom195</th>\n",
       "      <th>Bottom196</th>\n",
       "      <th>Bottom197</th>\n",
       "      <th>Bottom198</th>\n",
       "      <th>Bottom199</th>\n",
       "      <th>Bottom200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-01-01</th>\n",
       "      <td>1536.0182</td>\n",
       "      <td>757.3218</td>\n",
       "      <td>778.6964</td>\n",
       "      <td>360.5690</td>\n",
       "      <td>396.7528</td>\n",
       "      <td>410.6807</td>\n",
       "      <td>368.0157</td>\n",
       "      <td>8.1370</td>\n",
       "      <td>2.1910</td>\n",
       "      <td>4.7163</td>\n",
       "      <td>...</td>\n",
       "      <td>5.5609</td>\n",
       "      <td>5.2466</td>\n",
       "      <td>9.3058</td>\n",
       "      <td>8.7667</td>\n",
       "      <td>5.8731</td>\n",
       "      <td>11.3673</td>\n",
       "      <td>8.8627</td>\n",
       "      <td>8.1654</td>\n",
       "      <td>6.5542</td>\n",
       "      <td>8.5484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-02</th>\n",
       "      <td>1619.2435</td>\n",
       "      <td>790.8156</td>\n",
       "      <td>828.4279</td>\n",
       "      <td>389.4334</td>\n",
       "      <td>401.3822</td>\n",
       "      <td>431.6908</td>\n",
       "      <td>396.7371</td>\n",
       "      <td>8.9488</td>\n",
       "      <td>8.1517</td>\n",
       "      <td>4.8137</td>\n",
       "      <td>...</td>\n",
       "      <td>5.3390</td>\n",
       "      <td>5.3748</td>\n",
       "      <td>9.7310</td>\n",
       "      <td>12.0854</td>\n",
       "      <td>5.9546</td>\n",
       "      <td>11.8716</td>\n",
       "      <td>9.1413</td>\n",
       "      <td>8.2749</td>\n",
       "      <td>7.9835</td>\n",
       "      <td>7.4303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-03</th>\n",
       "      <td>1423.6574</td>\n",
       "      <td>684.3216</td>\n",
       "      <td>739.3358</td>\n",
       "      <td>329.0994</td>\n",
       "      <td>355.2222</td>\n",
       "      <td>387.1702</td>\n",
       "      <td>352.1656</td>\n",
       "      <td>8.1359</td>\n",
       "      <td>4.7902</td>\n",
       "      <td>4.6435</td>\n",
       "      <td>...</td>\n",
       "      <td>5.1115</td>\n",
       "      <td>5.1326</td>\n",
       "      <td>9.6746</td>\n",
       "      <td>8.9663</td>\n",
       "      <td>6.1468</td>\n",
       "      <td>9.7029</td>\n",
       "      <td>8.5609</td>\n",
       "      <td>7.8705</td>\n",
       "      <td>6.2287</td>\n",
       "      <td>6.7340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-04</th>\n",
       "      <td>1096.3325</td>\n",
       "      <td>529.1279</td>\n",
       "      <td>567.2046</td>\n",
       "      <td>249.3346</td>\n",
       "      <td>279.7933</td>\n",
       "      <td>296.2565</td>\n",
       "      <td>270.9481</td>\n",
       "      <td>6.4700</td>\n",
       "      <td>1.5121</td>\n",
       "      <td>3.2586</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9545</td>\n",
       "      <td>3.8832</td>\n",
       "      <td>6.7228</td>\n",
       "      <td>6.5009</td>\n",
       "      <td>4.7770</td>\n",
       "      <td>7.6188</td>\n",
       "      <td>7.4227</td>\n",
       "      <td>5.5232</td>\n",
       "      <td>4.8793</td>\n",
       "      <td>5.8216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-05</th>\n",
       "      <td>974.5526</td>\n",
       "      <td>475.1733</td>\n",
       "      <td>499.3793</td>\n",
       "      <td>231.4860</td>\n",
       "      <td>243.6873</td>\n",
       "      <td>264.9305</td>\n",
       "      <td>234.4488</td>\n",
       "      <td>5.5741</td>\n",
       "      <td>1.4916</td>\n",
       "      <td>2.9172</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5862</td>\n",
       "      <td>3.6739</td>\n",
       "      <td>5.5844</td>\n",
       "      <td>5.7266</td>\n",
       "      <td>4.0254</td>\n",
       "      <td>6.4069</td>\n",
       "      <td>6.9933</td>\n",
       "      <td>5.1315</td>\n",
       "      <td>4.0133</td>\n",
       "      <td>5.2404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-12-27</th>\n",
       "      <td>1606.0017</td>\n",
       "      <td>801.1668</td>\n",
       "      <td>804.8349</td>\n",
       "      <td>423.0887</td>\n",
       "      <td>378.0781</td>\n",
       "      <td>408.4916</td>\n",
       "      <td>396.3433</td>\n",
       "      <td>7.7495</td>\n",
       "      <td>9.0067</td>\n",
       "      <td>7.4960</td>\n",
       "      <td>...</td>\n",
       "      <td>11.3703</td>\n",
       "      <td>7.0951</td>\n",
       "      <td>5.6765</td>\n",
       "      <td>6.8699</td>\n",
       "      <td>5.4110</td>\n",
       "      <td>8.9070</td>\n",
       "      <td>4.3274</td>\n",
       "      <td>2.6546</td>\n",
       "      <td>12.9457</td>\n",
       "      <td>13.0458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-12-28</th>\n",
       "      <td>1567.3015</td>\n",
       "      <td>779.7629</td>\n",
       "      <td>787.5386</td>\n",
       "      <td>413.3880</td>\n",
       "      <td>366.3749</td>\n",
       "      <td>395.4527</td>\n",
       "      <td>392.0859</td>\n",
       "      <td>8.1097</td>\n",
       "      <td>9.2325</td>\n",
       "      <td>7.6123</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0934</td>\n",
       "      <td>7.4894</td>\n",
       "      <td>7.5004</td>\n",
       "      <td>7.0521</td>\n",
       "      <td>5.5878</td>\n",
       "      <td>9.2178</td>\n",
       "      <td>4.4601</td>\n",
       "      <td>2.6299</td>\n",
       "      <td>13.1887</td>\n",
       "      <td>11.6035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-12-29</th>\n",
       "      <td>1722.5124</td>\n",
       "      <td>878.3538</td>\n",
       "      <td>844.1586</td>\n",
       "      <td>428.7631</td>\n",
       "      <td>449.5907</td>\n",
       "      <td>429.9540</td>\n",
       "      <td>414.2046</td>\n",
       "      <td>6.2941</td>\n",
       "      <td>5.9546</td>\n",
       "      <td>17.2047</td>\n",
       "      <td>...</td>\n",
       "      <td>12.2824</td>\n",
       "      <td>7.6019</td>\n",
       "      <td>5.7080</td>\n",
       "      <td>6.9409</td>\n",
       "      <td>8.5787</td>\n",
       "      <td>7.2745</td>\n",
       "      <td>10.6823</td>\n",
       "      <td>2.8012</td>\n",
       "      <td>14.2313</td>\n",
       "      <td>13.4012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-12-30</th>\n",
       "      <td>1776.7038</td>\n",
       "      <td>918.6770</td>\n",
       "      <td>858.0268</td>\n",
       "      <td>475.8637</td>\n",
       "      <td>442.8133</td>\n",
       "      <td>421.0122</td>\n",
       "      <td>437.0146</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>9.3584</td>\n",
       "      <td>7.2440</td>\n",
       "      <td>...</td>\n",
       "      <td>12.1320</td>\n",
       "      <td>7.1799</td>\n",
       "      <td>6.5667</td>\n",
       "      <td>7.5545</td>\n",
       "      <td>2.8586</td>\n",
       "      <td>10.3623</td>\n",
       "      <td>8.6429</td>\n",
       "      <td>2.8779</td>\n",
       "      <td>14.2212</td>\n",
       "      <td>13.3731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-12-31</th>\n",
       "      <td>1353.1695</td>\n",
       "      <td>707.7901</td>\n",
       "      <td>645.3794</td>\n",
       "      <td>356.5805</td>\n",
       "      <td>351.2096</td>\n",
       "      <td>321.7236</td>\n",
       "      <td>323.6558</td>\n",
       "      <td>6.9438</td>\n",
       "      <td>7.0639</td>\n",
       "      <td>5.3488</td>\n",
       "      <td>...</td>\n",
       "      <td>5.3459</td>\n",
       "      <td>5.5850</td>\n",
       "      <td>5.4051</td>\n",
       "      <td>6.1172</td>\n",
       "      <td>2.1928</td>\n",
       "      <td>9.3541</td>\n",
       "      <td>5.8936</td>\n",
       "      <td>1.7567</td>\n",
       "      <td>11.7125</td>\n",
       "      <td>14.2597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>366 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Total        y1        y2       y11       y12       y21  \\\n",
       "2008-01-01  1536.0182  757.3218  778.6964  360.5690  396.7528  410.6807   \n",
       "2008-01-02  1619.2435  790.8156  828.4279  389.4334  401.3822  431.6908   \n",
       "2008-01-03  1423.6574  684.3216  739.3358  329.0994  355.2222  387.1702   \n",
       "2008-01-04  1096.3325  529.1279  567.2046  249.3346  279.7933  296.2565   \n",
       "2008-01-05   974.5526  475.1733  499.3793  231.4860  243.6873  264.9305   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "2008-12-27  1606.0017  801.1668  804.8349  423.0887  378.0781  408.4916   \n",
       "2008-12-28  1567.3015  779.7629  787.5386  413.3880  366.3749  395.4527   \n",
       "2008-12-29  1722.5124  878.3538  844.1586  428.7631  449.5907  429.9540   \n",
       "2008-12-30  1776.7038  918.6770  858.0268  475.8637  442.8133  421.0122   \n",
       "2008-12-31  1353.1695  707.7901  645.3794  356.5805  351.2096  321.7236   \n",
       "\n",
       "                 y22  Bottom1  Bottom2  Bottom3  ...  Bottom191  Bottom192  \\\n",
       "2008-01-01  368.0157   8.1370   2.1910   4.7163  ...     5.5609     5.2466   \n",
       "2008-01-02  396.7371   8.9488   8.1517   4.8137  ...     5.3390     5.3748   \n",
       "2008-01-03  352.1656   8.1359   4.7902   4.6435  ...     5.1115     5.1326   \n",
       "2008-01-04  270.9481   6.4700   1.5121   3.2586  ...     2.9545     3.8832   \n",
       "2008-01-05  234.4488   5.5741   1.4916   2.9172  ...     2.5862     3.6739   \n",
       "...              ...      ...      ...      ...  ...        ...        ...   \n",
       "2008-12-27  396.3433   7.7495   9.0067   7.4960  ...    11.3703     7.0951   \n",
       "2008-12-28  392.0859   8.1097   9.2325   7.6123  ...    11.0934     7.4894   \n",
       "2008-12-29  414.2046   6.2941   5.9546  17.2047  ...    12.2824     7.6019   \n",
       "2008-12-30  437.0146   9.7905   9.3584   7.2440  ...    12.1320     7.1799   \n",
       "2008-12-31  323.6558   6.9438   7.0639   5.3488  ...     5.3459     5.5850   \n",
       "\n",
       "            Bottom193  Bottom194  Bottom195  Bottom196  Bottom197  Bottom198  \\\n",
       "2008-01-01     9.3058     8.7667     5.8731    11.3673     8.8627     8.1654   \n",
       "2008-01-02     9.7310    12.0854     5.9546    11.8716     9.1413     8.2749   \n",
       "2008-01-03     9.6746     8.9663     6.1468     9.7029     8.5609     7.8705   \n",
       "2008-01-04     6.7228     6.5009     4.7770     7.6188     7.4227     5.5232   \n",
       "2008-01-05     5.5844     5.7266     4.0254     6.4069     6.9933     5.1315   \n",
       "...               ...        ...        ...        ...        ...        ...   \n",
       "2008-12-27     5.6765     6.8699     5.4110     8.9070     4.3274     2.6546   \n",
       "2008-12-28     7.5004     7.0521     5.5878     9.2178     4.4601     2.6299   \n",
       "2008-12-29     5.7080     6.9409     8.5787     7.2745    10.6823     2.8012   \n",
       "2008-12-30     6.5667     7.5545     2.8586    10.3623     8.6429     2.8779   \n",
       "2008-12-31     5.4051     6.1172     2.1928     9.3541     5.8936     1.7567   \n",
       "\n",
       "            Bottom199  Bottom200  \n",
       "2008-01-01     6.5542     8.5484  \n",
       "2008-01-02     7.9835     7.4303  \n",
       "2008-01-03     6.2287     6.7340  \n",
       "2008-01-04     4.8793     5.8216  \n",
       "2008-01-05     4.0133     5.2404  \n",
       "...               ...        ...  \n",
       "2008-12-27    12.9457    13.0458  \n",
       "2008-12-28    13.1887    11.6035  \n",
       "2008-12-29    14.2313    13.4012  \n",
       "2008-12-30    14.2212    13.3731  \n",
       "2008-12-31    11.7125    14.2597  \n",
       "\n",
       "[366 rows x 207 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels_left = [0, level0total, level0total+level1total, level0total+level1total+level2total]\n",
    "levels_right = [0, level1total, level1total+level2total, level1total+level2total+level3total]\n",
    "nb_ts_levels = [level0total, level1total, level2total, level3total]\n",
    "total_ts = [0,level0total,level0total+level1total,level0total+level1total+level2total,level0total+level1total+level2total+level3total]\n",
    "lengths = nb_ts_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes=[[2], [2,2], [50,50,50,50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  2, 50, 50, 50, 50])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_list = np.array([])\n",
    "for i in range(len(nodes)):\n",
    "    for j in range(len(nodes[i])):\n",
    "        nodes_list = np.append(nodes_list, int(nodes[i][j]))\n",
    "nodes_list = nodes_list.astype(int)\n",
    "nodes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_agg_mat(nb_ts_levels, nodes_list):\n",
    "    nb_total_ts = sum(nb_ts_levels)\n",
    "    nb_ts_agg =sum(nb_ts_levels[:len(nb_ts_levels)-1])\n",
    "    global_Matrix = pd.DataFrame(np.zeros((nb_ts_agg, nb_total_ts-1)))\n",
    "    \n",
    "    c = 0\n",
    "    for i in range(len(nodes_list)):\n",
    "        for j in range(nodes_list[i]):\n",
    "            global_Matrix.iloc[i,c+j] = 1\n",
    "        c += nodes_list[i]\n",
    "            \n",
    "\n",
    "    return global_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create global matrix A\n",
    "### create list of number of TS in each level\n",
    "\n",
    "def matrix_per_level(global_Matrix, levels_left, levels_right, l):\n",
    "    return np.array(global_Matrix.iloc[levels_left[l-1]:levels_left[l], levels_right[l-1]:levels_right[l]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 206 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  196  197  198  199  \\\n",
       "0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0  1.0  1.0   \n",
       "\n",
       "   200  201  202  203  204  205  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "5  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "6  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "\n",
       "[7 rows x 206 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = creat_agg_mat(nb_ts_levels, nodes_list)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_per_level(A, levels_left, levels_right, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "### pivot data such as index is the name of columns\n",
    "#data = data.pivot(index='date', columns='symbol', values='close')\n",
    "pivot_df = data.T\n",
    "\n",
    "n_ts = 207\n",
    "n_timepoints = 365\n",
    "n_dates = 20\n",
    "input_size = n_ts*n_dates\n",
    "pred_length = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wmape(actual_values, forecasted_values):\n",
    "    n = len(actual_values)\n",
    "    num = np.sum(np.abs(actual_values - forecasted_values))\n",
    "    den = np.sum(np.abs(actual_values))\n",
    "    wmape = 100*num/den\n",
    "    return wmape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(n_timepoints-n_dates-pred_length+1):\n",
    "    X = pivot_df.iloc[:,i:i+n_dates].T.to_numpy().reshape(1,-1)\n",
    "    X_train.append(X[0])\n",
    "    y = pivot_df.iloc[:,i+n_dates:i+n_dates+1].T.to_numpy()\n",
    "    y_train.append(y[0])\n",
    "\n",
    "for i in range(n_timepoints-n_dates-pred_length+1, n_timepoints-n_dates+1):\n",
    "    X = pivot_df.iloc[:,i:i+n_dates].T.to_numpy().reshape(1,-1)\n",
    "    X_test.append(X[0])\n",
    "    y = pivot_df.iloc[:,i+n_dates:i+n_dates+1].T.to_numpy()\n",
    "    y_test.append(y[0])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First model without coherency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fix seed\n",
    "keras.utils.set_random_seed(99)\n",
    "\n",
    "# Enable eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Create a sequential model\n",
    "inp1 = Input(shape=(input_size,))\n",
    "h1_w1 = Dense(512*2*2, activation='relu')(inp1)\n",
    "h2_w1  = Dense(512*2, activation='relu')(h1_w1)\n",
    "h3_w1 = Dense(256*2, activation='relu')(h2_w1)\n",
    "h4_w1 = Dense(256, activation='relu')(h3_w1)\n",
    "out1 = Dense(199, activation='linear')(h4_w1)\n",
    "\n",
    "mdl1 = Model(inputs=inp1, outputs=out1)\n",
    "\n",
    "mdl1.compile(loss=\"mse\", optimizer='adam')\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_epochs = 0\n",
    "best_batch_size = 0\n",
    "\n",
    "for epochs in [100, 150, 200]:\n",
    "    for batch_size in [16, 32, 64]:\n",
    "        history = mdl1.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        loss = history.history['val_loss'][-1] if 'val_loss' in history.history else history.history['loss'][-1]\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_epochs = epochs\n",
    "            best_batch_size = batch_size\n",
    "\n",
    "print(\"Best configuration - Epochs:\", best_epochs, \"Batch Size:\", best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 17435.9160\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 4173.6440\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 1271.8783\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 1055.4203\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 656.9713\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 500.8361\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 493.3472\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 367.5437\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 264.1004\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 257.7364\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 195.4594\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 175.5759\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 176.5396\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 185.1572\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 206.1746\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 172.8063\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 141.9704\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 134.5240\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 149.2590\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 182.0035\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 140.9694\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 134.1433\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 131.5882\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 131.1304\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 137.2453\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 152.3955\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 123.3085\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 123.6148\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 121.2775\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 135.9668\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 127.4525\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 122.4243\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 151.5418\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 190.9031\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 143.7899\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 231.5029\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 150.6647\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 124.3202\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 108.6539\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 108.6256\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 120.2715\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 160.8671\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 170.7136\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 128.9124\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 109.9796\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 114.4985\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 112.1096\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 98.4449\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 99.7951\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 109.3160\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 121.4952\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 101.2216\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 111.5378\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 92.5731\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 91.9394\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 129.0938\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 124.8658\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 179.8146\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 179.4809\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 94.5875\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 92.4677\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 92.6345\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 85.2781\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 251.6576\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 204.0028\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 192.9395\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 151.8961\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 1s 97ms/step - loss: 118.4232\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 119.0102\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 85.6582\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 100.4287\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 87.4858\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 77.7085\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 84.4811\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 82.8433\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 91.9030\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 94.1442\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 80.0588\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 71.9797\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 77.2344\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 70.1125\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 75.1667\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 68.1638\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 62.2712\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 71.4552\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 83.7368\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 62.5820\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 65.9376\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 59.4960\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 67.4221\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 64.2031\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 56.4397\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 1s 97ms/step - loss: 94.6565\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 99.8618\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 118.9834\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 116.2112\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 79.3029\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 120.0386\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 97.1999\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 73.4210\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 132.0215\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 61.6373\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 53.1479\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 60.3978\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 79.4837\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 69.0413\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 62.7598\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 74.0027\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 58.5801\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 53.3457\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 64.9088\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 99.1160\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 167.5963\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 146.7187\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 88.9599\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 1s 108ms/step - loss: 68.0986\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 52.7543\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 51.4870\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 45.0603\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 46.9201\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 40.8527\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 41.7077\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 42.5065\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 43.8806\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 44.1955\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 40.3863\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 45.7926\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 39.9601\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 48.6917\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 40.3753\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 46.5760\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 46.2663\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 37.3796\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 40.9533\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 38.0446\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 35.1262\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 48.4962\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 41.6024\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 50.7955\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 68.0449\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 73.5439\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 83.2425\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 80.0338\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 72.6421\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 58.6358\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 46.1343\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 84ms/step - loss: 41.0003\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 49.2050\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 38.7877\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 40.4475\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 37.1945\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 37.9108\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 51.7868\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 39.0653\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 30.4186\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 27.6929\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 28.8162\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 34.4762\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 32.7610\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 38.6162\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 79.8650\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 76.9779\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 93.5206\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 65.1923\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 65.7044\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 72.8931\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 59.8060\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 54.6779\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 1s 98ms/step - loss: 35.9608\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 30.4354\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 35.9361\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 30.8321\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 25.7283\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 27.6741\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 29.2113\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 25.8978\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 26.5448\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 84.2989\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 53.4305\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 85ms/step - loss: 34.4016\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 34.5161\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 32.7304\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 32.4967\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 39.0739\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 42.9578\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 32.7823\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 33.7295\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 28.2661\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 28.0011\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 21.5442\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 21.9862\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 21.0042\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 29.2517\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 26.4956\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 20.5423\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 23.3743\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 19.8823\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 23.5495\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 25.0067\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 33.1343\n"
     ]
    }
   ],
   "source": [
    "best_epochs = 200\n",
    "best_batch_size = 64\n",
    "\n",
    "## fix seed\n",
    "keras.utils.set_random_seed(99)\n",
    "\n",
    "# Enable eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Create a sequential model\n",
    "inp1 = Input(shape=(input_size,))\n",
    "h1_w1 = Dense(512*4, activation='relu')(inp1)\n",
    "h2_w1  = Dense(512*2, activation='relu')(h1_w1)\n",
    "h3_w1 = Dense(256*2, activation='relu')(h2_w1)\n",
    "h4_w1 = Dense(256, activation='relu')(h3_w1)\n",
    "out1 = Dense(207, activation='linear')(h4_w1)\n",
    "\n",
    "mdl1 = Model(inputs=inp1, outputs=out1)\n",
    "\n",
    "mdl1.compile(loss=\"mse\", optimizer='adam')\n",
    "\n",
    "history1 = mdl1.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history1.history\n",
    "history_dict.keys()\n",
    "loss_values = history_dict[\"loss\"]\n",
    "#val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "#plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.22098578853401"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = np.array(mdl1.predict(X_test))#.reshape(-1)\n",
    "calculate_wmape(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL FOR HIERARCHICAL TIME SERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_weights(weight, lengths):\n",
    "    start_idx = 0\n",
    "    result = []\n",
    "    for length in lengths:\n",
    "        sub_array = weight[start_idx:start_idx + length]\n",
    "        result.append(sub_array)\n",
    "        start_idx += length\n",
    "\n",
    "    result = np.array(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom loss function\n",
    "def custom_loss_with_regularization(reg_weight, weight_matrix, lengths, A, levels_left, levels_right):\n",
    "    def loss(y_true, y_pred):\n",
    "        mse_loss = keras.losses.mean_squared_error(y_true, y_pred)  # MSE loss\n",
    "        \n",
    "        def loss_up(weight_matrix):\n",
    "            weights_list = weight_matrix\n",
    "            n, m = weights_list.shape\n",
    "            regularization_loss = 0\n",
    "\n",
    "            for i in range(n):\n",
    "                w = reshape_weights(weights_list[i], lengths)\n",
    "                for l in range(len(w) - 1):\n",
    "                    mat_agg = matrix_per_level(A, levels_left, levels_right, l + 1)\n",
    "                    mat_agg = tf.convert_to_tensor(mat_agg, dtype=tf.float32)  # Convert to TensorFlow tensor\n",
    "                    w_l = w[l]\n",
    "                    w_l2 = w[l + 1]\n",
    "                    w_l_expanded = tf.expand_dims(w_l, axis=0)\n",
    "                    w_l2_expanded = tf.expand_dims(w_l2, axis=-1)\n",
    "                    regularization_loss += tf.reduce_sum(w_l_expanded - tf.transpose(tf.matmul(mat_agg, w_l2_expanded)))\n",
    "            \n",
    "            return regularization_loss\n",
    "        \n",
    "        custom_reg_loss = loss_up(weight_matrix)\n",
    "        total_loss = mse_loss + reg_weight * custom_reg_loss \n",
    "        return total_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hamza\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:371: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration - Epochs: 150 Batch Size: 64\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 189841.3594\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 189240.8125\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 188612.9219\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 188024.0781\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 187426.0156\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 186830.0312\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 186261.5938\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 185659.7500\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 185117.5312\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 184526.6406\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 183991.0000\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 183423.6719\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 182877.8281\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 182342.7344\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 181804.4062\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 181291.2812\n",
      "Epoch 17/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 180766.2188\n",
      "Epoch 18/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 180247.0156\n",
      "Epoch 19/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 179756.5781\n",
      "Epoch 20/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 179236.8438\n",
      "Epoch 21/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 178753.9062\n",
      "Epoch 22/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 178262.3438\n",
      "Epoch 23/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 177765.2500\n",
      "Epoch 24/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 177296.3594\n",
      "Epoch 25/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 176814.6562\n",
      "Epoch 26/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 176344.9062\n",
      "Epoch 27/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 175886.2188\n",
      "Epoch 28/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 175428.8906\n",
      "Epoch 29/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 174977.2656\n",
      "Epoch 30/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 174538.1719\n",
      "Epoch 31/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 174089.9844\n",
      "Epoch 32/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 173640.5781\n",
      "Epoch 33/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 173173.9062\n",
      "Epoch 34/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 172725.3594\n",
      "Epoch 35/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 172275.9375\n",
      "Epoch 36/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 171825.1875\n",
      "Epoch 37/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 171387.1562\n",
      "Epoch 38/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 170945.9844\n",
      "Epoch 39/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 170515.6562\n",
      "Epoch 40/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 170088.9062\n",
      "Epoch 41/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 169658.7500\n",
      "Epoch 42/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 169252.5156\n",
      "Epoch 43/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 168835.7500\n",
      "Epoch 44/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 168414.5000\n",
      "Epoch 45/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 168035.2812\n",
      "Epoch 46/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 167619.2812\n",
      "Epoch 47/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 167214.4062\n",
      "Epoch 48/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 166850.7344\n",
      "Epoch 49/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 166441.9219\n",
      "Epoch 50/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 166054.8125\n",
      "Epoch 51/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 165710.9219\n",
      "Epoch 52/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 165472.7188\n",
      "Epoch 53/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 165026.8438\n",
      "Epoch 54/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 164633.5312\n",
      "Epoch 55/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 164363.7500\n",
      "Epoch 56/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 164061.9844\n",
      "Epoch 57/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 163613.8594\n",
      "Epoch 58/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 163414.6406\n",
      "Epoch 59/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 163114.3906\n",
      "Epoch 60/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 162656.0000\n",
      "Epoch 61/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 162530.0156\n",
      "Epoch 62/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 162069.7500\n",
      "Epoch 63/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 161772.1562\n",
      "Epoch 64/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 161500.0781\n",
      "Epoch 65/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 161110.1406\n",
      "Epoch 66/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 160859.2344\n",
      "Epoch 67/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 160525.5000\n",
      "Epoch 68/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 160243.5625\n",
      "Epoch 69/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159959.3125\n",
      "Epoch 70/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159648.8438\n",
      "Epoch 71/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159403.2812\n",
      "Epoch 72/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 159092.6875\n",
      "Epoch 73/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 158854.2969\n",
      "Epoch 74/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 158556.8438\n",
      "Epoch 75/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 158286.2344\n",
      "Epoch 76/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 158021.7344\n",
      "Epoch 77/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 157783.9688\n",
      "Epoch 78/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 157522.9375\n",
      "Epoch 79/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 157253.3438\n",
      "Epoch 80/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 157000.4062\n",
      "Epoch 81/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 156749.9688\n",
      "Epoch 82/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 156500.0781\n",
      "Epoch 83/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 156251.2656\n",
      "Epoch 84/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 156004.3594\n",
      "Epoch 85/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 155789.1562\n",
      "Epoch 86/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 155518.9844\n",
      "Epoch 87/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 155309.6406\n",
      "Epoch 88/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 155065.7031\n",
      "Epoch 89/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 154808.8594\n",
      "Epoch 90/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 154623.8594\n",
      "Epoch 91/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 154334.7188\n",
      "Epoch 92/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 154117.4219\n",
      "Epoch 93/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 153881.3906\n",
      "Epoch 94/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 153634.7344\n",
      "Epoch 95/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 153410.4688\n",
      "Epoch 96/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 153186.8906\n",
      "Epoch 97/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152959.1094\n",
      "Epoch 98/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152730.6406\n",
      "Epoch 99/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152517.7031\n",
      "Epoch 100/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 152290.2188\n",
      "Epoch 101/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 152071.2188\n",
      "Epoch 102/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 151845.8906\n",
      "Epoch 103/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 151625.7031\n",
      "Epoch 104/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 151407.6875\n",
      "Epoch 105/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 151193.7969\n",
      "Epoch 106/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 150977.4375\n",
      "Epoch 107/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 150765.0625\n",
      "Epoch 108/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 150562.6094\n",
      "Epoch 109/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 150361.1719\n",
      "Epoch 110/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 150141.4531\n",
      "Epoch 111/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 149941.7969\n",
      "Epoch 112/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 149722.0000\n",
      "Epoch 113/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 149522.3906\n",
      "Epoch 114/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 149310.9062\n",
      "Epoch 115/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 149121.5156\n",
      "Epoch 116/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 148912.9219\n",
      "Epoch 117/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 148711.7188\n",
      "Epoch 118/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 148503.5938\n",
      "Epoch 119/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 148321.0000\n",
      "Epoch 120/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 148107.8906\n",
      "Epoch 121/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 147921.7188\n",
      "Epoch 122/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 147705.1094\n",
      "Epoch 123/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 147522.8125\n",
      "Epoch 124/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 147313.9375\n",
      "Epoch 125/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 147123.9844\n",
      "Epoch 126/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 146922.5938\n",
      "Epoch 127/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 146741.7656\n",
      "Epoch 128/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 146546.2188\n",
      "Epoch 129/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 146354.1250\n",
      "Epoch 130/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 146165.6094\n",
      "Epoch 131/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 145977.6562\n",
      "Epoch 132/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 145792.0938\n",
      "Epoch 133/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 145605.0938\n",
      "Epoch 134/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 145424.4062\n",
      "Epoch 135/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 145237.5312\n",
      "Epoch 136/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 145062.7188\n",
      "Epoch 137/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 144871.5625\n",
      "Epoch 138/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 144696.7500\n",
      "Epoch 139/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 144514.1406\n",
      "Epoch 140/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 144339.0625\n",
      "Epoch 141/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 144159.2344\n",
      "Epoch 142/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 143982.1406\n",
      "Epoch 143/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 143804.5312\n",
      "Epoch 144/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 143631.7812\n",
      "Epoch 145/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 143458.9688\n",
      "Epoch 146/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 143285.4688\n",
      "Epoch 147/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 143117.3438\n",
      "Epoch 148/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 142944.3594\n",
      "Epoch 149/150\n",
      "1/1 [==============================] - 2s 2s/step - loss: 142776.7188\n",
      "Epoch 150/150\n",
      "1/1 [==============================] - 1s 1s/step - loss: 142605.6406\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have X_train and y_train\n",
    "# Assuming you have lengths, A, levels_left, and levels_right defined\n",
    "\n",
    "## fix seed\n",
    "keras.utils.set_random_seed(99)\n",
    "\n",
    "# Enable eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "layer = 5\n",
    "\n",
    "\n",
    "# Create a sequential model\n",
    "inp = Input(shape=(input_size,))\n",
    "h1_w1 = Dense(512*2*2, activation='relu')(inp)\n",
    "h2_w1  = Dense(512*2, activation='relu')(h1_w1)\n",
    "h3_w1 = Dense(256*2, activation='relu')(h2_w1)\n",
    "h4_w1 = Dense(256, activation='relu')(h3_w1)\n",
    "out = Dense(199, activation='linear')(h4_w1)\n",
    "\n",
    "\n",
    "\n",
    "mdl = Model(inputs=inp, outputs=out)\n",
    "\n",
    "# Compile the model with the custom loss function and an optimizer\n",
    "custom_loss = custom_loss_with_regularization(0.01, mdl.layers[layer].kernel, lengths, A, levels_left, levels_right)\n",
    "\n",
    "mdl.compile(loss=custom_loss, optimizer='adam')\n",
    "\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_epochs = 0\n",
    "best_batch_size = 0\n",
    "#who reads is gay\n",
    "for epochs in [100, 150, 200]:\n",
    "    for batch_size in [16, 32, 64]:\n",
    "        history = mdl.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        loss = history.history['val_loss'][-1] if 'val_loss' in history.history else history.history['loss'][-1]\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_epochs = epochs\n",
    "            best_batch_size = batch_size\n",
    "\n",
    "print(\"Best configuration - Epochs:\", best_epochs, \"Batch Size:\", best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hamza\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:371: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 13s 2s/step - loss: 17354.1484\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 10s 2s/step - loss: 4016.9980\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 10s 2s/step - loss: 1296.1299\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1071.3180\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: 668.8987\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: 497.7047\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: 497.7209\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 9s 2s/step - loss: 365.9208\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 10s 2s/step - loss: 263.5145\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 22s 4s/step - loss: 242.4198\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 32s 5s/step - loss: 189.3530\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 19s 3s/step - loss: 170.8783\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 10s 2s/step - loss: 169.8857\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 61s 11s/step - loss: 179.2750\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 57s 9s/step - loss: 197.8365\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: 157.0132\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: 133.2392\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 14s 2s/step - loss: 126.7285\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 10s 2s/step - loss: 145.8709\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 51s 9s/step - loss: 174.9410\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 20s 4s/step - loss: 133.7981\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: 124.9681\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: 120.9393\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 21s 4s/step - loss: 120.4044\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 22s 4s/step - loss: 125.1244\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 14s 2s/step - loss: 137.9945\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 56s 10s/step - loss: 112.5911\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 42s 6s/step - loss: 111.1852\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 34s 7s/step - loss: 105.2203\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 58s 10s/step - loss: 119.0340\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 55s 9s/step - loss: 109.0954\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 67s 11s/step - loss: 105.1771\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 56s 9s/step - loss: 131.9586\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 58s 10s/step - loss: 169.8200\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 31s 4s/step - loss: 129.3982\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 19s 3s/step - loss: 220.4180\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 21s 4s/step - loss: 138.4539\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 18s 3s/step - loss: 94.9688\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 54s 10s/step - loss: 90.2077\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 64s 11s/step - loss: 93.2704\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 49s 8s/step - loss: 104.8656\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 63s 11s/step - loss: 136.5795\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 61s 10s/step - loss: 140.2103\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 65s 11s/step - loss: 110.9959\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 53s 8s/step - loss: 88.3157\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 33s 6s/step - loss: 93.0663\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 32s 5s/step - loss: 90.9800\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 31s 5s/step - loss: 74.6281\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 35s 6s/step - loss: 72.2978\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 32s 5s/step - loss: 79.3846\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 33s 6s/step - loss: 92.8560\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 47s 8s/step - loss: 83.2181\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 21s 3s/step - loss: 83.2668\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 20s 3s/step - loss: 60.6529\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 20s 3s/step - loss: 58.3335\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 20s 3s/step - loss: 80.2156\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 21s 3s/step - loss: 79.7325\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 20s 3s/step - loss: 131.9340\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: 143.1533\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 31s 5s/step - loss: 61.2964\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 17s 3s/step - loss: 58.1439\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 9s 2s/step - loss: 57.3696\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 10s 2s/step - loss: 51.3771\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 23s 4s/step - loss: 223.5509\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 29s 5s/step - loss: 185.9035\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: 171.0857\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 25s 5s/step - loss: 114.5151\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 25s 4s/step - loss: 97.6597\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 10s 2s/step - loss: 86.5581\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 29s 5s/step - loss: 45.2122\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 18s 3s/step - loss: 68.1645\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 10s 2s/step - loss: 51.1873\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 10s 2s/step - loss: 59.7690\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 10s 2s/step - loss: 64.5191\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 10s 2s/step - loss: 54.2946\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: 64.9454\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: 61.6751\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 18s 3s/step - loss: 42.0229\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 24s 4s/step - loss: 36.5169\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 25s 4s/step - loss: 39.4954\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 22s 4s/step - loss: 47.6938\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 24s 4s/step - loss: 53.9339\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 23s 4s/step - loss: 50.5284\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 22s 4s/step - loss: 43.6794\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 23s 4s/step - loss: 42.9945\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 23s 4s/step - loss: 60.1570\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 24s 4s/step - loss: 31.1044\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 25s 4s/step - loss: 21.5740\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 24s 4s/step - loss: 35.3510\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 23s 4s/step - loss: 28.8277\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 26s 5s/step - loss: 17.5550\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 23s 4s/step - loss: 11.3478\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 23s 4s/step - loss: 30.6588\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 22s 4s/step - loss: 6.2501\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 23s 4s/step - loss: 18.6473\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 23s 4s/step - loss: 10.1834\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 23s 4s/step - loss: 27.3073\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 23s 4s/step - loss: 61.8636\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 23s 4s/step - loss: 40.4371\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 23s 4s/step - loss: 13.7134\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 24s 4s/step - loss: 44.3702\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 29s 5s/step - loss: 0.5950\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 33s 6s/step - loss: -0.1898\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 26s 4s/step - loss: 10.8894\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 24s 4s/step - loss: 34.5630\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 26s 4s/step - loss: 21.7066\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 24s 4s/step - loss: 12.4494\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 23s 4s/step - loss: -4.2821\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 23s 4s/step - loss: -7.5413\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 23s 4s/step - loss: 18.0734\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 24s 4s/step - loss: 26.8279\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: 51.7331\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: 120.8265\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: 76.0931\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 16s 3s/step - loss: 18.2209\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: 8.1191\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: 4.3206\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -4.9420\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -10.0786\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -13.0417\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -17.1025\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -17.3164\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 14s 2s/step - loss: -15.3406\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -21.9496\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -19.8974\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -23.0486\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -20.5912\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -25.2215\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 14s 2s/step - loss: -24.9139\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -28.5898\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -17.9809\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -4.7233\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -16.8801\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -15.2252\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -23.6919\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -23.2196\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -8.1258\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -29.0302\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -33.4270\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 14s 2s/step - loss: -33.6927\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -30.8742\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -31.2665\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -32.6471\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -37.1911\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -32.6770\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -39.6722\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -42.3190\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: -17.8736\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: 2.3998\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: 6.1762\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 15s 3s/step - loss: -24.6957\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -38.6299\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -27.2026\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 21s 4s/step - loss: -33.0681\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 15s 2s/step - loss: -42.7184\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -48.3393\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 17s 3s/step - loss: -48.8212\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -42.3481\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -53.4928\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -44.8109\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: 5.7080\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: 12.0082\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: 80.1151\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: 33.7710\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: 52.0478\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: 48.6664\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: 8.6297\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: -25.8153\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -41.4884\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 15s 2s/step - loss: -49.3689\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -50.8496\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -52.8215\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -55.1882\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -60.5012\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -55.5584\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -62.0558\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 12s 2s/step - loss: -63.2325\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: -29.1499\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: -53.8010\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 19s 3s/step - loss: -60.4525\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: -59.3462\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: -64.2583\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: -51.4318\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 10s 2s/step - loss: -53.2244\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 10s 2s/step - loss: -64.3854\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: -70.1980\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: -70.3717\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 10s 2s/step - loss: -71.0087\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 10s 2s/step - loss: -75.4779\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 9s 2s/step - loss: -70.9413\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 9s 2s/step - loss: -71.2192\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 9s 2s/step - loss: -73.1521\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 9s 2s/step - loss: -60.7663\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 9s 2s/step - loss: -65.7129\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 9s 2s/step - loss: -60.5606\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 9s 2s/step - loss: -67.7248\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 9s 2s/step - loss: -72.1861\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 9s 2s/step - loss: -75.1749\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 13s 2s/step - loss: -77.9479\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 11s 2s/step - loss: -76.6474\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have X_train and y_train\n",
    "# Assuming you have lengths, A, levels_left, and levels_right defined\n",
    "\n",
    "best_epochs = 200\n",
    "best_batch_size = 64\n",
    "\n",
    "## fix seed\n",
    "keras.utils.set_random_seed(99)\n",
    "\n",
    "\n",
    "layer = 5\n",
    "\n",
    "\n",
    "# Create a sequential model\n",
    "inp = Input(shape=(input_size,))\n",
    "h1_w1 = Dense(512*4, activation='relu')(inp)\n",
    "h2_w1  = Dense(512*2, activation='relu')(h1_w1)\n",
    "h3_w1 = Dense(256*2, activation='relu')(h2_w1)\n",
    "h4_w1 = Dense(256, activation='relu')(h3_w1)\n",
    "out = Dense(207, activation='linear')(h4_w1)\n",
    "\n",
    "\n",
    "mdl = Model(inputs=inp, outputs=out)\n",
    "\n",
    "# Compile the model with the custom loss function and an optimizer\n",
    "custom_loss = custom_loss_with_regularization(0.01, mdl.layers[layer].kernel, lengths, A, levels_left, levels_right)\n",
    "\n",
    "mdl.compile(loss=custom_loss, optimizer='adam')\n",
    "\n",
    "# Now train the model with the best configuration\n",
    "history = mdl.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "loss_values = history_dict[\"loss\"]\n",
    "#val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "#plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.9557709017785525"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = np.array(mdl.predict(X_test))#.reshape(-1)\n",
    "calculate_wmape(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### METRIC WMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmape_level(actual_value, forecasted_value, total_ts, lengths):\n",
    "    nb_levels = len(lengths)\n",
    "    wmapes = []\n",
    "    for l in range(nb_levels):\n",
    "        actual_value_ts = actual_value[:, total_ts[l]:total_ts[l+1]]\n",
    "        forecasted_value_ts = forecasted_value[:, total_ts[l]:total_ts[l+1]]\n",
    "        wmapes.append(calculate_wmape(actual_value_ts, forecasted_value_ts))\n",
    "    return wmapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.9791267010022917, 3.1749977081943412, 3.2940427463694375, 23.43577599856998]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### w/o reco \n",
    "wmape_level(y_test, y_predict, total_ts, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.7006340527584745, 2.970819814396073, 3.1186610107799613, 23.0329687291797]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### with reco\n",
    "wmape_level(y_test, y_predict, total_ts, lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METRIC RMSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### round to int value of array\n",
    "def round_array(array):\n",
    "    for i in range(len(array)):\n",
    "        array[i] = round(array[i])\n",
    "        if array[i] <= 0:\n",
    "            array[i] = 0\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I have an array of shape (89,5)\n",
    "### create dataframe with predictions\n",
    "def create_df(y_predict, pred_length, data):\n",
    "    ### dataframe with name of columns same as in data_for_model_000\n",
    "    ### create a dataframe based on data, remove last pred_length rows, and add y_predict\n",
    "    ### return dataframe\n",
    "    y_predict_df = y_predict.astype(np.float32)\n",
    "    y_predict_df = pd.DataFrame(y_predict_df)\n",
    "    y_predict_df = y_predict_df\n",
    "    df = data.copy()\n",
    "    for i,col in enumerate(data.columns):\n",
    "        df[col][-(pred_length):] = round_array(y_predict_df[:][i])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_48596\\2594118051.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  array[i] = round(array[i])\n"
     ]
    }
   ],
   "source": [
    "data_pred = create_df(y_predict, pred_length, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsse_ts(pred_length, data, data_pred, ts):\n",
    "    H = pred_length\n",
    "    T = data.shape[0] - H\n",
    "    ts_array = data.iloc[:,ts].values\n",
    "    ts_array_pred = data_pred.iloc[:,ts].values\n",
    "    e = (1/H)*np.sum((ts_array[t] - ts_array_pred[t])**2 for t in range(T, T+H))\n",
    "    e_naive = (1/(T-1))*np.sum((ts_array[t] - ts_array[t-1])**2 for t in range(1, T))\n",
    "    return np.sqrt(e/e_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsse_level(pred_length, data, data_pred, total_ts, lengths):\n",
    "    nb_levels = len(lengths)\n",
    "    r_l = [0]*nb_levels\n",
    "    for l in range(nb_levels):\n",
    "        for j in range(total_ts[l], total_ts[l+1]):\n",
    "            #print(l, j)\n",
    "            r_l[l] += (1/lengths[l])*rmsse_ts(pred_length, data, data_pred, j)\n",
    "    #print(r_l)\n",
    "    R = np.mean(r_l)\n",
    "    return r_l, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_48596\\821578998.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  e = (1/H)*np.sum((ts_array[t] - ts_array_pred[t])**2 for t in range(T, T+H))\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_48596\\821578998.py:7: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  e_naive = (1/(T-1))*np.sum((ts_array[t] - ts_array[t-1])**2 for t in range(1, T))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.19583958339202442,\n",
       "  0.2146349954481625,\n",
       "  0.23792780388080723,\n",
       "  1.098861655168192],\n",
       " 0.43681600947229654)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### w/o reco\n",
    "rmsse_level(pred_length, data, data_pred, total_ts, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_48596\\821578998.py:6: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  e = (1/H)*np.sum((ts_array[t] - ts_array_pred[t])**2 for t in range(T, T+H))\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_48596\\821578998.py:7: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  e_naive = (1/(T-1))*np.sum((ts_array[t] - ts_array[t-1])**2 for t in range(1, T))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.1780926207037501,\n",
       "  0.19633887863962618,\n",
       "  0.2205140014101296,\n",
       "  1.0855012062285199],\n",
       " 0.4201116767455064)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### w reco\n",
    "rmsse_level(pred_length, data, data_pred, total_ts, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
